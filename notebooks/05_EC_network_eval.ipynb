{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddf1720a",
   "metadata": {},
   "source": [
    "#Ecuadorian Network\n",
    "Evaluation of data from Pypsa-Earth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "35ef2316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network loaded successfully from: c:\\Repositories\\Repos\\pypsa-earth-project\\EcuadorElectricGrid\\data\\raw\\networks\\base.nc\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Setup environment and load the base PyPSA-Earth network for a specified country.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import pypsa\n",
    "import warnings\n",
    "import pypsa\n",
    "from pathlib import Path\n",
    "import os\n",
    "import copy\n",
    "from os.path import join\n",
    "\n",
    "# Import all dirs\n",
    "parent_dir = Path(os.getcwd()).parents[0]\n",
    "sys.path.append(str(parent_dir))\n",
    "from src.paths import all_dirs\n",
    "\n",
    "dirs = all_dirs()\n",
    "\n",
    "\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "LOG_FILE = join(parent_dir, \"logs.log\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s: %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    handlers=[logging.FileHandler(LOG_FILE, encoding=\"utf-8\")],\n",
    ")\n",
    "\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "# Suppress unnecessary warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "# Define country parameters\n",
    "country_code = \"EC\"  # ISO 2-letter code (e.g., 'GH' for Ghana, 'CO' for Colombia)\n",
    "country_name = \"Ecuador\"  # Country name\n",
    "country_gadm = \"ECU\"  # ISO 3-letter GADM code\n",
    "\n",
    "# Load the base network file path\n",
    "network_dir = dirs[\"data/raw/networks\"]\n",
    "network_file = \"base.nc\"\n",
    "network_path = os.path.join(network_dir, network_file)\n",
    "\n",
    "# Load the PyPSA network\n",
    "network = pypsa.Network(network_path)\n",
    "network_original = copy.deepcopy(network)\n",
    "\n",
    "## Dictionary with all the networks\n",
    "networks_dict = {\n",
    "    \"network_original\": network_original\n",
    "}\n",
    "\n",
    "print(f\"Network loaded successfully from: {network_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b306835b",
   "metadata": {},
   "source": [
    "The values of voltage differ from the nominal values from Ecuador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "0edcca2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[buses] Updated 7 entries in v_nom:\n",
      "[buses] Updated 7 entries in v_nom (summary failed: Grouper for 'v_nom' not 1-dimensional)\n",
      "[lines] Updated 5 entries in v_nom:\n",
      "[lines] Updated 5 entries in v_nom (summary failed: Grouper for 'v_nom' not 1-dimensional)\n",
      "\n",
      "Final voltage levels:\n",
      "  buses : [ 48.  69. 138. 230. 500.]\n",
      "  lines : [ 48.  69. 138. 230. 500.]\n"
     ]
    }
   ],
   "source": [
    "def snap_voltages_to_ecuador_levels(network):\n",
    "    \"\"\"\n",
    "    Snap nominal voltages (v_nom) in a PyPSA network to Ecuador's standard levels.\n",
    "\n",
    "    Ecuador standard transmission voltages: 500, 230, 138, 69, 48 kV.\n",
    "\n",
    "    The function modifies the PyPSA network *in place*:\n",
    "      - network.buses.v_nom\n",
    "      - network.lines.v_nom\n",
    "\n",
    "    It adds backup columns 'v_nom_raw' to preserve original values.\n",
    "    Prints a short summary of all changes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network : pypsa.Network\n",
    "        The PyPSA network object to modify.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    network : pypsa.Network\n",
    "        The same network object (modified in place, returned for chaining).\n",
    "    \"\"\"\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    VALID_VOLTAGES_KV = np.array([500.0, 230.0, 138.0, 69.0, 48.0])\n",
    "\n",
    "    def _nearest_valid_voltage(values):\n",
    "        vals = np.asarray(values, dtype=float)\n",
    "        mask = ~np.isnan(vals)\n",
    "        snapped = np.full_like(vals, np.nan, dtype=float)\n",
    "        diffs = np.abs(vals[mask, None] - VALID_VOLTAGES_KV[None, :])\n",
    "        snapped[mask] = VALID_VOLTAGES_KV[np.argmin(diffs, axis=1)]\n",
    "        return snapped\n",
    "\n",
    "    def _apply_snap(df, label):\n",
    "        if \"v_nom\" not in df.columns:\n",
    "            print(f\"[WARN] '{label}' has no 'v_nom' column. Skipping.\")\n",
    "            return\n",
    "\n",
    "        if \"v_nom_raw\" not in df.columns:\n",
    "            df[\"v_nom_raw\"] = df[\"v_nom\"].copy()\n",
    "\n",
    "        before = df[\"v_nom\"].copy()\n",
    "        df[\"v_nom\"] = _nearest_valid_voltage(df[\"v_nom\"].values)\n",
    "\n",
    "        changed = before != df[\"v_nom\"]\n",
    "        n_changed = int(changed.sum())\n",
    "\n",
    "        if n_changed:\n",
    "            try: \n",
    "                print(f\"[{label}] Updated {n_changed} entries in v_nom:\")\n",
    "                summary = (\n",
    "                    pd.concat([before[changed], df[\"v_nom\"][changed]], axis=1)\n",
    "                    .rename(columns={0: \"old\", 1: \"new\"})\n",
    "                    .value_counts()\n",
    "                    .rename(\"count\")\n",
    "                )\n",
    "                for (old, new), cnt in summary.items():\n",
    "                    print(f\"  {old:.0f} → {new:.0f} kV : {cnt}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[{label}] Updated {n_changed} entries in v_nom (summary failed: {e})\")\n",
    "        else:\n",
    "            print(f\"[{label}] No changes needed.\")\n",
    "\n",
    "    # --- Apply to main components ---\n",
    "    _apply_snap(network.buses, \"buses\")\n",
    "    _apply_snap(network.lines, \"lines\")\n",
    "\n",
    "    # Optional: extend here if needed\n",
    "    # if hasattr(network, \"transformers\"):\n",
    "    #     _apply_snap(network.transformers, \"transformers\")\n",
    "\n",
    "    # --- Summary ---\n",
    "    print(\"\\nFinal voltage levels:\")\n",
    "    print(\"  buses :\", np.sort(network.buses.v_nom.unique()))\n",
    "    print(\"  lines :\", np.sort(network.lines.v_nom.unique()))\n",
    "\n",
    "    return network\n",
    "\n",
    "# Apply voltage snapping\n",
    "network_snapped = snap_voltages_to_ecuador_levels(network)\n",
    "networks_dict[\"network_snapped\"] = network_snapped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6015f36b",
   "metadata": {},
   "source": [
    "The new components from the Master Plan and from the Expansion plans need to be added.\n",
    "Additionally, appropiate buses for the nuclear option need to be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "a487218f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expansion merged: +23 buses, +16 lines, +15 transformers; updated 0 buses, 0 lines, 0 transformers.\n"
     ]
    }
   ],
   "source": [
    "# Copy network and merge expansion data for buses, lines, and transformers using madd\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Make a safe copy of the network already loaded as `network`\n",
    "network_exp = copy.deepcopy(network_snapped)\n",
    "\n",
    "# Base dir for expansion CSVs using your dirs mapping\n",
    "DATA_DIR = dirs[\"data/raw/networks\"]\n",
    "\n",
    "# CSV file paths\n",
    "BUS_EXP_CSV = os.path.join(DATA_DIR, \"EC_buses_expansion.csv\")\n",
    "LINE_EXP_CSV = os.path.join(DATA_DIR, \"EC_lines_expansion.csv\")\n",
    "TRAFO_EXP_CSV = os.path.join(DATA_DIR, \"EC_trafo_expansion.csv\")\n",
    "\n",
    "\n",
    "def _filter_to_allowed_attrs(nw, component, df):\n",
    "    helper_dict = {\n",
    "        \"Bus\": nw.buses,\n",
    "        \"Line\": nw.lines,\n",
    "        \"Transformer\": nw.transformers,\n",
    "    }\n",
    "    allowed = set(helper_dict[component].columns)\n",
    "    keep_cols = [c for c in df.columns if c in allowed]\n",
    "    return df[keep_cols]\n",
    "\n",
    "\n",
    "def _update_existing(nw, component, df):\n",
    "    table_map = {\n",
    "        \"Bus\": \"buses\",\n",
    "        \"Line\": \"lines\",\n",
    "        \"Transformer\": \"transformers\",\n",
    "    }\n",
    "    table = getattr(nw, table_map[component])\n",
    "    existing = df.index.intersection(table.index)\n",
    "    if len(existing) > 0:\n",
    "        cols = df.columns\n",
    "        table.loc[existing, cols] = df.loc[existing, cols]\n",
    "    return existing\n",
    "\n",
    "\n",
    "def _madd_new(nw, component, df):\n",
    "    if len(df) == 0:\n",
    "        return\n",
    "    # df must be indexed by component names and include only valid attrs\n",
    "    names = df.index\n",
    "    kwargs = {col: df[col] for col in df.columns}\n",
    "    nw.madd(component, names, **kwargs)\n",
    "\n",
    "\n",
    "# 1) Buses expansion\n",
    "bus_df = pd.read_csv(BUS_EXP_CSV)\n",
    "if \"Name\" not in bus_df.columns:\n",
    "    raise ValueError(\"EC_buses_expansion.csv must contain a 'Name' column.\")\n",
    "bus_df = bus_df.set_index(\"Bus\")\n",
    "bus_df = _filter_to_allowed_attrs(network_exp, \"Bus\", bus_df)\n",
    "\n",
    "existing_buses = _update_existing(network_exp, \"Bus\", bus_df)\n",
    "to_add_buses = bus_df.drop(index=existing_buses, errors=\"ignore\")\n",
    "_madd_new(network_exp, \"Bus\", to_add_buses)\n",
    "\n",
    "# 2) Lines expansion\n",
    "line_df = pd.read_csv(LINE_EXP_CSV)\n",
    "if \"Line\" not in line_df.columns:\n",
    "    raise ValueError(\"EC_lines_expansion.csv must contain a 'Line' column.\")\n",
    "line_df = line_df.set_index(\"ID\")\n",
    "line_df = _filter_to_allowed_attrs(network_exp, \"Line\", line_df)\n",
    "\n",
    "existing_lines = _update_existing(network_exp, \"Line\", line_df)\n",
    "to_add_lines = line_df.drop(index=existing_lines, errors=\"ignore\")\n",
    "_madd_new(network_exp, \"Line\", to_add_lines)\n",
    "\n",
    "\n",
    "# 3) Transformers expansion\n",
    "trafo_df = pd.read_csv(TRAFO_EXP_CSV)\n",
    "if \"Transformer\" not in trafo_df.columns:\n",
    "    raise ValueError(\"EC_trafo_expansion.csv must contain a 'Transformer' column.\")\n",
    "trafo_df = trafo_df.set_index(\"ID\")\n",
    "trafo_df = _filter_to_allowed_attrs(network_exp, \"Transformer\", trafo_df)\n",
    "\n",
    "\n",
    "existing_trafos = _update_existing(network_exp, \"Transformer\", trafo_df)\n",
    "to_add_trafos = trafo_df.drop(index=existing_trafos, errors=\"ignore\")\n",
    "_madd_new(network_exp, \"Transformer\", to_add_trafos)\n",
    "\n",
    "# Summary\n",
    "print(\n",
    "    f\"Expansion merged: +{len(to_add_buses)} buses, \"\n",
    "    f\"+{len(to_add_lines)} lines, \"\n",
    "    f\"+{len(to_add_trafos)} transformers; \"\n",
    "    f\"updated {len(existing_buses)} buses, {len(existing_lines)} lines, {len(existing_trafos)} transformers.\"\n",
    ")\n",
    "# plot_buses_and_lines_by_voltage(network_exp, save_name=\"ecuador_buses_expansion.png\")\n",
    "\n",
    "networks_dict[\"network_expanded\"] = network_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968cbe8a",
   "metadata": {},
   "source": [
    "Plot only the expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "fd213c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orphan_buses\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bus</th>\n",
       "      <th>v_nom</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>109</td>\n",
       "      <td>48.0</td>\n",
       "      <td>-78.7385</td>\n",
       "      <td>-2.9038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>122</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-78.7498</td>\n",
       "      <td>-1.7366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>134</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-78.7659</td>\n",
       "      <td>-3.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>135</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-79.9708</td>\n",
       "      <td>-3.5235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>168</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-80.5068</td>\n",
       "      <td>-0.8724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>172</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-80.1647</td>\n",
       "      <td>-0.8570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>180</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-78.7958</td>\n",
       "      <td>1.2621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>187</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-80.0950</td>\n",
       "      <td>-0.8791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>193</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-79.6392</td>\n",
       "      <td>-3.7076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>203</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-80.2042</td>\n",
       "      <td>-2.3207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>206</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-80.2780</td>\n",
       "      <td>-1.3372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>210</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-79.4589</td>\n",
       "      <td>-1.4211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>226</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-79.1919</td>\n",
       "      <td>-2.2390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>235</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-79.4243</td>\n",
       "      <td>-4.5381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>238</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-79.6412</td>\n",
       "      <td>-3.8890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>241</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-79.6221</td>\n",
       "      <td>-2.6455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>253</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-77.8418</td>\n",
       "      <td>-0.4140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Bus  v_nom      lon     lat\n",
       "109  109   48.0 -78.7385 -2.9038\n",
       "122  122   69.0 -78.7498 -1.7366\n",
       "134  134   69.0 -78.7659 -3.8451\n",
       "135  135   69.0 -79.9708 -3.5235\n",
       "168  168   69.0 -80.5068 -0.8724\n",
       "172  172   69.0 -80.1647 -0.8570\n",
       "180  180   69.0 -78.7958  1.2621\n",
       "187  187   69.0 -80.0950 -0.8791\n",
       "193  193   69.0 -79.6392 -3.7076\n",
       "203  203   69.0 -80.2042 -2.3207\n",
       "206  206   69.0 -80.2780 -1.3372\n",
       "210  210   69.0 -79.4589 -1.4211\n",
       "226  226   69.0 -79.1919 -2.2390\n",
       "235  235   69.0 -79.4243 -4.5381\n",
       "238  238   69.0 -79.6412 -3.8890\n",
       "241  241   69.0 -79.6221 -2.6455\n",
       "253  253   69.0 -77.8418 -0.4140"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bus_voltage_levels\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[48, 69, 138, 230, 500]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trafo_pairs_found_network\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(48, 138), (69, 138), (69, 230), (69, 500), (138, 230), (230, 500)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topology_summary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'230kV': {'n_components': 63, 'sizes': [5, 4, 3, 2, 2]},\n",
       " '500kV': {'n_components': 13, 'sizes': [6, 1, 1, 1, 1]}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Sanity + topology checks for expansion elements against the existing network\n",
    "from src.ec_network_eval import evaluate_network\n",
    "\n",
    "_, issues = evaluate_network(\n",
    "    network_exp,\n",
    "    data_dir = dirs[\"data/raw/networks\"],\n",
    "    downstream_path = (500,48))\n",
    "\n",
    "for k,v in issues:\n",
    "    if \"empty\" not in k:\n",
    "        print(k)\n",
    "        display(v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89826bb9",
   "metadata": {},
   "source": [
    "The orphan buses must be cleaned. Linking them with other components may lead to incorrect strcturing of the grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "e3c696b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_orphan_buses(n_exp, issues, log_csv_path=\"removed_orphan_buses.csv\"):\n",
    "    \"\"\"\n",
    "    Remove buses that are truly unreferenced by any component in the network.\n",
    "    - Deep-copies the input network (does not modify original).\n",
    "    - Uses orphan candidates from issues if present.\n",
    "    - Verifies true orphans by scanning all component bus references.\n",
    "    - Optionally writes removed buses to CSV (set log_csv_path=None to skip)\n",
    "    Returns\n",
    "    - n_exp_clean: deep-copied and cleaned network\n",
    "    \"\"\"\n",
    "    # 1) Extract candidate orphan buses from issues (accepts multiple keys)\n",
    "    orphan_df = None\n",
    "    for key, data in issues:\n",
    "        if key in (\"orphan_buses\", \"orphan_buses_lines_trafos_only\"):\n",
    "            orphan_df = data\n",
    "            break\n",
    "\n",
    "    candidate_orphans = pd.Index([])\n",
    "    if orphan_df is not None and \"Bus\" in orphan_df.columns:\n",
    "        candidate_orphans = pd.Index(orphan_df[\"Bus\"].tolist())\n",
    "\n",
    "    # 2) Collect all buses referenced by any component\n",
    "    def buses_referenced_by_components(n):\n",
    "        refs = set()\n",
    "        comps = [\n",
    "            (\"loads\", n.loads),\n",
    "            (\"generators\", n.generators),\n",
    "            (\"storage_units\", n.storage_units),\n",
    "            (\"stores\", n.stores),\n",
    "            (\"links\", n.links),\n",
    "            (\"lines\", n.lines),\n",
    "            (\"transformers\", n.transformers),\n",
    "            (\"shunt_impedances\", n.shunt_impedances),\n",
    "        ]\n",
    "        for _, df in comps:\n",
    "            if df is None or getattr(df, \"empty\", True):\n",
    "                continue\n",
    "            # Any column that starts with 'bus' is considered a bus reference\n",
    "            bus_cols = [c for c in df.columns if c.startswith(\"bus\")]\n",
    "            if not bus_cols:\n",
    "                continue\n",
    "            for col in bus_cols:\n",
    "                # Normalize to string IDs, drop NaNs\n",
    "                refs |= set(df[col].dropna().astype(str))\n",
    "        return refs\n",
    "\n",
    "    ref_buses = buses_referenced_by_components(n_exp)\n",
    "    # Normalize the network bus index to string for cross-compatibility\n",
    "    bus_index_str = n_exp.buses.index.astype(str)\n",
    "    # Buses not referenced by any component\n",
    "    true_orphans_idx = n_exp.buses.loc[~bus_index_str.isin(ref_buses)].index\n",
    "\n",
    "    # 3) Intersect with candidates (if provided)\n",
    "    if len(candidate_orphans) > 0:\n",
    "        candidate_orphans = pd.Index(candidate_orphans)\n",
    "        true_orphans_idx = candidate_orphans.intersection(true_orphans_idx)\n",
    "\n",
    "    # 4) Deep-copy the network and remove the confirmed orphan buses\n",
    "    n_exp_clean = copy.deepcopy(n_exp)\n",
    "    for b in true_orphans_idx:\n",
    "        n_exp_clean.remove(\"Bus\", b)\n",
    "\n",
    "    # 5) Optional: log removed buses\n",
    "    if log_csv_path:\n",
    "        pd.DataFrame({\"removed_bus\": pd.Index(true_orphans_idx)}).to_csv(\n",
    "            log_csv_path, index=False\n",
    "        )\n",
    "\n",
    "    # 6) Sanity assert\n",
    "    for b in true_orphans_idx:\n",
    "        assert b not in n_exp_clean.buses.index, f\"Bus {b} was not removed properly.\"\n",
    "    logging.info(\"All orphan buses cleared\")\n",
    "\n",
    "    return n_exp_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "904ae0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_exp_clean = clean_orphan_buses(network_exp, issues)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "cb4c779c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 2 lines to line_fix.csv\n",
      "          Line    ID  num_parallel  v_nom bus0 bus1    length     dc geometry  \\\n",
      "0  fix_181_277  1000           2.0  138.0  181  277  6.026806  False     None   \n",
      "1   fix_280_91  1001           2.0  138.0  280   91  9.669560  False     None   \n",
      "\n",
      "  bounds  ... v_ang_max sub_network  x_pu  r_pu  g_pu  b_pu  x_pu_eff  \\\n",
      "0   None  ...       inf               0.0   0.0   0.0   0.0       0.0   \n",
      "1   None  ...       inf               0.0   0.0   0.0   0.0       0.0   \n",
      "\n",
      "   r_pu_eff  s_nom_opt  max_loading  \n",
      "0       0.0        0.0          1.0  \n",
      "1       0.0        0.0          1.0  \n",
      "\n",
      "[2 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Great-circle distance in km between two (lat, lon) points.\"\"\"\n",
    "    R = 6371.0\n",
    "    phi1, phi2 = math.radians(lat1), math.radians(lat2)\n",
    "    dphi = math.radians(lat2 - lat1)\n",
    "    dlambda = math.radians(lon2 - lon1)\n",
    "    a = math.sin(dphi/2)**2 + math.cos(phi1)*math.cos(phi2)*math.sin(dlambda/2)**2\n",
    "    return 2 * R * math.asin(math.sqrt(a))\n",
    "\n",
    "\n",
    "def _resolve_bus_id(bus_id, buses_index):\n",
    "    \"\"\"\n",
    "    Normalize bus id (int or str) to the string index used in network.buses.\n",
    "    \"\"\"\n",
    "    s = str(bus_id)\n",
    "    if s in buses_index:\n",
    "        return s\n",
    "    raise KeyError(f\"Bus {bus_id} not found in network.buses.index\")\n",
    "\n",
    "\n",
    "def write_csv_lines(network, bus_pair_list, out_file=\"line_fix.csv\"):\n",
    "    \"\"\"\n",
    "    Create a lines CSV (line_fix.csv) with rows connecting the given bus pairs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network : pypsa.Network\n",
    "    bus_pair_list : list of (bus0, bus1)\n",
    "        Each element can be int or str, but must correspond to an entry in network.buses.index.\n",
    "    out_file : str\n",
    "        Output CSV file name (default: 'line_fix.csv').\n",
    "\n",
    "    The CSV will have the columns:\n",
    "    Line,num_parallel,v_nom,bus0,bus1,length,dc,geometry,bounds,carrier,type,\n",
    "    s_max_pu,s_nom,underwater_fraction,v_nom_raw,x,r,g,b,s_nom_mod,s_nom_extendable,\n",
    "    s_nom_min,s_nom_max,capital_cost,build_year,lifetime,terrain_factor,\n",
    "    v_ang_min,v_ang_max,sub_network,x_pu,r_pu,g_pu,b_pu,x_pu_eff,r_pu_eff,\n",
    "    s_nom_opt,max_loading\n",
    "    \"\"\"\n",
    "    buses = network.buses\n",
    "    lines = network.lines\n",
    "\n",
    "    required_bus_cols = {\"lon\", \"lat\", \"v_nom\"}\n",
    "    if not required_bus_cols.issubset(buses.columns):\n",
    "        missing = required_bus_cols - set(buses.columns)\n",
    "        raise ValueError(f\"network.buses missing required columns: {missing}\")\n",
    "\n",
    "    # Precompute template lines by v_nom (first line for each voltage)\n",
    "    templates_by_v = {}\n",
    "    if \"v_nom\" in lines.columns and not lines.empty:\n",
    "        for v in sorted(lines[\"v_nom\"].dropna().unique()):\n",
    "            same_v = lines[lines[\"v_nom\"] == v]\n",
    "            if not same_v.empty:\n",
    "                templates_by_v[v] = same_v.iloc[0]\n",
    "    # Global fallback template\n",
    "    global_template = lines.iloc[0] if not lines.empty else None\n",
    "\n",
    "    # CSV column order (as you specified)\n",
    "    cols = [\n",
    "        \"Line\",\"ID\",\"num_parallel\",\"v_nom\",\"bus0\",\"bus1\",\"length\",\"dc\",\"geometry\",\"bounds\",\n",
    "        \"carrier\",\"type\",\"s_max_pu\",\"s_nom\",\"underwater_fraction\",\"v_nom_raw\",\n",
    "        \"x\",\"r\",\"g\",\"b\",\"s_nom_mod\",\"s_nom_extendable\",\"s_nom_min\",\"s_nom_max\",\n",
    "        \"capital_cost\",\"build_year\",\"lifetime\",\"terrain_factor\",\n",
    "        \"v_ang_min\",\"v_ang_max\",\"sub_network\",\n",
    "        \"x_pu\",\"r_pu\",\"g_pu\",\"b_pu\",\"x_pu_eff\",\"r_pu_eff\",\"s_nom_opt\",\"max_loading\",\n",
    "    ]\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for idx, (bus0, bus1) in enumerate(bus_pair_list):\n",
    "        # ---- resolve bus ids (to strings) ----\n",
    "        b0_id = _resolve_bus_id(bus0, buses.index)\n",
    "        b1_id = _resolve_bus_id(bus1, buses.index)\n",
    "\n",
    "        b0 = buses.loc[b0_id]\n",
    "        b1 = buses.loc[b1_id]\n",
    "\n",
    "        lon0, lat0 = float(b0[\"lon\"]), float(b0[\"lat\"])\n",
    "        lon1, lat1 = float(b1[\"lon\"]), float(b1[\"lat\"])\n",
    "\n",
    "        v0 = float(b0[\"v_nom\"])\n",
    "        v1 = float(b1[\"v_nom\"])\n",
    "        if not np.isclose(v0, v1):\n",
    "            raise ValueError(\n",
    "                f\"Bus {b0_id} has v_nom={v0}, bus {b1_id} has v_nom={v1}; \"\n",
    "                \"this pair should be a transformer, not a line.\"\n",
    "            )\n",
    "        v_nom = v0\n",
    "\n",
    "        length_km = haversine_km(lat0, lon0, lat1, lon1)\n",
    "\n",
    "        geometry = f\"LINESTRING ({lon0} {lat0}, {lon1} {lat1})\"\n",
    "        bounds   = f\"MULTIPOINT (({lon0} {lat0}), ({lon1} {lat1}))\"\n",
    "\n",
    "        # Choose template: same voltage if available, else global\n",
    "        template = templates_by_v.get(v_nom, global_template)\n",
    "\n",
    "        # Start row with defaults\n",
    "        row = {\n",
    "            \"Line\":              f\"fix_{b0_id}_{b1_id}\", \n",
    "            \"ID\":                idx+1000,\n",
    "            \"num_parallel\":      1.0,\n",
    "            \"v_nom\":             v_nom,\n",
    "            \"bus0\":              b0_id,\n",
    "            \"bus1\":              b1_id,\n",
    "            \"length\":            length_km,\n",
    "            \"dc\":                False,\n",
    "            \"geometry\":          None, #geometry,\n",
    "            \"bounds\":            None, #bounds,\n",
    "            \"carrier\":           \"AC\",\n",
    "            \"type\":              \"\",\n",
    "            \"s_max_pu\":          1.0,\n",
    "            \"s_nom\":             0.0,\n",
    "            \"underwater_fraction\": 0.0,\n",
    "            \"v_nom_raw\":         v_nom,\n",
    "            \"x\":                 0.0,\n",
    "            \"r\":                 0.0,\n",
    "            \"g\":                 0.0,\n",
    "            \"b\":                 0.0,\n",
    "            \"s_nom_mod\":         0.0,\n",
    "            \"s_nom_extendable\":  False,\n",
    "            \"s_nom_min\":         0.0,\n",
    "            \"s_nom_max\":         float(\"inf\"),\n",
    "            \"capital_cost\":      0.0,\n",
    "            \"build_year\":        0,\n",
    "            \"lifetime\":          float(\"inf\"),\n",
    "            \"terrain_factor\":    1.0,\n",
    "            \"v_ang_min\":         float(\"-inf\"),\n",
    "            \"v_ang_max\":         float(\"inf\"),\n",
    "            \"sub_network\":       0,\n",
    "            \"x_pu\":              0.0,\n",
    "            \"r_pu\":              0.0,\n",
    "            \"g_pu\":              0.0,\n",
    "            \"b_pu\":              0.0,\n",
    "            \"x_pu_eff\":          0.0,\n",
    "            \"r_pu_eff\":          0.0,\n",
    "            \"s_nom_opt\":         0.0,\n",
    "            \"max_loading\":       1.0,\n",
    "        }\n",
    "\n",
    "        # If we have a template, overwrite with its values where appropriate\n",
    "        if template is not None:\n",
    "            for k in [\"ID\",\n",
    "                \"num_parallel\",\"carrier\",\"type\",\"s_max_pu\",\"s_nom\",\n",
    "                \"underwater_fraction\",\"x\",\"r\",\"g\",\"b\",\"s_nom_mod\",\n",
    "                \"s_nom_extendable\",\"s_nom_min\",\"s_nom_max\",\n",
    "                \"capital_cost\",\"build_year\",\"lifetime\",\"terrain_factor\",\n",
    "                \"v_ang_min\",\"v_ang_max\",\"sub_network\",\n",
    "                \"x_pu\",\"r_pu\",\"g_pu\",\"b_pu\",\"x_pu_eff\",\"r_pu_eff\",\n",
    "                \"s_nom_opt\",\"max_loading\",\n",
    "            ]:\n",
    "                if k in template.index:\n",
    "                    row[k] = template[k]\n",
    "\n",
    "            # if template has v_nom_raw etc. we keep our v_nom_raw = v_nom,\n",
    "            # but you could also copy if you prefer:\n",
    "            # if \"v_nom_raw\" in template.index:\n",
    "            #     row[\"v_nom_raw\"] = template[\"v_nom_raw\"]\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=cols)\n",
    "    df.to_csv(out_file, index=False)\n",
    "    print(f\"Wrote {len(df)} lines to {out_file}\")\n",
    "    return df\n",
    "\n",
    "# Example: your two fixes\n",
    "bus_pairs = [(\"181\", \"277\"), (\"280\", \"91\")]\n",
    "\n",
    "df_fix = write_csv_lines(network, bus_pairs, out_file=\"line_fix.csv\")\n",
    "print(df_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "fb20eca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Lines expansion\n",
    "line_df = pd.read_csv(\"line_fix.csv\")\n",
    "if \"Line\" not in line_df.columns:\n",
    "    raise ValueError(\"EC_lines_expansion.csv must contain a 'Line' column.\")\n",
    "line_df = line_df.set_index(\"ID\")\n",
    "line_df = _filter_to_allowed_attrs(n_exp_clean, \"Line\", line_df)\n",
    "\n",
    "existing_lines = _update_existing(n_exp_clean, \"Line\", line_df)\n",
    "to_add_lines = line_df.drop(index=existing_lines, errors=\"ignore\")\n",
    "_madd_new(n_exp_clean, \"Line\", to_add_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "ffa61055",
   "metadata": {},
   "outputs": [],
   "source": [
    "networks_dict[\"network_expanded_no_orphans\"] =n_exp_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9234a8a7",
   "metadata": {},
   "source": [
    "Now, we need three network objects\n",
    "1) One containing only the original network structure, that is, no buses, trafos, or lines that represent the expansion\n",
    "2) One that contians the network structure as per the master plan\n",
    "3) One that contians the nuclear buses \n",
    "\n",
    "The final network is the one already containing the nuclear option, so we remove from there\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e15bcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_nuclear = copy.deepcopy(n_exp_clean)\n",
    "\n",
    "# Run sanity cheack\n",
    "def check_network(network):\n",
    "    _, issues_n = evaluate_network(\n",
    "        network,\n",
    "        data_dir = dirs[\"data/raw/networks\"],\n",
    "        downstream_path = (500,48))\n",
    "    for k,v in issues_n:\n",
    "        if \"orphan\" in k:\n",
    "            display(v)\n",
    "            raise ValueError(\"Network still has orphan buses\")\n",
    "    \n",
    "    logging.info(\"Network has no orphan buses\")\n",
    "\n",
    "check_network(network_nuclear)\n",
    "\n",
    "networks_dict[\"network_nuclear\"]= network_nuclear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826c21ab",
   "metadata": {},
   "source": [
    "Remove the nuclear buses and related components to get only the \"productive mix\" network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "77db757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "\n",
    "def remove_nuclear_assets(network_nuclear, dirs, buses_csv=\"EC_buses_expansion.csv\"):\n",
    "    \"\"\"\n",
    "    Deep-copy the network, then remove:\n",
    "    - all buses whose Name contains 'nuclear' (case-insensitive) from the expansion buses CSV\n",
    "    - all lines/transformers connected to those buses\n",
    "\n",
    "    Logging:\n",
    "    - Logs the nuclear bus IDs found\n",
    "    - Logs the line and transformer IDs that are removed\n",
    "\n",
    "    Returns\n",
    "    - network_prod_mix: cleaned network (deep copy of input)\n",
    "    \"\"\"\n",
    "    network_prod_mix = copy.deepcopy(network_nuclear)\n",
    "\n",
    "    # 1) Nuclear bus IDs (robust to NaNs/whitespace; read from expansion CSV)\n",
    "    bus_csv_path = join(dirs[\"data/raw/networks\"], buses_csv)\n",
    "    bus_new = pd.read_csv(bus_csv_path)\n",
    "    mask_nuc = (\n",
    "        bus_new[\"Name\"].astype(str).str.strip().str.lower().str.contains(\"nuclear\", na=False)\n",
    "    )\n",
    "    ids_bus_nuclear = (\n",
    "        pd.to_numeric(bus_new.loc[mask_nuc, \"Bus\"], errors=\"coerce\")\n",
    "        .dropna()\n",
    "        .astype(\"Int64\")\n",
    "    )\n",
    "    logging.info(f\"nuclear buses: {ids_bus_nuclear.dropna().tolist()}\")\n",
    "\n",
    "    # 2) Work on full line/trafo tables from the deep-copied network\n",
    "    lines_existing = network_prod_mix.lines.copy()\n",
    "    trafo_existing = network_prod_mix.transformers.copy()\n",
    "\n",
    "    # 3) Align dtypes for membership tests using auxiliary numeric columns\n",
    "    def _add_numeric_bus_cols(df):\n",
    "        if df is None or getattr(df, \"empty\", True):\n",
    "            return df\n",
    "        for col in (\"bus0\", \"bus1\"):\n",
    "            if col in df.columns:\n",
    "                df[col + \"_num\"] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int64\")\n",
    "        return df\n",
    "\n",
    "    lines_existing = _add_numeric_bus_cols(lines_existing)\n",
    "    trafo_existing = _add_numeric_bus_cols(trafo_existing)\n",
    "\n",
    "    # 4) Filter lines and transformers touching nuclear buses\n",
    "    ids_lines_nuclear = []\n",
    "    if lines_existing is not None and not lines_existing.empty:\n",
    "        mask_lines = (\n",
    "            lines_existing.get(\"bus0_num\").isin(ids_bus_nuclear)\n",
    "            | lines_existing.get(\"bus1_num\").isin(ids_bus_nuclear)\n",
    "        )\n",
    "        ids_lines_nuclear = lines_existing.index[mask_lines.fillna(False)].tolist()\n",
    "\n",
    "    ids_trafo_nuclear = []\n",
    "    if trafo_existing is not None and not trafo_existing.empty:\n",
    "        mask_trafos = (\n",
    "            trafo_existing.get(\"bus0_num\").isin(ids_bus_nuclear)\n",
    "            | trafo_existing.get(\"bus1_num\").isin(ids_bus_nuclear)\n",
    "        )\n",
    "        ids_trafo_nuclear = trafo_existing.index[mask_trafos.fillna(False)].tolist()\n",
    "\n",
    "    logging.info(f\"lines hitting nuclear buses: {ids_lines_nuclear}\")\n",
    "    logging.info(f\"trafos hitting nuclear buses: {ids_trafo_nuclear}\")\n",
    "\n",
    "    # 5) Remove lines and transformers first, then buses (string IDs for safety)\n",
    "    i_l, i_t, i_b = 0,0,0\n",
    "    for l in ids_lines_nuclear:\n",
    "        network_prod_mix.remove(\"Line\", str(l))\n",
    "        i_l+=1\n",
    "    for t in ids_trafo_nuclear:\n",
    "        network_prod_mix.remove(\"Transformer\", str(t))\n",
    "        i_t+=1\n",
    "    for b in ids_bus_nuclear.dropna().tolist():\n",
    "        network_prod_mix.remove(\"Bus\", str(int(b)))\n",
    "        i_b+=1\n",
    "\n",
    "    logging.info(f\"{i_l} Lines removes\")\n",
    "    logging.info(f\"{i_t} Transformers removed\")\n",
    "    logging.info(f\"{i_b} Buses removed\")\n",
    "    logging.info(\"Finished removing nuclear assets.\")\n",
    "    return network_prod_mix\n",
    "\n",
    "network_prod_mix = remove_nuclear_assets(network_nuclear, dirs)\n",
    "networks_dict[\"network_prod_mix\"]= network_prod_mix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1324f39a",
   "metadata": {},
   "source": [
    "Now we remove the expansion assets to reflect the current status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "b0c69c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_expansion_assets(\n",
    "    network_in,\n",
    "    dirs,\n",
    "    *,\n",
    "    buses_csv=\"EC_buses_expansion.csv\",\n",
    "    lines_csv=\"EC_lines_expansion.csv\",\n",
    "    trafos_csv=\"EC_trafo_expansion.csv\",\n",
    "    bus_id_col=\"Bus\",\n",
    "    line_id_col=\"ID\",\n",
    "    trafo_id_col=\"ID\",\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Deep-copy the network, then remove expansion assets:\n",
    "    - Buses listed in buses_csv (column bus_id_col)\n",
    "    - Lines listed in lines_csv (column line_id_col)\n",
    "    - Transformers listed in trafos_csv (column trafo_id_col)\n",
    "\n",
    "    Logging:\n",
    "    - Logs IDs found in each CSV\n",
    "    - Logs how many of each component were removed\n",
    "\n",
    "    Returns\n",
    "    - network_base: cleaned network (deep copy of input)\n",
    "    \"\"\"\n",
    "    network_base = copy.deepcopy(network_in)\n",
    "\n",
    "    base_dir = dirs[\"data/raw/networks\"]\n",
    "    bus_path = join(base_dir, buses_csv)\n",
    "    line_path = join(base_dir, lines_csv)\n",
    "    trafo_path = join(base_dir, trafos_csv)\n",
    "\n",
    "    # Read CSVs\n",
    "    bus_df = pd.read_csv(bus_path)\n",
    "    line_df = pd.read_csv(line_path)\n",
    "    trafo_df = pd.read_csv(trafo_path)\n",
    "\n",
    "    # Extract IDs, coerce as needed\n",
    "    bus_ids = pd.Series([], dtype=object)\n",
    "    if bus_id_col in bus_df.columns:\n",
    "        # buses often numeric in expansion files; keep robust casting\n",
    "        bus_ids = pd.to_numeric(bus_df[bus_id_col], errors=\"coerce\").dropna().astype(\"Int64\")\n",
    "\n",
    "    line_ids = pd.Series([], dtype=object)\n",
    "    if line_id_col in line_df.columns:\n",
    "        line_ids = line_df[line_id_col].dropna().astype(str)\n",
    "\n",
    "    trafo_ids = pd.Series([], dtype=object)\n",
    "    if trafo_id_col in trafo_df.columns:\n",
    "        trafo_ids = trafo_df[trafo_id_col].dropna().astype(str)\n",
    "\n",
    "    logging.info(f\"expansion buses to remove (count={len(bus_ids)}): {bus_ids.dropna().tolist()}\")\n",
    "    logging.info(f\"expansion lines to remove (count={len(line_ids)}): {line_ids.tolist()}\")\n",
    "    logging.info(f\"expansion transformers to remove (count={len(trafo_ids)}): {trafo_ids.tolist()}\")\n",
    "\n",
    "    # Remove lines and transformers first, then buses\n",
    "    removed_lines = 0\n",
    "    for l in line_ids.tolist():\n",
    "        try:\n",
    "            network_base.remove(\"Line\", str(l))\n",
    "            removed_lines += 1\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    removed_trafos = 0\n",
    "    for t in trafo_ids.tolist():\n",
    "        try:\n",
    "            network_base.remove(\"Transformer\", str(t))\n",
    "            removed_trafos += 1\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    removed_buses = 0\n",
    "    for b in bus_ids.dropna().tolist():\n",
    "        try:\n",
    "            network_base.remove(\"Bus\", str(int(b)))\n",
    "            removed_buses += 1\n",
    "        except Exception:\n",
    "            # fall back to raw string if needed\n",
    "            try:\n",
    "                network_base.remove(\"Bus\", str(b))\n",
    "                removed_buses += 1\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    logging.info(f\"{removed_lines} Lines removed\")\n",
    "    logging.info(f\"{removed_trafos} Transformers removed\")\n",
    "    logging.info(f\"{removed_buses} Buses removed\")\n",
    "    logging.info(\"Finished removing expansion assets.\")\n",
    "\n",
    "    return network_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "a149e3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_base = remove_expansion_assets(network_prod_mix, dirs)\n",
    "networks_dict[\"network_base\"]= network_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "b4b46e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_parallel</th>\n",
       "      <th>v_nom</th>\n",
       "      <th>bus0</th>\n",
       "      <th>bus1</th>\n",
       "      <th>length</th>\n",
       "      <th>dc</th>\n",
       "      <th>geometry</th>\n",
       "      <th>bounds</th>\n",
       "      <th>carrier</th>\n",
       "      <th>type</th>\n",
       "      <th>...</th>\n",
       "      <th>v_ang_max</th>\n",
       "      <th>sub_network</th>\n",
       "      <th>x_pu</th>\n",
       "      <th>r_pu</th>\n",
       "      <th>g_pu</th>\n",
       "      <th>b_pu</th>\n",
       "      <th>x_pu_eff</th>\n",
       "      <th>r_pu_eff</th>\n",
       "      <th>s_nom_opt</th>\n",
       "      <th>v_nom_raw</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Line</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fix_280_91</th>\n",
       "      <td>1.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>280</td>\n",
       "      <td>91</td>\n",
       "      <td>9.66956</td>\n",
       "      <td>False</td>\n",
       "      <td>LINESTRING (-78.4113 -0.3306, -78.3299 -0.3)</td>\n",
       "      <td>abc</td>\n",
       "      <td>AC</td>\n",
       "      <td>243-AL1/39-ST1A 20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>inf</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>138.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            num_parallel  v_nom bus0 bus1   length     dc  \\\n",
       "Line                                                        \n",
       "fix_280_91           1.0  138.0  280   91  9.66956  False   \n",
       "\n",
       "                                                geometry bounds carrier  \\\n",
       "Line                                                                      \n",
       "fix_280_91  LINESTRING (-78.4113 -0.3306, -78.3299 -0.3)    abc      AC   \n",
       "\n",
       "                            type  ...  v_ang_max  sub_network  x_pu  r_pu  \\\n",
       "Line                              ...                                       \n",
       "fix_280_91  243-AL1/39-ST1A 20.0  ...        inf                0.0   0.0   \n",
       "\n",
       "            g_pu  b_pu  x_pu_eff  r_pu_eff  s_nom_opt  v_nom_raw  \n",
       "Line                                                              \n",
       "fix_280_91   0.0   0.0       0.0       0.0        0.0      138.0  \n",
       "\n",
       "[1 rows x 36 columns]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_base.lines[network_base.lines[\"bus0\"]==\"280\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1c7482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "cb4e613e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Network  Buses  Lines  Transformers  Generators  Loads\n",
      "0             network_original    313    247            62           0      0\n",
      "1              network_snapped    313    247            62           0      0\n",
      "2             network_expanded    336    263            77           0      0\n",
      "3  network_expanded_no_orphans    319    265            77           0      0\n",
      "4              network_nuclear    319    265            77           0      0\n",
      "5             network_prod_mix    317    263            75           0      0\n",
      "6                 network_base    296    249            62           0      0\n"
     ]
    }
   ],
   "source": [
    "#compare the shape of the tee networks for buses, lines and trafos\n",
    "import pandas as pd\n",
    "\n",
    "# Define a helper to get basic network element counts\n",
    "def net_summary(net, name):\n",
    "    return {\n",
    "        \"Network\": name,\n",
    "        \"Buses\": len(net.buses),\n",
    "        \"Lines\": len(net.lines),\n",
    "        \"Transformers\": len(net.transformers),\n",
    "        \"Generators\": len(net.generators),\n",
    "        \"Loads\": len(net.loads),\n",
    "        #\"Links\": len(net.links)\n",
    "    }\n",
    "\n",
    "# Build a comparison table\n",
    "summary = pd.DataFrame([\n",
    "    net_summary(v, k) for k,v in networks_dict.items()\n",
    "])\n",
    "\n",
    "# Display the result\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "c9d5c2c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unsupported dtype for netCDF4 variable: bool",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[289]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m networks_dict.items():\n\u001b[32m     12\u001b[39m     filename = join(output_path, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.nc\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexport_to_netcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m exported to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fpenaherrera_vaca\\.conda\\envs\\pypsa-earth-ec\\Lib\\site-packages\\pypsa\\io.py:815\u001b[39m, in \u001b[36mexport_to_netcdf\u001b[39m\u001b[34m(network, path, export_standard_types, compression, float32)\u001b[39m\n\u001b[32m    775\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    776\u001b[39m \u001b[33;03mExport network and components to a netCDF file.\u001b[39;00m\n\u001b[32m    777\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    812\u001b[39m \u001b[33;03m>>> network.export_to_netcdf(\"my_file.nc\")\u001b[39;00m\n\u001b[32m    813\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    814\u001b[39m basename = os.path.basename(path) \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m815\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mExporterNetCDF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexporter\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    816\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_export_to_exporter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    817\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    818\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexporter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    819\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbasename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbasename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    820\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexport_standard_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexport_standard_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    821\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexporter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mds\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fpenaherrera_vaca\\.conda\\envs\\pypsa-earth-ec\\Lib\\site-packages\\pypsa\\io.py:80\u001b[39m, in \u001b[36mImpExper.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\n\u001b[32m     74\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     75\u001b[39m     exc_type: \u001b[38;5;28mtype\u001b[39m,\n\u001b[32m     76\u001b[39m     exc_val: \u001b[38;5;167;01mBaseException\u001b[39;00m,\n\u001b[32m     77\u001b[39m     exc_tb: TracebackType,\n\u001b[32m     78\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfinish\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     83\u001b[39m         \u001b[38;5;28mself\u001b[39m.ds.\u001b[34m__exit__\u001b[39m(exc_type, exc_val, exc_tb)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fpenaherrera_vaca\\.conda\\envs\\pypsa-earth-ec\\Lib\\site-packages\\pypsa\\io.py:478\u001b[39m, in \u001b[36mExporterNetCDF.finish\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    476\u001b[39m _path = Path(\u001b[38;5;28mself\u001b[39m.path)\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _path.open(\u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_netcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fpenaherrera_vaca\\.conda\\envs\\pypsa-earth-ec\\Lib\\site-packages\\xarray\\core\\dataset.py:2380\u001b[39m, in \u001b[36mDataset.to_netcdf\u001b[39m\u001b[34m(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf, auto_complex)\u001b[39m\n\u001b[32m   2377\u001b[39m     encoding = {}\n\u001b[32m   2378\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxarray\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_netcdf\n\u001b[32m-> \u001b[39m\u001b[32m2380\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_netcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]  # mypy cannot resolve the overloads:(\u001b[39;49;00m\n\u001b[32m   2381\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2382\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2384\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2386\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2387\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2388\u001b[39m \u001b[43m    \u001b[49m\u001b[43munlimited_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43munlimited_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2390\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultifile\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2391\u001b[39m \u001b[43m    \u001b[49m\u001b[43minvalid_netcdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43minvalid_netcdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2392\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauto_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauto_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2393\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fpenaherrera_vaca\\.conda\\envs\\pypsa-earth-ec\\Lib\\site-packages\\xarray\\backends\\api.py:1928\u001b[39m, in \u001b[36mto_netcdf\u001b[39m\u001b[34m(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf, auto_complex)\u001b[39m\n\u001b[32m   1923\u001b[39m \u001b[38;5;66;03m# TODO: figure out how to refactor this logic (here and in save_mfdataset)\u001b[39;00m\n\u001b[32m   1924\u001b[39m \u001b[38;5;66;03m# to avoid this mess of conditionals\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1926\u001b[39m     \u001b[38;5;66;03m# TODO: allow this work (setting up the file for writing array data)\u001b[39;00m\n\u001b[32m   1927\u001b[39m     \u001b[38;5;66;03m# to be parallelized with dask\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1928\u001b[39m     \u001b[43mdump_to_store\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1929\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlimited_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43munlimited_dims\u001b[49m\n\u001b[32m   1930\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m autoclose:\n\u001b[32m   1932\u001b[39m         store.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fpenaherrera_vaca\\.conda\\envs\\pypsa-earth-ec\\Lib\\site-packages\\xarray\\backends\\api.py:1975\u001b[39m, in \u001b[36mdump_to_store\u001b[39m\u001b[34m(dataset, store, writer, encoder, encoding, unlimited_dims)\u001b[39m\n\u001b[32m   1972\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encoder:\n\u001b[32m   1973\u001b[39m     variables, attrs = encoder(variables, attrs)\n\u001b[32m-> \u001b[39m\u001b[32m1975\u001b[39m \u001b[43mstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_encoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlimited_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43munlimited_dims\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fpenaherrera_vaca\\.conda\\envs\\pypsa-earth-ec\\Lib\\site-packages\\xarray\\backends\\common.py:458\u001b[39m, in \u001b[36mAbstractWritableDataStore.store\u001b[39m\u001b[34m(self, variables, attributes, check_encoding_set, writer, unlimited_dims)\u001b[39m\n\u001b[32m    456\u001b[39m \u001b[38;5;28mself\u001b[39m.set_attributes(attributes)\n\u001b[32m    457\u001b[39m \u001b[38;5;28mself\u001b[39m.set_dimensions(variables, unlimited_dims=unlimited_dims)\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mset_variables\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_encoding_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlimited_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43munlimited_dims\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fpenaherrera_vaca\\.conda\\envs\\pypsa-earth-ec\\Lib\\site-packages\\xarray\\backends\\common.py:496\u001b[39m, in \u001b[36mAbstractWritableDataStore.set_variables\u001b[39m\u001b[34m(self, variables, check_encoding_set, writer, unlimited_dims)\u001b[39m\n\u001b[32m    494\u001b[39m name = _encode_variable_name(vn)\n\u001b[32m    495\u001b[39m check = vn \u001b[38;5;129;01min\u001b[39;00m check_encoding_set\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m target, source = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare_variable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlimited_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43munlimited_dims\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    500\u001b[39m writer.add(source, target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fpenaherrera_vaca\\.conda\\envs\\pypsa-earth-ec\\Lib\\site-packages\\xarray\\backends\\netCDF4_.py:553\u001b[39m, in \u001b[36mNetCDF4DataStore.prepare_variable\u001b[39m\u001b[34m(self, name, variable, check_encoding, unlimited_dims)\u001b[39m\n\u001b[32m    551\u001b[39m fill_value = attrs.pop(\u001b[33m\"\u001b[39m\u001b[33m_FillValue\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    552\u001b[39m datatype: np.dtype | ncEnumType | h5EnumType\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m datatype = \u001b[43m_get_datatype\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_on_invalid_encoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_encoding\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# check enum metadata and use netCDF4.EnumType\u001b[39;00m\n\u001b[32m    557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    558\u001b[39m     (meta := np.dtype(datatype).metadata)\n\u001b[32m    559\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (e_name := meta.get(\u001b[33m\"\u001b[39m\u001b[33menum_name\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m    560\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (e_dict := meta.get(\u001b[33m\"\u001b[39m\u001b[33menum\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m    561\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fpenaherrera_vaca\\.conda\\envs\\pypsa-earth-ec\\Lib\\site-packages\\xarray\\backends\\netCDF4_.py:151\u001b[39m, in \u001b[36m_get_datatype\u001b[39m\u001b[34m(var, nc_format, raise_on_invalid_encoding)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_datatype\u001b[39m(\n\u001b[32m    148\u001b[39m     var, nc_format=\u001b[33m\"\u001b[39m\u001b[33mNETCDF4\u001b[39m\u001b[33m\"\u001b[39m, raise_on_invalid_encoding=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    149\u001b[39m ) -> np.dtype:\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nc_format == \u001b[33m\"\u001b[39m\u001b[33mNETCDF4\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nc4_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m var.encoding:\n\u001b[32m    153\u001b[39m         encoded_dtype = var.encoding[\u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fpenaherrera_vaca\\.conda\\envs\\pypsa-earth-ec\\Lib\\site-packages\\xarray\\backends\\netCDF4_.py:172\u001b[39m, in \u001b[36m_nc4_dtype\u001b[39m\u001b[34m(var)\u001b[39m\n\u001b[32m    170\u001b[39m     dtype = var.dtype\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33munsupported dtype for netCDF4 variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dtype\n",
      "\u001b[31mValueError\u001b[39m: unsupported dtype for netCDF4 variable: bool"
     ]
    }
   ],
   "source": [
    "#drop them as nc objects so they can be loaded later\n",
    "# Save networks as NetCDF files (recommended PyPSA format)\n",
    "import os\n",
    "\n",
    "output_path = dirs[\"data/processed/networks\"]\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Save each network to NetCDF in the output folder\n",
    "for k,v in networks_dict.items():\n",
    "    filename = join(output_path, f\"{k}.nc\")\n",
    "    v.export_to_netcdf(filename)\n",
    "    logging.info(f\"{k} exported to {filename} \")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
