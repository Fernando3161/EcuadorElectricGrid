{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddf1720a",
   "metadata": {},
   "source": [
    "#Ecuadorian Network\n",
    "Evaluation of data from Pypsa-Earth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35ef2316",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot find header.dxf (GDAL_DATA is not defined)\n",
      "c:\\Users\\fpenaherrera_vaca\\.conda\\envs\\pypsa-earth-ec\\Lib\\site-packages\\xarray\\backends\\plugins.py:110: RuntimeWarning:\n",
      "\n",
      "Engine 'cfgrib' loading failed:\n",
      "Cannot find the ecCodes library\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network loaded successfully from: c:\\Repositories\\Repos\\pypsa-earth-project\\EcuadorElectricGrid\\data\\raw\\networks\\base.nc\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Setup environment and load the base PyPSA-Earth network for a specified country.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import pypsa\n",
    "import warnings\n",
    "import pypsa\n",
    "from pathlib import Path\n",
    "import os\n",
    "import copy\n",
    "from os.path import join\n",
    "\n",
    "# Import all dirs\n",
    "parent_dir = Path(os.getcwd()).parents[0]\n",
    "sys.path.append(str(parent_dir))\n",
    "from src.paths import all_dirs\n",
    "\n",
    "dirs = all_dirs()\n",
    "\n",
    "\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "LOG_FILE = join(parent_dir, \"logs.log\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s: %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    handlers=[logging.FileHandler(LOG_FILE, encoding=\"utf-8\")],\n",
    ")\n",
    "\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "# Suppress unnecessary warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "# Define country parameters\n",
    "country_code = \"EC\"  # ISO 2-letter code (e.g., 'GH' for Ghana, 'CO' for Colombia)\n",
    "country_name = \"Ecuador\"  # Country name\n",
    "country_gadm = \"ECU\"  # ISO 3-letter GADM code\n",
    "\n",
    "# Load the base network file path\n",
    "network_dir = dirs[\"data/raw/networks\"]\n",
    "network_file = \"base.nc\"\n",
    "network_path = os.path.join(network_dir, network_file)\n",
    "\n",
    "# Load the PyPSA network\n",
    "network = pypsa.Network(network_path)\n",
    "network_original = copy.deepcopy(network)\n",
    "\n",
    "## Dictionary with all the networks\n",
    "networks_dict = {\n",
    "    \"network_original\": network_original\n",
    "}\n",
    "\n",
    "print(f\"Network loaded successfully from: {network_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b306835b",
   "metadata": {},
   "source": [
    "The values of voltage differ from the nominal values from Ecuador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0edcca2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[buses] Updated 7 entries in v_nom:\n",
      "[buses] Updated 7 entries in v_nom (summary failed: Grouper for 'v_nom' not 1-dimensional)\n",
      "[lines] Updated 5 entries in v_nom:\n",
      "[lines] Updated 5 entries in v_nom (summary failed: Grouper for 'v_nom' not 1-dimensional)\n",
      "\n",
      "Final voltage levels:\n",
      "  buses : [ 48.  69. 138. 230. 500.]\n",
      "  lines : [ 48.  69. 138. 230. 500.]\n"
     ]
    }
   ],
   "source": [
    "def snap_voltages_to_ecuador_levels(network):\n",
    "    \"\"\"\n",
    "    Snap nominal voltages (v_nom) in a PyPSA network to Ecuador's standard levels.\n",
    "\n",
    "    Ecuador standard transmission voltages: 500, 230, 138, 69, 48 kV.\n",
    "\n",
    "    The function modifies the PyPSA network *in place*:\n",
    "      - network.buses.v_nom\n",
    "      - network.lines.v_nom\n",
    "\n",
    "    It adds backup columns 'v_nom_raw' to preserve original values.\n",
    "    Prints a short summary of all changes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network : pypsa.Network\n",
    "        The PyPSA network object to modify.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    network : pypsa.Network\n",
    "        The same network object (modified in place, returned for chaining).\n",
    "    \"\"\"\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    VALID_VOLTAGES_KV = np.array([500.0, 230.0, 138.0, 69.0, 48.0])\n",
    "\n",
    "    def _nearest_valid_voltage(values):\n",
    "        vals = np.asarray(values, dtype=float)\n",
    "        mask = ~np.isnan(vals)\n",
    "        snapped = np.full_like(vals, np.nan, dtype=float)\n",
    "        diffs = np.abs(vals[mask, None] - VALID_VOLTAGES_KV[None, :])\n",
    "        snapped[mask] = VALID_VOLTAGES_KV[np.argmin(diffs, axis=1)]\n",
    "        return snapped\n",
    "\n",
    "    def _apply_snap(df, label):\n",
    "        if \"v_nom\" not in df.columns:\n",
    "            print(f\"[WARN] '{label}' has no 'v_nom' column. Skipping.\")\n",
    "            return\n",
    "\n",
    "        if \"v_nom_raw\" not in df.columns:\n",
    "            df[\"v_nom_raw\"] = df[\"v_nom\"].copy()\n",
    "\n",
    "        before = df[\"v_nom\"].copy()\n",
    "        df[\"v_nom\"] = _nearest_valid_voltage(df[\"v_nom\"].values)\n",
    "\n",
    "        changed = before != df[\"v_nom\"]\n",
    "        n_changed = int(changed.sum())\n",
    "\n",
    "        if n_changed:\n",
    "            try: \n",
    "                print(f\"[{label}] Updated {n_changed} entries in v_nom:\")\n",
    "                summary = (\n",
    "                    pd.concat([before[changed], df[\"v_nom\"][changed]], axis=1)\n",
    "                    .rename(columns={0: \"old\", 1: \"new\"})\n",
    "                    .value_counts()\n",
    "                    .rename(\"count\")\n",
    "                )\n",
    "                for (old, new), cnt in summary.items():\n",
    "                    print(f\"  {old:.0f} â†’ {new:.0f} kV : {cnt}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[{label}] Updated {n_changed} entries in v_nom (summary failed: {e})\")\n",
    "        else:\n",
    "            print(f\"[{label}] No changes needed.\")\n",
    "\n",
    "    # --- Apply to main components ---\n",
    "    _apply_snap(network.buses, \"buses\")\n",
    "    _apply_snap(network.lines, \"lines\")\n",
    "\n",
    "    # Optional: extend here if needed\n",
    "    # if hasattr(network, \"transformers\"):\n",
    "    #     _apply_snap(network.transformers, \"transformers\")\n",
    "\n",
    "    # --- Summary ---\n",
    "    print(\"\\nFinal voltage levels:\")\n",
    "    print(\"  buses :\", np.sort(network.buses.v_nom.unique()))\n",
    "    print(\"  lines :\", np.sort(network.lines.v_nom.unique()))\n",
    "\n",
    "    return network\n",
    "\n",
    "# Apply voltage snapping\n",
    "network_snapped = snap_voltages_to_ecuador_levels(network)\n",
    "networks_dict[\"network_snapped\"] = network_snapped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6015f36b",
   "metadata": {},
   "source": [
    "The new components from the Master Plan and from the Expansion plans need to be added.\n",
    "Additionally, appropiate buses for the nuclear option need to be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a487218f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expansion merged: +23 buses, +16 lines, +15 transformers; updated 0 buses, 0 lines, 0 transformers.\n"
     ]
    }
   ],
   "source": [
    "# Copy network and merge expansion data for buses, lines, and transformers using madd\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Make a safe copy of the network already loaded as `network`\n",
    "network_exp = copy.deepcopy(network_snapped)\n",
    "\n",
    "# Base dir for expansion CSVs using your dirs mapping\n",
    "DATA_DIR = dirs[\"data/raw/networks\"]\n",
    "\n",
    "# CSV file paths\n",
    "BUS_EXP_CSV = os.path.join(DATA_DIR, \"EC_buses_expansion.csv\")\n",
    "LINE_EXP_CSV = os.path.join(DATA_DIR, \"EC_lines_expansion.csv\")\n",
    "TRAFO_EXP_CSV = os.path.join(DATA_DIR, \"EC_trafo_expansion.csv\")\n",
    "\n",
    "\n",
    "def _filter_to_allowed_attrs(nw, component, df):\n",
    "    helper_dict = {\n",
    "        \"Bus\": nw.buses,\n",
    "        \"Line\": nw.lines,\n",
    "        \"Transformer\": nw.transformers,\n",
    "    }\n",
    "    allowed = set(helper_dict[component].columns)\n",
    "    keep_cols = [c for c in df.columns if c in allowed]\n",
    "    return df[keep_cols]\n",
    "\n",
    "\n",
    "def _update_existing(nw, component, df):\n",
    "    table_map = {\n",
    "        \"Bus\": \"buses\",\n",
    "        \"Line\": \"lines\",\n",
    "        \"Transformer\": \"transformers\",\n",
    "    }\n",
    "    table = getattr(nw, table_map[component])\n",
    "    existing = df.index.intersection(table.index)\n",
    "    if len(existing) > 0:\n",
    "        cols = df.columns\n",
    "        table.loc[existing, cols] = df.loc[existing, cols]\n",
    "    return existing\n",
    "\n",
    "\n",
    "def _madd_new(nw, component, df):\n",
    "    if len(df) == 0:\n",
    "        return\n",
    "    # df must be indexed by component names and include only valid attrs\n",
    "    names = df.index\n",
    "    kwargs = {col: df[col] for col in df.columns}\n",
    "    nw.madd(component, names, **kwargs)\n",
    "\n",
    "\n",
    "# 1) Buses expansion\n",
    "bus_df = pd.read_csv(BUS_EXP_CSV)\n",
    "if \"Name\" not in bus_df.columns:\n",
    "    raise ValueError(\"EC_buses_expansion.csv must contain a 'Name' column.\")\n",
    "bus_df = bus_df.set_index(\"Bus\")\n",
    "bus_df = _filter_to_allowed_attrs(network_exp, \"Bus\", bus_df)\n",
    "\n",
    "existing_buses = _update_existing(network_exp, \"Bus\", bus_df)\n",
    "to_add_buses = bus_df.drop(index=existing_buses, errors=\"ignore\")\n",
    "_madd_new(network_exp, \"Bus\", to_add_buses)\n",
    "\n",
    "# 2) Lines expansion\n",
    "line_df = pd.read_csv(LINE_EXP_CSV)\n",
    "if \"Line\" not in line_df.columns:\n",
    "    raise ValueError(\"EC_lines_expansion.csv must contain a 'Line' column.\")\n",
    "line_df = line_df.set_index(\"ID\")\n",
    "line_df = _filter_to_allowed_attrs(network_exp, \"Line\", line_df)\n",
    "\n",
    "existing_lines = _update_existing(network_exp, \"Line\", line_df)\n",
    "to_add_lines = line_df.drop(index=existing_lines, errors=\"ignore\")\n",
    "_madd_new(network_exp, \"Line\", to_add_lines)\n",
    "\n",
    "\n",
    "# 3) Transformers expansion\n",
    "trafo_df = pd.read_csv(TRAFO_EXP_CSV)\n",
    "if \"Transformer\" not in trafo_df.columns:\n",
    "    raise ValueError(\"EC_trafo_expansion.csv must contain a 'Transformer' column.\")\n",
    "trafo_df = trafo_df.set_index(\"ID\")\n",
    "trafo_df = _filter_to_allowed_attrs(network_exp, \"Transformer\", trafo_df)\n",
    "\n",
    "\n",
    "existing_trafos = _update_existing(network_exp, \"Transformer\", trafo_df)\n",
    "to_add_trafos = trafo_df.drop(index=existing_trafos, errors=\"ignore\")\n",
    "_madd_new(network_exp, \"Transformer\", to_add_trafos)\n",
    "\n",
    "# Summary\n",
    "print(\n",
    "    f\"Expansion merged: +{len(to_add_buses)} buses, \"\n",
    "    f\"+{len(to_add_lines)} lines, \"\n",
    "    f\"+{len(to_add_trafos)} transformers; \"\n",
    "    f\"updated {len(existing_buses)} buses, {len(existing_lines)} lines, {len(existing_trafos)} transformers.\"\n",
    ")\n",
    "# plot_buses_and_lines_by_voltage(network_exp, save_name=\"ecuador_buses_expansion.png\")\n",
    "\n",
    "networks_dict[\"network_expanded\"] = network_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968cbe8a",
   "metadata": {},
   "source": [
    "Plot only the expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd213c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orphan_buses\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bus</th>\n",
       "      <th>v_nom</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>109</td>\n",
       "      <td>48.0</td>\n",
       "      <td>-78.7385</td>\n",
       "      <td>-2.9038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>122</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-78.7498</td>\n",
       "      <td>-1.7366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>134</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-78.7659</td>\n",
       "      <td>-3.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>135</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-79.9708</td>\n",
       "      <td>-3.5235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>168</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-80.5068</td>\n",
       "      <td>-0.8724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>172</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-80.1647</td>\n",
       "      <td>-0.8570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>180</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-78.7958</td>\n",
       "      <td>1.2621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>187</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-80.0950</td>\n",
       "      <td>-0.8791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>193</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-79.6392</td>\n",
       "      <td>-3.7076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>203</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-80.2042</td>\n",
       "      <td>-2.3207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>206</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-80.2780</td>\n",
       "      <td>-1.3372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>210</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-79.4589</td>\n",
       "      <td>-1.4211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>226</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-79.1919</td>\n",
       "      <td>-2.2390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>235</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-79.4243</td>\n",
       "      <td>-4.5381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>238</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-79.6412</td>\n",
       "      <td>-3.8890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>241</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-79.6221</td>\n",
       "      <td>-2.6455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>253</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-77.8418</td>\n",
       "      <td>-0.4140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Bus  v_nom      lon     lat\n",
       "109  109   48.0 -78.7385 -2.9038\n",
       "122  122   69.0 -78.7498 -1.7366\n",
       "134  134   69.0 -78.7659 -3.8451\n",
       "135  135   69.0 -79.9708 -3.5235\n",
       "168  168   69.0 -80.5068 -0.8724\n",
       "172  172   69.0 -80.1647 -0.8570\n",
       "180  180   69.0 -78.7958  1.2621\n",
       "187  187   69.0 -80.0950 -0.8791\n",
       "193  193   69.0 -79.6392 -3.7076\n",
       "203  203   69.0 -80.2042 -2.3207\n",
       "206  206   69.0 -80.2780 -1.3372\n",
       "210  210   69.0 -79.4589 -1.4211\n",
       "226  226   69.0 -79.1919 -2.2390\n",
       "235  235   69.0 -79.4243 -4.5381\n",
       "238  238   69.0 -79.6412 -3.8890\n",
       "241  241   69.0 -79.6221 -2.6455\n",
       "253  253   69.0 -77.8418 -0.4140"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bus_voltage_levels\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[48, 69, 138, 230, 500]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trafo_pairs_found_network\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(48, 138), (69, 138), (69, 230), (69, 500), (138, 230), (230, 500)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topology_summary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'230kV': {'n_components': 63, 'sizes': [5, 4, 3, 2, 2]},\n",
       " '500kV': {'n_components': 13, 'sizes': [6, 1, 1, 1, 1]}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Sanity + topology checks for expansion elements against the existing network\n",
    "from src.ec_network_eval import evaluate_network\n",
    "\n",
    "_, issues = evaluate_network(\n",
    "    network_exp,\n",
    "    data_dir = dirs[\"data/raw/networks\"],\n",
    "    downstream_path = (500,48))\n",
    "\n",
    "for k,v in issues:\n",
    "    if \"empty\" not in k:\n",
    "        print(k)\n",
    "        display(v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89826bb9",
   "metadata": {},
   "source": [
    "The orphan buses must be cleaned. Linking them with other components may lead to incorrect strcturing of the grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3c696b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_orphan_buses(n_exp, issues, log_csv_path=\"removed_orphan_buses.csv\"):\n",
    "    \"\"\"\n",
    "    Remove buses that are truly unreferenced by any component in the network.\n",
    "    - Deep-copies the input network (does not modify original).\n",
    "    - Uses orphan candidates from issues if present.\n",
    "    - Verifies true orphans by scanning all component bus references.\n",
    "    - Optionally writes removed buses to CSV (set log_csv_path=None to skip)\n",
    "    Returns\n",
    "    - n_exp_clean: deep-copied and cleaned network\n",
    "    \"\"\"\n",
    "    # 1) Extract candidate orphan buses from issues (accepts multiple keys)\n",
    "    orphan_df = None\n",
    "    for key, data in issues:\n",
    "        if key in (\"orphan_buses\", \"orphan_buses_lines_trafos_only\"):\n",
    "            orphan_df = data\n",
    "            break\n",
    "\n",
    "    candidate_orphans = pd.Index([])\n",
    "    if orphan_df is not None and \"Bus\" in orphan_df.columns:\n",
    "        candidate_orphans = pd.Index(orphan_df[\"Bus\"].tolist())\n",
    "\n",
    "    # 2) Collect all buses referenced by any component\n",
    "    def buses_referenced_by_components(n):\n",
    "        refs = set()\n",
    "        comps = [\n",
    "            (\"loads\", n.loads),\n",
    "            (\"generators\", n.generators),\n",
    "            (\"storage_units\", n.storage_units),\n",
    "            (\"stores\", n.stores),\n",
    "            (\"links\", n.links),\n",
    "            (\"lines\", n.lines),\n",
    "            (\"transformers\", n.transformers),\n",
    "            (\"shunt_impedances\", n.shunt_impedances),\n",
    "        ]\n",
    "        for _, df in comps:\n",
    "            if df is None or getattr(df, \"empty\", True):\n",
    "                continue\n",
    "            # Any column that starts with 'bus' is considered a bus reference\n",
    "            bus_cols = [c for c in df.columns if c.startswith(\"bus\")]\n",
    "            if not bus_cols:\n",
    "                continue\n",
    "            for col in bus_cols:\n",
    "                # Normalize to string IDs, drop NaNs\n",
    "                refs |= set(df[col].dropna().astype(str))\n",
    "        return refs\n",
    "\n",
    "    ref_buses = buses_referenced_by_components(n_exp)\n",
    "    # Normalize the network bus index to string for cross-compatibility\n",
    "    bus_index_str = n_exp.buses.index.astype(str)\n",
    "    # Buses not referenced by any component\n",
    "    true_orphans_idx = n_exp.buses.loc[~bus_index_str.isin(ref_buses)].index\n",
    "\n",
    "    # 3) Intersect with candidates (if provided)\n",
    "    if len(candidate_orphans) > 0:\n",
    "        candidate_orphans = pd.Index(candidate_orphans)\n",
    "        true_orphans_idx = candidate_orphans.intersection(true_orphans_idx)\n",
    "\n",
    "    # 4) Deep-copy the network and remove the confirmed orphan buses\n",
    "    n_exp_clean = copy.deepcopy(n_exp)\n",
    "    for b in true_orphans_idx:\n",
    "        n_exp_clean.remove(\"Bus\", b)\n",
    "\n",
    "    # 5) Optional: log removed buses\n",
    "    if log_csv_path:\n",
    "        pd.DataFrame({\"removed_bus\": pd.Index(true_orphans_idx)}).to_csv(\n",
    "            log_csv_path, index=False\n",
    "        )\n",
    "\n",
    "    # 6) Sanity assert\n",
    "    for b in true_orphans_idx:\n",
    "        assert b not in n_exp_clean.buses.index, f\"Bus {b} was not removed properly.\"\n",
    "    logging.info(\"All orphan buses cleared\")\n",
    "\n",
    "    return n_exp_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904ae0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_exp_clean = clean_orphan_buses(network_exp, issues)\n",
    "\n",
    "\n",
    "\n",
    "networks_dict[\"network_expanded_no_orphans\"] =n_exp_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9234a8a7",
   "metadata": {},
   "source": [
    "Now, we need three network objects\n",
    "1) One containing only the original network structure, that is, no buses, trafos, or lines that represent the expansion\n",
    "2) One that contians the network structure as per the master plan\n",
    "3) One that contians the nuclear buses \n",
    "\n",
    "The final network is the one already containing the nuclear option, so we remove from there\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e15bcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_nuclear = copy.deepcopy(n_exp_clean)\n",
    "\n",
    "# Run sanity cheack\n",
    "def check_network(network):\n",
    "    _, issues_n = evaluate_network(\n",
    "        network,\n",
    "        data_dir = dirs[\"data/raw/networks\"],\n",
    "        downstream_path = (500,48))\n",
    "    for k,v in issues_n:\n",
    "        if \"orphan\" in k:\n",
    "            display(v)\n",
    "            raise ValueError(\"Network still has orphan buses\")\n",
    "    \n",
    "    logging.info(\"Network has no orphan buses\")\n",
    "\n",
    "check_network(network_nuclear)\n",
    "\n",
    "networks_dict[\"network_nuclear\"]= network_nuclear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826c21ab",
   "metadata": {},
   "source": [
    "Remove the nuclear buses and related components to get only the \"productive mix\" network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77db757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "\n",
    "def remove_nuclear_assets(network_nuclear, dirs, buses_csv=\"EC_buses_expansion.csv\"):\n",
    "    \"\"\"\n",
    "    Deep-copy the network, then remove:\n",
    "    - all buses whose Name contains 'nuclear' (case-insensitive) from the expansion buses CSV\n",
    "    - all lines/transformers connected to those buses\n",
    "\n",
    "    Logging:\n",
    "    - Logs the nuclear bus IDs found\n",
    "    - Logs the line and transformer IDs that are removed\n",
    "\n",
    "    Returns\n",
    "    - network_prod_mix: cleaned network (deep copy of input)\n",
    "    \"\"\"\n",
    "    network_prod_mix = copy.deepcopy(network_nuclear)\n",
    "\n",
    "    # 1) Nuclear bus IDs (robust to NaNs/whitespace; read from expansion CSV)\n",
    "    bus_csv_path = join(dirs[\"data/raw/networks\"], buses_csv)\n",
    "    bus_new = pd.read_csv(bus_csv_path)\n",
    "    mask_nuc = (\n",
    "        bus_new[\"Name\"].astype(str).str.strip().str.lower().str.contains(\"nuclear\", na=False)\n",
    "    )\n",
    "    ids_bus_nuclear = (\n",
    "        pd.to_numeric(bus_new.loc[mask_nuc, \"Bus\"], errors=\"coerce\")\n",
    "        .dropna()\n",
    "        .astype(\"Int64\")\n",
    "    )\n",
    "    logging.info(f\"nuclear buses: {ids_bus_nuclear.dropna().tolist()}\")\n",
    "\n",
    "    # 2) Work on full line/trafo tables from the deep-copied network\n",
    "    lines_existing = network_prod_mix.lines.copy()\n",
    "    trafo_existing = network_prod_mix.transformers.copy()\n",
    "\n",
    "    # 3) Align dtypes for membership tests using auxiliary numeric columns\n",
    "    def _add_numeric_bus_cols(df):\n",
    "        if df is None or getattr(df, \"empty\", True):\n",
    "            return df\n",
    "        for col in (\"bus0\", \"bus1\"):\n",
    "            if col in df.columns:\n",
    "                df[col + \"_num\"] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int64\")\n",
    "        return df\n",
    "\n",
    "    lines_existing = _add_numeric_bus_cols(lines_existing)\n",
    "    trafo_existing = _add_numeric_bus_cols(trafo_existing)\n",
    "\n",
    "    # 4) Filter lines and transformers touching nuclear buses\n",
    "    ids_lines_nuclear = []\n",
    "    if lines_existing is not None and not lines_existing.empty:\n",
    "        mask_lines = (\n",
    "            lines_existing.get(\"bus0_num\").isin(ids_bus_nuclear)\n",
    "            | lines_existing.get(\"bus1_num\").isin(ids_bus_nuclear)\n",
    "        )\n",
    "        ids_lines_nuclear = lines_existing.index[mask_lines.fillna(False)].tolist()\n",
    "\n",
    "    ids_trafo_nuclear = []\n",
    "    if trafo_existing is not None and not trafo_existing.empty:\n",
    "        mask_trafos = (\n",
    "            trafo_existing.get(\"bus0_num\").isin(ids_bus_nuclear)\n",
    "            | trafo_existing.get(\"bus1_num\").isin(ids_bus_nuclear)\n",
    "        )\n",
    "        ids_trafo_nuclear = trafo_existing.index[mask_trafos.fillna(False)].tolist()\n",
    "\n",
    "    logging.info(f\"lines hitting nuclear buses: {ids_lines_nuclear}\")\n",
    "    logging.info(f\"trafos hitting nuclear buses: {ids_trafo_nuclear}\")\n",
    "\n",
    "    # 5) Remove lines and transformers first, then buses (string IDs for safety)\n",
    "    i_l, i_t, i_b = 0,0,0\n",
    "    for l in ids_lines_nuclear:\n",
    "        network_prod_mix.remove(\"Line\", str(l))\n",
    "        i_l+=1\n",
    "    for t in ids_trafo_nuclear:\n",
    "        network_prod_mix.remove(\"Transformer\", str(t))\n",
    "        i_t+=1\n",
    "    for b in ids_bus_nuclear.dropna().tolist():\n",
    "        network_prod_mix.remove(\"Bus\", str(int(b)))\n",
    "        i_b+=1\n",
    "\n",
    "    logging.info(f\"{i_l} Lines removes\")\n",
    "    logging.info(f\"{i_t} Transformers removed\")\n",
    "    logging.info(f\"{i_b} Buses removed\")\n",
    "    logging.info(\"Finished removing nuclear assets.\")\n",
    "    return network_prod_mix\n",
    "\n",
    "network_prod_mix = remove_nuclear_assets(network_nuclear, dirs)\n",
    "networks_dict[\"network_prod_mix\"]= network_prod_mix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1324f39a",
   "metadata": {},
   "source": [
    "Now we remove the expansion assets to reflect the current status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0c69c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_expansion_assets(\n",
    "    network_in,\n",
    "    dirs,\n",
    "    *,\n",
    "    buses_csv=\"EC_buses_expansion.csv\",\n",
    "    lines_csv=\"EC_lines_expansion.csv\",\n",
    "    trafos_csv=\"EC_trafo_expansion.csv\",\n",
    "    bus_id_col=\"Bus\",\n",
    "    line_id_col=\"ID\",\n",
    "    trafo_id_col=\"ID\",\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Deep-copy the network, then remove expansion assets:\n",
    "    - Buses listed in buses_csv (column bus_id_col)\n",
    "    - Lines listed in lines_csv (column line_id_col)\n",
    "    - Transformers listed in trafos_csv (column trafo_id_col)\n",
    "\n",
    "    Logging:\n",
    "    - Logs IDs found in each CSV\n",
    "    - Logs how many of each component were removed\n",
    "\n",
    "    Returns\n",
    "    - network_base: cleaned network (deep copy of input)\n",
    "    \"\"\"\n",
    "    network_base = copy.deepcopy(network_in)\n",
    "\n",
    "    base_dir = dirs[\"data/raw/networks\"]\n",
    "    bus_path = join(base_dir, buses_csv)\n",
    "    line_path = join(base_dir, lines_csv)\n",
    "    trafo_path = join(base_dir, trafos_csv)\n",
    "\n",
    "    # Read CSVs\n",
    "    bus_df = pd.read_csv(bus_path)\n",
    "    line_df = pd.read_csv(line_path)\n",
    "    trafo_df = pd.read_csv(trafo_path)\n",
    "\n",
    "    # Extract IDs, coerce as needed\n",
    "    bus_ids = pd.Series([], dtype=object)\n",
    "    if bus_id_col in bus_df.columns:\n",
    "        # buses often numeric in expansion files; keep robust casting\n",
    "        bus_ids = pd.to_numeric(bus_df[bus_id_col], errors=\"coerce\").dropna().astype(\"Int64\")\n",
    "\n",
    "    line_ids = pd.Series([], dtype=object)\n",
    "    if line_id_col in line_df.columns:\n",
    "        line_ids = line_df[line_id_col].dropna().astype(str)\n",
    "\n",
    "    trafo_ids = pd.Series([], dtype=object)\n",
    "    if trafo_id_col in trafo_df.columns:\n",
    "        trafo_ids = trafo_df[trafo_id_col].dropna().astype(str)\n",
    "\n",
    "    logging.info(f\"expansion buses to remove (count={len(bus_ids)}): {bus_ids.dropna().tolist()}\")\n",
    "    logging.info(f\"expansion lines to remove (count={len(line_ids)}): {line_ids.tolist()}\")\n",
    "    logging.info(f\"expansion transformers to remove (count={len(trafo_ids)}): {trafo_ids.tolist()}\")\n",
    "\n",
    "    # Remove lines and transformers first, then buses\n",
    "    removed_lines = 0\n",
    "    for l in line_ids.tolist():\n",
    "        try:\n",
    "            network_base.remove(\"Line\", str(l))\n",
    "            removed_lines += 1\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    removed_trafos = 0\n",
    "    for t in trafo_ids.tolist():\n",
    "        try:\n",
    "            network_base.remove(\"Transformer\", str(t))\n",
    "            removed_trafos += 1\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    removed_buses = 0\n",
    "    for b in bus_ids.dropna().tolist():\n",
    "        try:\n",
    "            network_base.remove(\"Bus\", str(int(b)))\n",
    "            removed_buses += 1\n",
    "        except Exception:\n",
    "            # fall back to raw string if needed\n",
    "            try:\n",
    "                network_base.remove(\"Bus\", str(b))\n",
    "                removed_buses += 1\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    logging.info(f\"{removed_lines} Lines removed\")\n",
    "    logging.info(f\"{removed_trafos} Transformers removed\")\n",
    "    logging.info(f\"{removed_buses} Buses removed\")\n",
    "    logging.info(\"Finished removing expansion assets.\")\n",
    "\n",
    "    return network_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a149e3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_base = remove_expansion_assets(network_prod_mix, dirs)\n",
    "networks_dict[\"network_base\"]= network_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb4e613e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Network  Buses  Lines  Transformers  Generators  Loads\n",
      "0             network_original    313    247            62           0      0\n",
      "1              network_snapped    313    247            62           0      0\n",
      "2             network_expanded    336    263            77           0      0\n",
      "3  network_expanded_no_orphans    319    263            77           0      0\n",
      "4              network_nuclear    319    263            77           0      0\n",
      "5             network_prod_mix    317    261            75           0      0\n",
      "6                 network_base    296    247            62           0      0\n"
     ]
    }
   ],
   "source": [
    "#compare the shape of the tee networks for buses, lines and trafos\n",
    "import pandas as pd\n",
    "\n",
    "# Define a helper to get basic network element counts\n",
    "def net_summary(net, name):\n",
    "    return {\n",
    "        \"Network\": name,\n",
    "        \"Buses\": len(net.buses),\n",
    "        \"Lines\": len(net.lines),\n",
    "        \"Transformers\": len(net.transformers),\n",
    "        \"Generators\": len(net.generators),\n",
    "        \"Loads\": len(net.loads),\n",
    "        #\"Links\": len(net.links)\n",
    "    }\n",
    "\n",
    "# Build a comparison table\n",
    "summary = pd.DataFrame([\n",
    "    net_summary(v, k) for k,v in networks_dict.items()\n",
    "])\n",
    "\n",
    "# Display the result\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9d5c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop them as nc objects so they can be loaded later\n",
    "# Save networks as NetCDF files (recommended PyPSA format)\n",
    "import os\n",
    "\n",
    "output_path = dirs[\"data/processed/networks\"]\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Save each network to NetCDF in the output folder\n",
    "for k,v in networks_dict.items():\n",
    "    filename = join(output_path, f\"{k}.nc\")\n",
    "    v.export_to_netcdf(filename)\n",
    "    logging.info(f\"{k} exported to {filename} \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4eb7d093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['network_original', 'network_snapped', 'network_expanded', 'network_expanded_no_orphans', 'network_nuclear', 'network_prod_mix', 'network_base'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "networks_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b090cd0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 69., 230., 138.,  46., 500., 145.,  48.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_original.buses.v_nom.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf7dbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
