{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3c7457f",
   "metadata": {},
   "source": [
    "Setup environment and load the base PyPSA-Earth network for a specified country.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "842e85e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core imports\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import logging\n",
    "import warnings\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Numerical / data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "\n",
    "# --- Power systems / geospatial\n",
    "import pypsa\n",
    "import atlite\n",
    "from shapely.geometry import Point, box\n",
    "from shapely.ops import unary_union\n",
    "import shapely\n",
    "\n",
    "# Optional plotting (only used in helper plot function)\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# Optional: powerplantmatching (picks up config but we keep local CSV as source of truth)\n",
    "try:\n",
    "    import powerplantmatching as pm\n",
    "    HAVE_PPM = True\n",
    "except Exception:\n",
    "    HAVE_PPM = False\n",
    "\n",
    "# --- Silence noisy warnings\n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# --- Logging\n",
    "parent_dir = Path(os.getcwd()).parents[0]          # project/\n",
    "LOG_FILE = join(parent_dir, \"logs.log\")\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s: %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    handlers=[logging.FileHandler(LOG_FILE, encoding=\"utf-8\"), logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Project paths helper\n",
    "sys.path.append(str(parent_dir))\n",
    "from src.paths import all_dirs  # must exist in your repo\n",
    "dirs = all_dirs()\n",
    "\n",
    "# --- Add PyPSA-Earth scripts to PATH (assumes repo layout: <project>/../pypsa-earth/scripts)\n",
    "scripts_path = os.path.join(parent_dir.parents[0], \"pypsa-earth\", \"scripts\")\n",
    "assert os.path.isdir(scripts_path), f\"Path not found: {scripts_path}\"\n",
    "sys.path.append(scripts_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66b617ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-11 11:00:59: Loading .nc networks …\n",
      "2025-11-11 11:00:59: Imported network network_original.nc has buses, lines, transformers\n",
      "2025-11-11 11:00:59:   ✓ loaded network_original.nc\n",
      "2025-11-11 11:00:59: Imported network network_snapped.nc has buses, lines, transformers\n",
      "2025-11-11 11:00:59:   ✓ loaded network_snapped.nc\n",
      "2025-11-11 11:00:59: Imported network network_expanded.nc has buses, lines, transformers\n",
      "2025-11-11 11:00:59:   ✓ loaded network_expanded.nc\n",
      "2025-11-11 11:00:59: Imported network network_expanded_no_orphans.nc has buses, lines, transformers\n",
      "2025-11-11 11:00:59:   ✓ loaded network_expanded_no_orphans.nc\n",
      "2025-11-11 11:01:00: Imported network network_nuclear.nc has buses, lines, transformers\n",
      "2025-11-11 11:01:00:   ✓ loaded network_nuclear.nc\n",
      "2025-11-11 11:01:00: Imported network network_prod_mix.nc has buses, lines, transformers\n",
      "2025-11-11 11:01:00:   ✓ loaded network_prod_mix.nc\n",
      "2025-11-11 11:01:00: Imported network network_base.nc has buses, lines, transformers\n",
      "2025-11-11 11:01:00:   ✓ loaded network_base.nc\n"
     ]
    }
   ],
   "source": [
    "# 1) Load Networks\n",
    "network_dir = dirs[\"data/processed/networks\"]\n",
    "network_files = [\n",
    "    \"network_original\",\n",
    "    \"network_snapped\",\n",
    "    \"network_expanded\",\n",
    "    \"network_expanded_no_orphans\",\n",
    "    \"network_nuclear\",\n",
    "    \"network_prod_mix\",\n",
    "    \"network_base\",\n",
    "]\n",
    "\n",
    "logger.info(\"Loading .nc networks …\")\n",
    "networks_dict = {}\n",
    "for nf in network_files:\n",
    "    f = join(network_dir, f\"{nf}.nc\")\n",
    "    if os.path.isfile(f):\n",
    "        try:\n",
    "            networks_dict[nf] = pypsa.Network(f)\n",
    "            logger.info(f\"  ✓ loaded {nf}.nc\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"  ⚠ failed to load {nf}.nc: {e}\")\n",
    "    else:\n",
    "        logger.warning(f\"  ⚠ missing file: {f}\")\n",
    "\n",
    "# Work on a copy of the base network\n",
    "assert \"network_base\" in networks_dict, \"network_base.nc not found/loaded.\"\n",
    "network = networks_dict[\"network_base\"].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61b91478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-11 11:01:00: Loading load profile: c:\\Repositories\\Repos\\pypsa-earth-project\\EcuadorElectricGrid\\data\\processed\\scaled_loads\\load_base_2030_linear.csv\n",
      "2025-11-11 11:01:00: 17 load buses not found; reassigning to nearest existing bus.\n",
      "2025-11-11 11:01:00: Load bus remapping performed (missing → nearest existing):\n",
      "2025-11-11 11:01:00:   load '109' → mapped_to:65 (src_coords: -78.7385, -2.9038)\n",
      "2025-11-11 11:01:00:   load '122' → mapped_to:260 (src_coords: -78.7498, -1.7366)\n",
      "2025-11-11 11:01:00:   load '134' → mapped_to:97 (src_coords: -78.7659, -3.8451)\n",
      "2025-11-11 11:01:00:   load '135' → mapped_to:183 (src_coords: -79.9708, -3.5235)\n",
      "2025-11-11 11:01:00:   load '168' → mapped_to:169 (src_coords: -80.5068, -0.8724)\n",
      "2025-11-11 11:01:00:   load '172' → mapped_to:171 (src_coords: -80.1647, -0.857)\n",
      "2025-11-11 11:01:00:   load '180' → mapped_to:179 (src_coords: -78.7958, 1.2621)\n",
      "2025-11-11 11:01:00:   load '187' → mapped_to:176 (src_coords: -80.095, -0.8791)\n",
      "2025-11-11 11:01:00:   load '193' → mapped_to:192 (src_coords: -79.6392, -3.7076)\n",
      "2025-11-11 11:01:00:   load '203' → mapped_to:270 (src_coords: -80.2042, -2.3207)\n",
      "2025-11-11 11:01:00:   load '206' → mapped_to:167 (src_coords: -80.278, -1.3372)\n",
      "2025-11-11 11:01:00:   load '210' → mapped_to:299 (src_coords: -79.4589, -1.4211)\n",
      "2025-11-11 11:01:00:   load '226' → mapped_to:303 (src_coords: -79.1919, -2.239)\n",
      "2025-11-11 11:01:00:   load '235' → mapped_to:305 (src_coords: -79.4243, -4.5381)\n",
      "2025-11-11 11:01:00:   load '238' → mapped_to:306 (src_coords: -79.6412, -3.889)\n",
      "2025-11-11 11:01:00:   load '241' → mapped_to:204 (src_coords: -79.6221, -2.6455)\n",
      "2025-11-11 11:01:00:   load '253' → mapped_to:257 (src_coords: -77.8418, -0.414)\n",
      "2025-11-11 11:01:00: Peak load in p_set time series: 6.6 GW\n"
     ]
    }
   ],
   "source": [
    "# 2) Loads: add p_set time series\n",
    "# 2) Loads: add p_set time series with nearest-bus remapping for unknown buses\n",
    "try:\n",
    "    path_loads = dirs[\"data/processed/scaled_loads\"]\n",
    "    load_profile_name = \"load_base_2030_linear.csv\"\n",
    "    load_profile_file = join(path_loads, load_profile_name)\n",
    "    logger.info(f\"Loading load profile: {load_profile_file}\")\n",
    "    load_profile = pd.read_csv(load_profile_file, index_col=0, parse_dates=True)\n",
    "\n",
    "    # --- Ensure column names are strings\n",
    "    load_cols = [str(c) for c in load_profile.columns]\n",
    "    load_profile.columns = load_cols\n",
    "\n",
    "    # --- Existing buses in the active network\n",
    "    buses_now = network.buses.copy()\n",
    "    buses_now.index = buses_now.index.astype(str)\n",
    "    existing_buses = set(buses_now.index)\n",
    "\n",
    "    # --- Find missing bus labels\n",
    "    missing = [c for c in load_cols if c not in existing_buses]\n",
    "    if missing:\n",
    "        logger.warning(f\"{len(missing)} load buses not found; reassigning to nearest existing bus.\")\n",
    "    else:\n",
    "        logger.info(\"All load buses found in current network.\")\n",
    "\n",
    "    # --- Build KDTree on current network buses\n",
    "    from scipy.spatial import cKDTree as KDTree\n",
    "    use_lonlat = {\"lon\", \"lat\"}.issubset(buses_now.columns)\n",
    "    if use_lonlat:\n",
    "        coords_now = buses_now.loc[:, [\"lon\", \"lat\"]].to_numpy(dtype=float)\n",
    "    else:\n",
    "        coords_now = buses_now.loc[:, [\"x\", \"y\"]].to_numpy(dtype=float)\n",
    "    kdt_now = KDTree(coords_now)\n",
    "\n",
    "    # --- Helper: try to get coordinates for a missing bus from any fallback network\n",
    "    def find_bus_coords_in_fallbacks(bus_name: str):\n",
    "        for nf, net_fb in networks_dict.items():\n",
    "            try:\n",
    "                df = net_fb.buses.copy()\n",
    "                df.index = df.index.astype(str)\n",
    "                if bus_name in df.index:\n",
    "                    if {\"lon\", \"lat\"}.issubset(df.columns):\n",
    "                        return (\"lonlat\",\n",
    "                                float(df.loc[bus_name, \"lon\"]),\n",
    "                                float(df.loc[bus_name, \"lat\"]))\n",
    "                    elif {\"x\", \"y\"}.issubset(df.columns):\n",
    "                        return (\"xy\",\n",
    "                                float(df.loc[bus_name, \"x\"]),\n",
    "                                float(df.loc[bus_name, \"y\"]))\n",
    "            except Exception:\n",
    "                pass\n",
    "        return None\n",
    "\n",
    "    # --- Build a bus assignment vector aligned with load columns\n",
    "    bus_assignments = []\n",
    "    remap_report = []\n",
    "\n",
    "    for c in load_cols:\n",
    "        if c in existing_buses:\n",
    "            bus_assignments.append(c)  # unchanged\n",
    "            continue\n",
    "\n",
    "        # Try to recover coordinates for this bus label from other networks\n",
    "        info = find_bus_coords_in_fallbacks(c)\n",
    "        if info is not None:\n",
    "            kind, bx, by = info\n",
    "            # Query KDTree (always built for current network in the coordinate type we have)\n",
    "            if use_lonlat and kind == \"lonlat\":\n",
    "                q = np.array([[bx, by]])\n",
    "            elif (not use_lonlat) and kind == \"xy\":\n",
    "                q = np.array([[bx, by]])\n",
    "            else:\n",
    "                # Coordinate kind mismatch; cannot reliably transform → use crude nearest by lon/lat if both exist,\n",
    "                # else by x/y. If mismatch, try whichever exists in current network.\n",
    "                if use_lonlat and kind == \"xy\":\n",
    "                    # We only have xy for missing, but KDTree is lon/lat → fallback to closest by index 0\n",
    "                    # (transparent about this in logs)\n",
    "                    remap_report.append((c, None, None, \"coord_kind_mismatch\"))\n",
    "                    # Pick the closest *by index* fallback: use network's geometric center\n",
    "                    # (KDTree needs a point; we’ll use median lon/lat)\n",
    "                    q = np.nanmedian(coords_now, axis=0)[None, :]\n",
    "                elif (not use_lonlat) and kind == \"lonlat\":\n",
    "                    remap_report.append((c, None, None, \"coord_kind_mismatch\"))\n",
    "                    q = np.nanmedian(coords_now, axis=0)[None, :]\n",
    "                else:\n",
    "                    q = np.nanmedian(coords_now, axis=0)[None, :]\n",
    "            idx = int(kdt_now.query(q)[1][0])\n",
    "            nearest_bus = buses_now.index[idx]\n",
    "            bus_assignments.append(nearest_bus)\n",
    "            remap_report.append((c, bx, by, f\"mapped_to:{nearest_bus}\"))\n",
    "        else:\n",
    "            # No coordinates found anywhere → attach to globally nearest by network centroid\n",
    "            idx = int(kdt_now.query(np.nanmedian(coords_now, axis=0)[None, :])[1][0])\n",
    "            nearest_bus = buses_now.index[idx]\n",
    "            bus_assignments.append(nearest_bus)\n",
    "            remap_report.append((c, None, None, f\"mapped_to:{nearest_bus};reason:no_coords\"))\n",
    "\n",
    "    if remap_report:\n",
    "        logger.info(\"Load bus remapping performed (missing → nearest existing):\")\n",
    "        for r in remap_report:\n",
    "            logger.info(f\"  load '{r[0]}' → {r[3]} (src_coords: {r[1]}, {r[2]})\")\n",
    "\n",
    "    # --- Finally add Loads with reassigned buses\n",
    "    network.madd(\"Load\", load_cols, bus=bus_assignments, p_set=load_profile)\n",
    "\n",
    "    # Peak (GW)\n",
    "    peak_load = round(network.loads_t.p_set.T.sum().max() / 1000, 1)\n",
    "    logger.info(f\"Peak load in p_set time series: {peak_load} GW\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Could not attach loads: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de37d1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Repositories\\Repos\\pypsa-earth-project\\pypsa-earth\\configs\\powerplantmatching_config.yaml\n",
      "2025-11-11 11:01:00: Applied powerplantmatching normalization (fill years + to_pypsa_names).\n",
      "2025-11-11 11:01:00: Capacities [kW]\n",
      "  total:                16,076\n",
      "  connected (SNI):      15,070\n",
      "  connected <= 2017: 7,177\n",
      "  filtered (>= 10 kW): 7,022\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "#                      3) Power plants table (CSV)\n",
    "# ======================================================================\n",
    "ppl_csv = join(dirs[\"data/processed/generation\"], \"powerplants_all.csv\")\n",
    "assert os.path.isfile(ppl_csv), f\"Power plants CSV not found: {ppl_csv}\"\n",
    "ppl_raw = pd.read_csv(ppl_csv, index_col=0)\n",
    "\n",
    "# --- OPTIONAL (recommended): normalize using powerplantmatching\n",
    "ppl = ppl_raw.copy()\n",
    "try:\n",
    "    import powerplantmatching as pm\n",
    "\n",
    "    # Load PPM config (pypsa-earth config shipped in your repo)\n",
    "    ppmatching = os.path.join(\n",
    "        Path(os.getcwd()).parents[1], \"pypsa-earth\", \"configs\", \"powerplantmatching_config.yaml\"\n",
    "    )\n",
    "    print(ppmatching)\n",
    "    config = pm.get_config(ppmatching)\n",
    "    config[\"target_countries\"] = [\"EC\"]  # Ecuador\n",
    "\n",
    "    # These utilities are in pm.powerplant\n",
    "    # 1) fill missing commissioning years\n",
    "    ppl_f = ppl.powerplant.fill_missing_commissioning_years()\n",
    "    # 2) convert to PyPSA-compatible column names (bus/generator naming conventions)\n",
    "    ppl_p = ppl_f.powerplant.to_pypsa_names()\n",
    "\n",
    "        # Drop all-empty columns that may be introduced by upstream merges\n",
    "    ppl = ppl_p.dropna(axis=1, how=\"all\")\n",
    "\n",
    "    logger.info(\"Applied powerplantmatching normalization (fill years + to_pypsa_names).\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"powerplantmatching step skipped (using raw CSV): {e}\")\n",
    "    ppl = ppl_raw.copy()\n",
    "\n",
    "\n",
    "ppl.columns\n",
    "\n",
    "# ---- Normalize capacities and carriers (ES->EN) on the (possibly PPM-normalized) table\n",
    "def _normalize_text(s):\n",
    "    import unicodedata, re\n",
    "    if pd.isna(s):\n",
    "        return None\n",
    "    s = str(s).strip().lower()\n",
    "    s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "# capacity as float (handles \"1'500,00\")\n",
    "if \"p_nom\" in ppl.columns:\n",
    "    ppl[\"p_nom\"] = (\n",
    "        ppl[\"p_nom\"].astype(str).str.replace(\"'\", \"\", regex=False).str.replace(\",\", \".\", regex=False)\n",
    "    )\n",
    "    ppl[\"p_nom\"] = pd.to_numeric(ppl[\"p_nom\"], errors=\"coerce\").fillna(0.0)\n",
    "else:\n",
    "    logger.warning(\"Column 'p_nom' not found in plants table after normalization.\")\n",
    "\n",
    "# map carriers\n",
    "if \"carrier\" in ppl.columns:\n",
    "    ppl[\"carrier_norm\"] = ppl[\"carrier\"].apply(_normalize_text)\n",
    "    carrier_map_es2en = {\n",
    "        \"hidraulica\": \"hydro\",\n",
    "        \"hidroelectrico\": \"hydro\",\n",
    "        \"termica\": \"oil\",            # general fallback, could also be \"natural gas\" or \"coal\" depending on subtype\n",
    "        \"termoelectrico\": \"oil\",\n",
    "        \"biomasa\": \"biomass\",\n",
    "        \"fotovoltaica\": \"solar\",\n",
    "        \"solar\": \"solar\",\n",
    "        \"eolica\": \"onwind\",\n",
    "        \"eolico\": \"onwind\",\n",
    "        \"biogas\": \"biomass\",         # or \"biogas\" if you keep it distinct later\n",
    "        \"ernc\": \"onwind\",            # “Energía Renovable No Convencional”, usually wind/solar, adjust if needed\n",
    "        \"nuclear\": \"nuclear\",\n",
    "        \"geotermica\": \"geothermal\",\n",
    "        \"carbón\": \"coal\",\n",
    "        \"carbon\": \"coal\",\n",
    "        \"lignito\": \"lignite\",\n",
    "        \"gas natural\": \"natural gas\",\n",
    "        \"gas\": \"natural gas\",\n",
    "        \"phs\": \"PHS\",\n",
    "        \"pasada\": \"ror\",             # hydro run-of-river\n",
    "        \"embalse\": \"hydro\",          # reservoir-type hydro\n",
    "        \"offshore\": \"offwind-ac\",    # adjust if you know DC vs AC\n",
    "        \"offshore_dc\": \"offwind-dc\",\n",
    "    }\n",
    "    ppl[\"carrier\"] = ppl[\"carrier_norm\"].map(carrier_map_es2en).fillna(ppl[\"carrier_norm\"])\n",
    "else:\n",
    "    logger.warning(\"Column 'carrier' not found in plants table after normalization.\")\n",
    "\n",
    "# ---- Filter: SNI, by year and minimum size\n",
    "FILTERING_POWER = 10.0   # kW threshold\n",
    "DATE_IN = 2017\n",
    "if {\"component\", \"DateIn\", \"p_nom\"}.issubset(ppl.columns):\n",
    "    ppl_connected = ppl[ppl[\"component\"] == \"S.N.I.\"]\n",
    "    ppl_connected_current = ppl_connected[ppl_connected[\"DateIn\"] <= DATE_IN]\n",
    "    ppl_filtered = ppl_connected_current[ppl_connected_current[\"p_nom\"] >= FILTERING_POWER].copy()\n",
    "else:\n",
    "    logger.warning(\"Required columns for filtering missing; skipping filter.\")\n",
    "    ppl_filtered = ppl.copy()\n",
    "\n",
    "# ---- Totals\n",
    "def safe_sum(series):\n",
    "    try: return float(series.sum())\n",
    "    except: return np.nan\n",
    "\n",
    "total_capacity = safe_sum(ppl.get(\"p_nom\", pd.Series(dtype=float)))\n",
    "total_connected = safe_sum(ppl_connected.get(\"p_nom\", pd.Series(dtype=float))) if 'ppl_connected' in locals() else np.nan\n",
    "total_connected_current = safe_sum(ppl_connected_current.get(\"p_nom\", pd.Series(dtype=float))) if 'ppl_connected_current' in locals() else np.nan\n",
    "total_filtered = safe_sum(ppl_filtered.get(\"p_nom\", pd.Series(dtype=float)))\n",
    "logger.info(\n",
    "    \"Capacities [kW]\\n\"\n",
    "    f\"  total:                {total_capacity:,.0f}\\n\"\n",
    "    f\"  connected (SNI):      {total_connected:,.0f}\\n\"\n",
    "    f\"  connected <= {DATE_IN}: {total_connected_current:,.0f}\\n\"\n",
    "    f\"  filtered (>= {FILTERING_POWER:.0f} kW): {total_filtered:,.0f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f50df8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Attach filtered plants to nearest LV substation\n",
    "\n",
    "from scipy.spatial import cKDTree as KDTree\n",
    "\n",
    "def attach_to_nearest_lv_bus(net: pypsa.Network, plants_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Attach plants to nearest bus among those marked substation_lv.\"\"\"\n",
    "    if \"substation_lv\" not in net.buses.columns:\n",
    "        raise ValueError(\"network.buses must contain a boolean 'substation_lv' column\")\n",
    "\n",
    "    if not {\"lon\", \"lat\"}.issubset(plants_df.columns):\n",
    "        raise ValueError(\"plants_df must contain 'lon' and 'lat' columns\")\n",
    "\n",
    "    sub_idx = net.buses.query(\"substation_lv\").index\n",
    "    if len(sub_idx) == 0:\n",
    "        raise ValueError(\"No LV substations found (substation_lv == True).\")\n",
    "\n",
    "    kdt = KDTree(net.buses.loc[sub_idx, [\"x\", \"y\"]].values)\n",
    "\n",
    "    # Query nearest bus for each plant lon/lat -> need plant x/y; network.buses stores x,y in projected units.\n",
    "    # If network.buses has lon/lat columns, prefer those:\n",
    "    if {\"lon\", \"lat\"}.issubset(net.buses.columns):\n",
    "        # Build a lon/lat->index KDTree as fallback using lon/lat and not x/y\n",
    "        # But your buses KDTree above already used x/y; keep consistent by translating plants to bus CRS when available.\n",
    "        pass\n",
    "\n",
    "    # Best-effort: assume bus x/y ~ proj coords corresponding to lon/lat mapping already in network\n",
    "    # If network carries bus lon/lat, we project plants into the same 'x/y' via a crude nearest in lon/lat space:\n",
    "    # For robustness (and speed), use lon/lat KDTree when bus lon/lat exist:\n",
    "    use_lonlat_tree = {\"lon\", \"lat\"}.issubset(net.buses.columns)\n",
    "    if use_lonlat_tree:\n",
    "        kdt_ll = KDTree(net.buses.loc[sub_idx, [\"lon\", \"lat\"]].values)\n",
    "        tree_i = kdt_ll.query(plants_df.loc[:, [\"lon\", \"lat\"]].values)[1]\n",
    "    else:\n",
    "        # fallback to x/y KDTree (will work if plant lon/lat columns are actually x/y already)\n",
    "        tree_i = kdt.query(plants_df.loc[:, [\"lon\", \"lat\"]].values)[1]\n",
    "\n",
    "    attached = plants_df.copy()\n",
    "    attached[\"bus\"] = sub_idx.append(pd.Index([np.nan]))[tree_i].astype(str)\n",
    "    missing = ~attached[\"bus\"].isin(net.buses.index)\n",
    "    if missing.any():\n",
    "        logger.warning(f\"Found {missing.sum()} plants with non-existing bus assignments.\")\n",
    "    return attached\n",
    "\n",
    "try:\n",
    "    ppl_attached = attach_to_nearest_lv_bus(network, ppl_filtered)\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Could not attach plants to LV buses: {e}\")\n",
    "    ppl_attached = ppl_filtered.copy()\n",
    "    ppl_attached[\"bus\"] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ecc4490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-11 11:01:01: Generators added: 64\n"
     ]
    }
   ],
   "source": [
    "# 5) Safely add generators to the network\n",
    "\n",
    "def add_generators_safe(net: pypsa.Network, pp_df: pd.DataFrame):\n",
    "    needed = [\"bus\", \"carrier\", \"p_nom\"]\n",
    "    if not set(needed).issubset(pp_df.columns):\n",
    "        raise ValueError(f\"Missing columns in plant df. Need at least: {needed}\")\n",
    "\n",
    "    # PyPSA expects per-generator rows. We’ll use index as names.\n",
    "    added = 0\n",
    "    for i in pp_df.index:\n",
    "        row = pp_df.loc[[i]]\n",
    "        try:\n",
    "            \n",
    "            net.madd(\"Generator\",\n",
    "                     row.index,\n",
    "                     bus=row[\"bus\"].values,\n",
    "                     carrier=row[\"carrier\"].values,\n",
    "                     p_nom=row[\"p_nom\"].values)\n",
    "            added += 1\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"  ⚠ could not add {i}: {e}\")\n",
    "    logger.info(f\"Generators added: {added}\")\n",
    "\n",
    "try:\n",
    "    add_generators_safe(network, ppl_attached)\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Adding generators failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0677cfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-11 11:01:01: Merged tech costs into network.generators\n"
     ]
    }
   ],
   "source": [
    "# 6) Technology costs merge\n",
    "try:\n",
    "    cost_filename = join(dirs[\"data/raw/generation\"], \"technology_cost.csv\")\n",
    "    costs = pd.read_csv(cost_filename, index_col=0, comment=\"#\")\n",
    "    # Expecting index to be carrier names; columns e.g. marginal_cost, capital_cost, etc.\n",
    "    # Remove any pre-existing columns to avoid duplicate merge keys\n",
    "    for c in [\"marginal_cost\", \"capital_cost\"]:\n",
    "        if c in network.generators.columns:\n",
    "            network.generators.drop(columns=[c], inplace=True)\n",
    "\n",
    "    network.generators = pd.merge(\n",
    "        network.generators,\n",
    "        costs,\n",
    "        left_on=\"carrier\",\n",
    "        right_index=True,\n",
    "        how=\"left\",\n",
    "        suffixes=(\"\", \"_cost\"),\n",
    "    )\n",
    "    logger.info(\"Merged tech costs into network.generators\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Could not merge technology costs: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09748521",
   "metadata": {},
   "source": [
    "Assertion that the buses RHS and LHS of the equation are balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d8e9cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapshots: DatetimeIndex(['2013-01-01 00:00:00', '2013-01-01 01:00:00',\n",
      "               '2013-01-01 02:00:00'],\n",
      "              dtype='datetime64[ns]', name='snapshot', freq=None) ... total: 8760\n",
      "Buses with non-zero RHS count: 235\n",
      "Buses with RHS≠0 but no LHS variables: ['90']\n",
      "Connected components: 24\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "n = network  # just a shorter alias\n",
    "\n",
    "# 1) Check you actually have snapshots and they match your time series\n",
    "print(\"Snapshots:\", n.snapshots[:3], \"... total:\", len(n.snapshots))\n",
    "\n",
    "# 2) Buses that carry any non-zero RHS (net fixed injections/withdrawals)\n",
    "rhs_load = (n.loads_t.p_set.groupby(n.loads.bus, axis=1).sum()\n",
    "            if not n.loads_t.p_set.empty else pd.DataFrame(index=n.snapshots))\n",
    "rhs_other = 0\n",
    "# add other fixed RHS terms if you use them, e.g. fixed generator p_set, shunts, etc.\n",
    "\n",
    "rhs_by_bus = rhs_load.sum(axis=0)  # sum over time\n",
    "rhs_nonzero = rhs_by_bus[abs(rhs_by_bus) > 1e-6]\n",
    "\n",
    "print(\"Buses with non-zero RHS count:\", rhs_nonzero.shape[0])\n",
    "\n",
    "# 3) Buses that have any LHS variables attached (anything that can balance)\n",
    "lhs_buses = set()\n",
    "\n",
    "# Generators with capacity (fixed or extendable)\n",
    "if len(n.generators):\n",
    "    cap = n.generators.p_nom.copy()\n",
    "    cap[n.generators.p_nom_extendable.fillna(False)] = cap.where(~n.generators.p_nom_extendable, 1.0)\n",
    "    lhs_buses |= set(n.generators.bus[cap > 0])\n",
    "\n",
    "# StorageUnits / Stores\n",
    "if len(n.storage_units):\n",
    "    cap = n.storage_units.p_nom.copy()\n",
    "    cap[n.storage_units.p_nom_extendable.fillna(False)] = cap.where(~n.storage_units.p_nom_extendable, 1.0)\n",
    "    lhs_buses |= set(n.storage_units.bus[cap > 0])\n",
    "\n",
    "if len(n.stores):\n",
    "    cap = n.stores.e_nom.copy()\n",
    "    cap[n.stores.e_nom_extendable.fillna(False)] = cap.where(~n.stores.e_nom_extendable, 1.0)\n",
    "    lhs_buses |= set(n.stores.bus[cap > 0])\n",
    "\n",
    "# Lines / Transformers (any positive rating yields flow variables)\n",
    "if len(n.lines):\n",
    "    lhs_buses |= set(n.lines.bus0[n.lines.s_nom > 0])\n",
    "    lhs_buses |= set(n.lines.bus1[n.lines.s_nom > 0])\n",
    "\n",
    "if len(n.transformers):\n",
    "    lhs_buses |= set(n.transformers.bus0[n.transformers.s_nom > 0])\n",
    "    lhs_buses |= set(n.transformers.bus1[n.transformers.s_nom > 0])\n",
    "\n",
    "# Links (DC/HVDC links also provide variables)\n",
    "if len(n.links):\n",
    "    lhs_buses |= set(n.links.bus0[n.links.p_nom > 0])\n",
    "    lhs_buses |= set(n.links.bus1[n.links.p_nom > 0])\n",
    "\n",
    "problem_buses = [b for b in rhs_nonzero.index if b not in lhs_buses]\n",
    "print(\"Buses with RHS≠0 but no LHS variables:\", problem_buses)\n",
    "\n",
    "# 4) Also check for islands without supply\n",
    "G = n.graph()\n",
    "islands = list(nx.connected_components(G))\n",
    "print(f\"Connected components: {len(islands)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5691869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads @90:\n",
      "      bus carrier type  p_set  q_set  sign\n",
      "Load                                     \n",
      "90    90                 0.0    0.0  -1.0\n",
      "Generators @90:\n",
      " Empty DataFrame\n",
      "Columns: [bus, carrier, p_nom, control, type, p_nom_mod, p_nom_extendable, p_nom_min, p_nom_max, p_min_pu, p_max_pu, p_set, q_set, sign, marginal_cost_quadratic, build_year, lifetime, efficiency, committable, start_up_cost, shut_down_cost, stand_by_cost, min_up_time, min_down_time, up_time_before, down_time_before, ramp_limit_up, ramp_limit_down, ramp_limit_start_up, ramp_limit_shut_down, weight, p_nom_opt, marginal_cost, capital_cost, notes]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 35 columns]\n",
      "Stores @90:\n",
      " Empty DataFrame\n",
      "Columns: [bus, type, carrier, e_nom, e_nom_mod, e_nom_extendable, e_nom_min, e_nom_max, e_min_pu, e_max_pu, e_initial, e_initial_per_period, e_cyclic, e_cyclic_per_period, p_set, q_set, sign, marginal_cost, marginal_cost_quadratic, marginal_cost_storage, capital_cost, standing_loss, build_year, lifetime, e_nom_opt]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 25 columns]\n",
      "StorageUnits @90:\n",
      " Empty DataFrame\n",
      "Columns: [bus, control, type, p_nom, p_nom_mod, p_nom_extendable, p_nom_min, p_nom_max, p_min_pu, p_max_pu, p_set, q_set, sign, carrier, spill_cost, marginal_cost, marginal_cost_quadratic, marginal_cost_storage, capital_cost, build_year, lifetime, state_of_charge_initial, state_of_charge_initial_per_period, state_of_charge_set, cyclic_state_of_charge, cyclic_state_of_charge_per_period, max_hours, efficiency_store, efficiency_dispatch, standing_loss, inflow, p_nom_opt]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 32 columns]\n",
      "Shunts @90:\n",
      " Empty DataFrame\n",
      "Columns: [bus, g, b, sign, g_pu, b_pu]\n",
      "Index: []\n",
      "Lines touching 90 (as bus0):\n",
      " []\n",
      "Lines touching 90 (as bus1):\n",
      " []\n",
      "Transformers touching 90 (bus0):\n",
      " []\n",
      "Transformers touching 90 (bus1):\n",
      " []\n",
      "Links touching 90 (bus0):\n",
      " []\n",
      "Links touching 90 (bus1):\n",
      " []\n"
     ]
    }
   ],
   "source": [
    "b = \"90\"  # o 90, según tu índice; abajo normalizamos a str\n",
    "\n",
    "def _mask_bus(df, col=\"bus\", bus=b):\n",
    "    if df.empty or col not in df.columns: \n",
    "        return df.iloc[0:0]\n",
    "    return df[df[col].astype(str) == str(bus)]\n",
    "\n",
    "loads_90        = _mask_bus(network.loads, \"bus\")\n",
    "gens_90         = _mask_bus(network.generators, \"bus\")\n",
    "stores_90       = _mask_bus(network.stores, \"bus\")\n",
    "sus_90          = _mask_bus(network.storage_units, \"bus\")\n",
    "shunts_90       = _mask_bus(network.shunt_impedances, \"bus\") if hasattr(network, \"shunt_impedances\") else network.buses.iloc[0:0]\n",
    "\n",
    "lines_90_0      = _mask_bus(network.lines, \"bus0\")\n",
    "lines_90_1      = _mask_bus(network.lines, \"bus1\")\n",
    "trafos_90_0     = _mask_bus(network.transformers, \"bus0\")\n",
    "trafos_90_1     = _mask_bus(network.transformers, \"bus1\")\n",
    "links_90_0      = _mask_bus(network.links, \"bus0\")\n",
    "links_90_1      = _mask_bus(network.links, \"bus1\")\n",
    "\n",
    "print(\"Loads @90:\\n\", loads_90)\n",
    "print(\"Generators @90:\\n\", gens_90)\n",
    "print(\"Stores @90:\\n\", stores_90)\n",
    "print(\"StorageUnits @90:\\n\", sus_90)\n",
    "print(\"Shunts @90:\\n\", shunts_90)\n",
    "\n",
    "print(\"Lines touching 90 (as bus0):\\n\", lines_90_0.index.tolist())\n",
    "print(\"Lines touching 90 (as bus1):\\n\", lines_90_1.index.tolist())\n",
    "print(\"Transformers touching 90 (bus0):\\n\", trafos_90_0.index.tolist())\n",
    "print(\"Transformers touching 90 (bus1):\\n\", trafos_90_1.index.tolist())\n",
    "print(\"Links touching 90 (bus0):\\n\", links_90_0.index.tolist())\n",
    "print(\"Links touching 90 (bus1):\\n\", links_90_1.index.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdfbeb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'224', '10', '44', '80', '267', '285', '115', '167', '61', '106', '24', '269', '31', '185', '129', '86', '33', '120', '37', '217', '81', '82', '20', '311', '137', '160', '273', '239', '233', '306', '272', '195', '97', '30', '83', '251', '84', '21', '92', '72', '198', '266', '245', '228', '48', '220', '8', '234', '247', '42', '102', '294', '9', '139', '2', '216', '25', '194', '60', '87', '56', '252', '67', '104', '40', '146', '188', '79', '197', '34', '142', '230', '57', '221', '11', '23', '268', '282', '7', '257', '141', '6', '27', '126', '47', '35', '301', '118', '236', '260', '77', '32', '256', '166', '175', '103', '112', '145', '132', '144', '138', '265', '204', '229', '1', '302', '157', '250', '19', '173', '296', '205', '161', '231', '222', '312', '303', '152', '154', '232', '38', '248', '14', '281', '290', '75', '93', '52', '123', '16', '147', '304', '215', '28', '43', '55', '121', '3', '111', '249', '58', '270', '73', '261', '17', '100', '298', '140', '263', '29', '227', '305', '12', '190', '184', '164', '22', '98', '94', '264', '108', '201', '179', '85', '143', '113', '125', '69', '15', '287', '41', '45', '176', '54', '78', '4', '68', '162', '18', '292', '13', '107', '181', '53', '163', '36', '71', '88', '46', '101', '65', '124', '225', '105', '307', '153', '117', '110', '136', '99', '240', '76', '119', '70', '297', '177', '39', '223', '50', '51', '89', '0', '62', '237', '91', '74', '289', '200', '95', '130', '158', '128', '66', '202', '26', '196', '288', '271', '59', '96', '278', '131', '5', '49', '258', '262', '155', '218', '114'} 224\n",
      "here it would be added\n",
      "    #    network.add(\"Generator\", f\"slack_224\", bus=b, p_nom=1e6, marginal_cost=1e6)\n",
      "    \n",
      "{'151', '277', '64', '300', '219', '212', '63'} 151\n",
      "here it would be added\n",
      "    #    network.add(\"Generator\", f\"slack_151\", bus=b, p_nom=1e6, marginal_cost=1e6)\n",
      "    \n",
      "{'90'} 90\n",
      "here it would be added\n",
      "    #    network.add(\"Generator\", f\"slack_90\", bus=b, p_nom=1e6, marginal_cost=1e6)\n",
      "    \n",
      "{'259', '116'} 259\n",
      "here it would be added\n",
      "    #    network.add(\"Generator\", f\"slack_259\", bus=b, p_nom=1e6, marginal_cost=1e6)\n",
      "    \n",
      "{'308', '244', '127', '242', '243'} 308\n",
      "here it would be added\n",
      "    #    network.add(\"Generator\", f\"slack_308\", bus=b, p_nom=1e6, marginal_cost=1e6)\n",
      "    \n",
      "{'156', '133'} 156\n",
      "here it would be added\n",
      "    #    network.add(\"Generator\", f\"slack_156\", bus=b, p_nom=1e6, marginal_cost=1e6)\n",
      "    \n",
      "{'274', '148'} 274\n",
      "here it would be added\n",
      "    #    network.add(\"Generator\", f\"slack_274\", bus=b, p_nom=1e6, marginal_cost=1e6)\n",
      "    \n",
      "{'149', '275'} 149\n",
      "here it would be added\n",
      "    #    network.add(\"Generator\", f\"slack_149\", bus=b, p_nom=1e6, marginal_cost=1e6)\n",
      "    \n",
      "{'276', '150'} 276\n",
      "here it would be added\n",
      "    #    network.add(\"Generator\", f\"slack_276\", bus=b, p_nom=1e6, marginal_cost=1e6)\n",
      "    \n",
      "{'159', '279'} 159\n",
      "here it would be added\n",
      "    #    network.add(\"Generator\", f\"slack_159\", bus=b, p_nom=1e6, marginal_cost=1e6)\n",
      "    \n",
      "{'280', '165'} 280\n",
      "here it would be added\n",
      "    #    network.add(\"Generator\", f\"slack_280\", bus=b, p_nom=1e6, marginal_cost=1e6)\n",
      "    \n",
      "{'171', '170', '207', '169', '283'} 171\n",
      "here it would be added\n",
      "    #    network.add(\"Generator\", f\"slack_171\", bus=b, p_nom=1e6, marginal_cost=1e6)\n",
      "    \n",
      "{'174', '284'} 174\n",
      "here it would be added\n",
      "    #    network.add(\"Generator\", f\"slack_174\", bus=b, p_nom=1e6, marginal_cost=1e6)\n",
      "    \n",
      "{'178', '286'} 178\n",
      "here it would be added\n",
      "    #    network.add(\"Generator\", f\"slack_178\", bus=b, p_nom=1e6, marginal_cost=1e6)\n",
      "    \n",
      "{'182', '183'} 182\n",
      "here it would be added\n",
      "    #    network.add(\"Generator\", f\"slack_182\", bus=b, p_nom=1e6, marginal_cost=1e6)\n",
      "    \n",
      "{'291', '186'} 291\n",
      "here it would be added\n",
      "    #    network.add(\"Generator\", f\"slack_291\", bus=b, p_nom=1e6, marginal_cost=1e6)\n",
      "    \n",
      "{'189', '293'} 189\n",
      "here it would be added\n",
      "    #    network.add(\"Generator\", f\"slack_189\", bus=b, p_nom=1e6, marginal_cost=1e6)\n",
      "    \n",
      "{'192', '191'} 192\n",
      "here it would be added\n",
      "    #    network.add(\"Generator\", f\"slack_192\", bus=b, p_nom=1e6, marginal_cost=1e6)\n",
      "    \n",
      "{'295', '199'} 295\n",
      "here it would be added\n",
      "    #    network.add(\"Generator\", f\"slack_295\", bus=b, p_nom=1e6, marginal_cost=1e6)\n",
      "    \n",
      "{'209', '208'} 209\n",
      "here it would be added\n",
      "    #    network.add(\"Generator\", f\"slack_209\", bus=b, p_nom=1e6, marginal_cost=1e6)\n",
      "    \n",
      "{'211', '299'} 211\n",
      "here it would be added\n",
      "    #    network.add(\"Generator\", f\"slack_211\", bus=b, p_nom=1e6, marginal_cost=1e6)\n",
      "    \n",
      "{'213', '214', '310'} 213\n",
      "here it would be added\n",
      "    #    network.add(\"Generator\", f\"slack_213\", bus=b, p_nom=1e6, marginal_cost=1e6)\n",
      "    \n",
      "{'246', '309'} 246\n",
      "here it would be added\n",
      "    #    network.add(\"Generator\", f\"slack_246\", bus=b, p_nom=1e6, marginal_cost=1e6)\n",
      "    \n",
      "{'255', '254'} 255\n",
      "here it would be added\n",
      "    #    network.add(\"Generator\", f\"slack_255\", bus=b, p_nom=1e6, marginal_cost=1e6)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "for comp in nx.connected_components(network.graph()):\n",
    "    b = next(iter(comp))\n",
    "    print(comp, b)\n",
    "    if f\"slack_{b}\" not in network.generators.index:\n",
    "        print(\"here it would be added\" + \n",
    "              f\"\"\"\n",
    "    #    network.add(\"Generator\", f\"slack_{b}\", bus=b, p_nom=1e6, marginal_cost=1e6)\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ef2c040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed loads connected to bus 90: ['90']\n",
      "Removed bus 90\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# manually remove bus 90, just to test \n",
    "bus_id = \"90\"\n",
    "\n",
    "# 1. Remove any loads connected to that bus\n",
    "\n",
    "loads_to_remove = network.loads.index[network.loads.bus.astype(str) == str(bus_id)]\n",
    "for load_id in loads_to_remove:\n",
    "    network.remove(\"Load\", load_id)\n",
    "print(f\"Removed loads connected to bus {bus_id}: {list(loads_to_remove)}\")\n",
    "\n",
    "# 2. Remove the bus itself\n",
    "if bus_id in network.buses.index.astype(str):\n",
    "    network.remove(\"Bus\", bus_id)\n",
    "    print(f\"Removed bus {bus_id}\")\n",
    "else:\n",
    "    print(f\"Bus {bus_id} not found in network.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e545be00",
   "metadata": {},
   "outputs": [],
   "source": [
    "network.carriers = pd.read_csv(\n",
    "    join(dirs[\"data/raw/generation\"], \"carriers.csv\"), index_col=0\n",
    ").reset_index()\n",
    "\n",
    "network.carriers\n",
    "\n",
    "network.generators.carrier.unique()\n",
    "\n",
    "network.generators.to_csv(\"carriers_gen.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc036fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-11 11:01:01: The following transformers have zero r, which could break the linear load flow:\n",
      "Index(['transf_0_0', 'transf_1_0', 'transf_2_0', 'transf_3_0', 'transf_3_1',\n",
      "       'transf_4_0', 'transf_5_0', 'transf_6_0', 'transf_7_0', 'transf_7_1',\n",
      "       'transf_8_0', 'transf_9_0', 'transf_9_1', 'transf_10_0', 'transf_10_1',\n",
      "       'transf_10_2', 'transf_11_0', 'transf_12_0', 'transf_13_0',\n",
      "       'transf_14_0', 'transf_15_0', 'transf_16_0', 'transf_17_0',\n",
      "       'transf_18_0', 'transf_18_1', 'transf_19_0', 'transf_20_0',\n",
      "       'transf_20_1', 'transf_21_0', 'transf_23_0', 'transf_24_0',\n",
      "       'transf_26_0', 'transf_27_0', 'transf_28_0', 'transf_29_0',\n",
      "       'transf_32_0', 'transf_32_1', 'transf_33_0', 'transf_34_0',\n",
      "       'transf_35_0', 'transf_36_0', 'transf_36_1', 'transf_37_0',\n",
      "       'transf_40_0', 'transf_42_0', 'transf_43_0', 'transf_45_0',\n",
      "       'transf_45_1', 'transf_46_0', 'transf_47_0', 'transf_48_0',\n",
      "       'transf_48_1', 'transf_49_0', 'transf_49_1', 'transf_50_0',\n",
      "       'transf_64_0', 'transf_68_0', 'transf_84_0', 'transf_86_0',\n",
      "       'transf_93_0', 'transf_116_0', 'transf_155_0'],\n",
      "      dtype='object', name='Transformer')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-11 11:01:01: The following buses have carriers which are not defined:\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '303', '304', '305', '306', '307', '308', '309', '310', '311', '312'],\n",
      "      dtype='object', name='Bus', length=295)\n",
      "2025-11-11 11:01:01: The following lines have carriers which are not defined:\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '237', '238', '239', '240', '241', '242', '243', '244', '245', '246'],\n",
      "      dtype='object', name='Line', length=247)\n",
      "2025-11-11 11:01:01: The following generators have carriers which are not defined:\n",
      "Index(['Abanico', 'Agoyan', 'Alao', 'Alvaro_Tinajero', 'Anibal_Santos_(Gas)',\n",
      "       'Anibal_Santos_(Vapor)', 'Baba', 'Calope', 'Catamayo',\n",
      "       'Coca_Codo_Sinclair', 'Cumbaya', 'Delsitanisagua_0', 'Due',\n",
      "       'Ecoelectric', 'Ecudos_A-G', 'El_Descanso', 'Enrique_Garcia',\n",
      "       'Esmeraldas_I', 'Esmeraldas_II', 'Generoca', 'Gonzalo_Zevallos_(Gas)',\n",
      "       'Gonzalo_Zevallos_(Vapor)', 'Gualberto_Hernandez', 'Guangopolo_0',\n",
      "       'Guangopolo_1', 'Guangopolo_2', 'Hidrosanbartolo', 'Ipnegal',\n",
      "       'Jaramijo', 'Jivino_II', 'Jivino_III', 'Manduriacu', 'Manta_II',\n",
      "       'Marcel_Laniado', 'Mazar', 'Minas_San_Francisco', 'Miraflores', 'Nayon',\n",
      "       'Normandia_0', 'Ocana', 'Paute', 'Pucara', 'Pusuno_0', 'Quevedo_II',\n",
      "       'Recuperadora', 'San_Carlos', 'San_Francisco', 'Santa_Elena_II',\n",
      "       'Santa_Elena_III', 'Santa_Rosa', 'Saucay', 'Saymirin', 'Selva_Alegre',\n",
      "       'Sibimbe', 'Sigchos', 'Sopladora', 'Termogas_Machala_I',\n",
      "       'Termogas_Machala_II', 'Termoguayas', 'Topo', 'Trinitaria', 'Victoria',\n",
      "       'Victoria_II', 'Villonaco'],\n",
      "      dtype='object', name='Generator')\n",
      "2025-11-11 11:01:01: Encountered nan's in static data for columns ['marginal_cost', 'capital_cost'] of component 'Generator'.\n",
      "2025-11-11 11:01:01: The following transformers have zero r, which could break the linear load flow:\n",
      "Index(['transf_0_0', 'transf_1_0', 'transf_2_0', 'transf_3_0', 'transf_3_1',\n",
      "       'transf_4_0', 'transf_5_0', 'transf_6_0', 'transf_7_0', 'transf_7_1',\n",
      "       'transf_8_0', 'transf_9_0', 'transf_9_1', 'transf_10_0', 'transf_10_1',\n",
      "       'transf_10_2', 'transf_11_0', 'transf_12_0', 'transf_13_0',\n",
      "       'transf_14_0', 'transf_15_0', 'transf_16_0', 'transf_17_0',\n",
      "       'transf_18_0', 'transf_18_1', 'transf_19_0', 'transf_20_0',\n",
      "       'transf_20_1', 'transf_21_0', 'transf_23_0', 'transf_24_0',\n",
      "       'transf_26_0', 'transf_27_0', 'transf_28_0', 'transf_29_0',\n",
      "       'transf_32_0', 'transf_32_1', 'transf_33_0', 'transf_34_0',\n",
      "       'transf_35_0', 'transf_36_0', 'transf_36_1', 'transf_37_0',\n",
      "       'transf_40_0', 'transf_42_0', 'transf_43_0', 'transf_45_0',\n",
      "       'transf_45_1', 'transf_46_0', 'transf_47_0', 'transf_48_0',\n",
      "       'transf_48_1', 'transf_49_0', 'transf_49_1', 'transf_50_0',\n",
      "       'transf_64_0', 'transf_68_0', 'transf_84_0', 'transf_86_0',\n",
      "       'transf_93_0', 'transf_116_0', 'transf_155_0'],\n",
      "      dtype='object', name='Transformer')\n",
      "2025-11-11 11:01:01: The following buses have carriers which are not defined:\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '303', '304', '305', '306', '307', '308', '309', '310', '311', '312'],\n",
      "      dtype='object', name='Bus', length=295)\n",
      "2025-11-11 11:01:01: The following lines have carriers which are not defined:\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '237', '238', '239', '240', '241', '242', '243', '244', '245', '246'],\n",
      "      dtype='object', name='Line', length=247)\n",
      "2025-11-11 11:01:01: The following generators have carriers which are not defined:\n",
      "Index(['Abanico', 'Agoyan', 'Alao', 'Alvaro_Tinajero', 'Anibal_Santos_(Gas)',\n",
      "       'Anibal_Santos_(Vapor)', 'Baba', 'Calope', 'Catamayo',\n",
      "       'Coca_Codo_Sinclair', 'Cumbaya', 'Delsitanisagua_0', 'Due',\n",
      "       'Ecoelectric', 'Ecudos_A-G', 'El_Descanso', 'Enrique_Garcia',\n",
      "       'Esmeraldas_I', 'Esmeraldas_II', 'Generoca', 'Gonzalo_Zevallos_(Gas)',\n",
      "       'Gonzalo_Zevallos_(Vapor)', 'Gualberto_Hernandez', 'Guangopolo_0',\n",
      "       'Guangopolo_1', 'Guangopolo_2', 'Hidrosanbartolo', 'Ipnegal',\n",
      "       'Jaramijo', 'Jivino_II', 'Jivino_III', 'Manduriacu', 'Manta_II',\n",
      "       'Marcel_Laniado', 'Mazar', 'Minas_San_Francisco', 'Miraflores', 'Nayon',\n",
      "       'Normandia_0', 'Ocana', 'Paute', 'Pucara', 'Pusuno_0', 'Quevedo_II',\n",
      "       'Recuperadora', 'San_Carlos', 'San_Francisco', 'Santa_Elena_II',\n",
      "       'Santa_Elena_III', 'Santa_Rosa', 'Saucay', 'Saymirin', 'Selva_Alegre',\n",
      "       'Sibimbe', 'Sigchos', 'Sopladora', 'Termogas_Machala_I',\n",
      "       'Termogas_Machala_II', 'Termoguayas', 'Topo', 'Trinitaria', 'Victoria',\n",
      "       'Victoria_II', 'Villonaco'],\n",
      "      dtype='object', name='Generator')\n",
      "2025-11-11 11:01:01: Encountered nan's in static data for columns ['marginal_cost', 'capital_cost'] of component 'Generator'.\n",
      "2025-11-11 11:01:02:  Solve problem using Highs solver\n",
      "2025-11-11 11:01:02:  Writing time: 0.04s\n",
      "2025-11-11 11:01:02: Optimization potentially failed: \n",
      "Status: warning\n",
      "Termination condition: infeasible\n",
      "Solution: 0 primals, 0 duals\n",
      "Objective: nan\n",
      "Solver model: available\n",
      "Solver message: Infeasible\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('warning', 'infeasible')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.lines['x'] = 0.1\n",
    "network.lines['r'] = 0.01\n",
    "solver = \"highs\"\n",
    "# Select first 4 snapshots\n",
    "snapshots_subset = network.snapshots[:4]\n",
    "\n",
    "# Optimize only over those 4 snapshots\n",
    "network.optimize(snapshots=snapshots_subset, solver_name=solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73644dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "network.buses.substation_lv = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31fa4c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Carrier</th>\n",
       "      <th>co2_emissions</th>\n",
       "      <th>color</th>\n",
       "      <th>nice_name</th>\n",
       "      <th>max_growth</th>\n",
       "      <th>max_relative_growth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coal</td>\n",
       "      <td>0.354</td>\n",
       "      <td>#707070</td>\n",
       "      <td>Coal</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>natural gas</td>\n",
       "      <td>0.187</td>\n",
       "      <td>#d35050</td>\n",
       "      <td>Natural Gas</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nuclear</td>\n",
       "      <td>0.000</td>\n",
       "      <td>#ff9000</td>\n",
       "      <td>Nuclear</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oil</td>\n",
       "      <td>0.248</td>\n",
       "      <td>#262626</td>\n",
       "      <td>Oil</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lignite</td>\n",
       "      <td>0.334</td>\n",
       "      <td>#9e5a01</td>\n",
       "      <td>Lignite</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>geothermal</td>\n",
       "      <td>0.026</td>\n",
       "      <td>#ba91b1</td>\n",
       "      <td>Geothermal</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>biomass</td>\n",
       "      <td>0.000</td>\n",
       "      <td>#0c6013</td>\n",
       "      <td>Biomass</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hydro</td>\n",
       "      <td>0.000</td>\n",
       "      <td>#08ad97</td>\n",
       "      <td>Reservoir &amp; Dam</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>offwind-ac</td>\n",
       "      <td>0.000</td>\n",
       "      <td>#6895dd</td>\n",
       "      <td>Offshore Wind (AC)</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>offwind-dc</td>\n",
       "      <td>0.000</td>\n",
       "      <td>#74c6f2</td>\n",
       "      <td>Offshore Wind (DC)</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>onwind</td>\n",
       "      <td>0.000</td>\n",
       "      <td>#235ebc</td>\n",
       "      <td>Onshore Wind</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>solar</td>\n",
       "      <td>0.000</td>\n",
       "      <td>#f9d002</td>\n",
       "      <td>Solar</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PHS</td>\n",
       "      <td>0.000</td>\n",
       "      <td>#08ad97</td>\n",
       "      <td>Pumped Hydro Storage</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ror</td>\n",
       "      <td>0.000</td>\n",
       "      <td>#4adbc8</td>\n",
       "      <td>Run of River</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Carrier  co2_emissions    color             nice_name  max_growth  \\\n",
       "0          coal          0.354  #707070                  Coal         inf   \n",
       "1   natural gas          0.187  #d35050           Natural Gas         inf   \n",
       "2       nuclear          0.000  #ff9000               Nuclear         inf   \n",
       "3           oil          0.248  #262626                   Oil         inf   \n",
       "4       lignite          0.334  #9e5a01               Lignite         inf   \n",
       "5    geothermal          0.026  #ba91b1            Geothermal         inf   \n",
       "6       biomass          0.000  #0c6013               Biomass         inf   \n",
       "7         hydro          0.000  #08ad97       Reservoir & Dam         inf   \n",
       "8    offwind-ac          0.000  #6895dd    Offshore Wind (AC)         inf   \n",
       "9    offwind-dc          0.000  #74c6f2    Offshore Wind (DC)         inf   \n",
       "10       onwind          0.000  #235ebc          Onshore Wind         inf   \n",
       "11        solar          0.000  #f9d002                 Solar         inf   \n",
       "12          PHS          0.000  #08ad97  Pumped Hydro Storage         inf   \n",
       "13          ror          0.000  #4adbc8          Run of River         inf   \n",
       "\n",
       "    max_relative_growth  \n",
       "0                   0.0  \n",
       "1                   0.0  \n",
       "2                   0.0  \n",
       "3                   0.0  \n",
       "4                   0.0  \n",
       "5                   0.0  \n",
       "6                   0.0  \n",
       "7                   0.0  \n",
       "8                   0.0  \n",
       "9                   0.0  \n",
       "10                  0.0  \n",
       "11                  0.0  \n",
       "12                  0.0  \n",
       "13                  0.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.carriers\n",
    "\n",
    "#attempt to solve for 1 month w/o clusering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fb91d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-11 11:01:03: Bus-to-region mapping written to bus_mapped.csv\n",
      "2025-11-11 11:01:03: Unique regions in busmap: 23 (over 295 buses)\n",
      "2025-11-11 11:01:03: Setting representative lon/lat from region centroids for 295 buses.\n",
      "2025-11-11 11:01:03: Recomputed bus x/y from lon/lat in EPSG:3857.\n",
      "2025-11-11 11:01:03: Assigned representative coordinates per region to all clustered buses.\n",
      "2025-11-11 11:01:03: Cluster 'Azuay': unified voltages to 230 kV. Counts (snapped): {230.0: 4, 69.0: 3, 138.0: 2}\n",
      "2025-11-11 11:01:03: Cluster 'Bolivar': unified voltages to 69 kV. Counts (snapped): {69.0: 5, 500.0: 1}\n",
      "2025-11-11 11:01:03: Cluster 'Carchi': unified voltages to 69 kV. Counts (snapped): {69.0: 5, 138.0: 1}\n",
      "2025-11-11 11:01:03: Cluster 'Cañar': unified voltages to 69 kV. Counts (snapped): {69.0: 6, 230.0: 4}\n",
      "2025-11-11 11:01:03: Cluster 'Chimborazo': unified voltages to 69 kV. Counts (snapped): {69.0: 6, 230.0: 1}\n",
      "2025-11-11 11:01:03: Cluster 'Cotopaxi': unified voltages to 69 kV. Counts (snapped): {69.0: 6, 138.0: 1}\n",
      "2025-11-11 11:01:03: Cluster 'El Oro': unified voltages to 69 kV. Counts (snapped): {69.0: 6, 230.0: 5, 138.0: 3}\n",
      "2025-11-11 11:01:03: Cluster 'Esmeraldas': unified voltages to 69 kV. Counts (snapped): {69.0: 17, 138.0: 2, 230.0: 1}\n",
      "2025-11-11 11:01:03: Cluster 'Guayas': unified voltages to 69 kV. Counts (snapped): {69.0: 28, 138.0: 10, 230.0: 6, 500.0: 1}\n",
      "2025-11-11 11:01:03: Cluster 'Imbabura': unified voltages to 69 kV. Counts (snapped): {69.0: 7, 138.0: 3, 230.0: 2}\n",
      "2025-11-11 11:01:03: Cluster 'Loja': unified voltages to 69 kV. Counts (snapped): {69.0: 16, 138.0: 1}\n",
      "2025-11-11 11:01:03: Cluster 'Los Rios': unified voltages to 69 kV. Counts (snapped): {69.0: 15, 138.0: 2, 230.0: 2}\n",
      "2025-11-11 11:01:03: Cluster 'Manabi': unified voltages to 69 kV. Counts (snapped): {69.0: 20, 138.0: 6, 230.0: 1}\n",
      "2025-11-11 11:01:03: Cluster 'Morona Santiago': unified voltages to 138 kV. Counts (snapped): {138.0: 5, 69.0: 2, 230.0: 1}\n",
      "2025-11-11 11:01:03: Cluster 'Napo': unified voltages to 500 kV. Counts (snapped): {500.0: 2, 138.0: 2}\n",
      "2025-11-11 11:01:03: Cluster 'Orellana': unified voltages to 69 kV. Counts (snapped): {69.0: 3, 138.0: 2}\n",
      "2025-11-11 11:01:03: Cluster 'Pastaza': unified voltages to 138 kV. Counts (snapped): {69.0: 1, 138.0: 1}\n",
      "2025-11-11 11:01:03: Cluster 'Pichincha': unified voltages to 138 kV. Counts (snapped): {138.0: 11, 69.0: 11, 230.0: 4, 500.0: 1}\n",
      "2025-11-11 11:01:03: Cluster 'Santa Elena': unified voltages to 69 kV. Counts (snapped): {69.0: 8, 138.0: 3}\n",
      "2025-11-11 11:01:03: Cluster 'Santo Domingo de los Tsachilas': unified voltages to 69 kV. Counts (snapped): {69.0: 6, 230.0: 3, 138.0: 2}\n",
      "2025-11-11 11:01:03: Cluster 'Sucumbios': unified voltages to 69 kV. Counts (snapped): {69.0: 6, 230.0: 3}\n",
      "2025-11-11 11:01:03: Cluster 'Tungurahua': unified voltages to 138 kV. Counts (snapped): {138.0: 6, 69.0: 3, 230.0: 2, 500.0: 1}\n",
      "2025-11-11 11:01:03: Cluster 'Zamora Chinchipe': unified voltages to 69 kV. Counts (snapped): {69.0: 5, 230.0: 1, 138.0: 1}\n",
      "2025-11-11 11:01:03: Harmonized voltage in 23/23 region clusters (fields: v_nom_raw, v_nom).\n",
      "2025-11-11 11:01:03: Clustering failed: In Line cluster v_nom, the values of attribute v_nom do not agree:\n",
      "Line\n",
      "33     230.0\n",
      "62      69.0\n",
      "98      69.0\n",
      "157     69.0\n",
      "170    230.0\n",
      "Name: v_nom, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 7) GADM regions and bus mapping (with nearest-region fallback)\n",
    "COUNTRY_ISO3 = \"ECU\"\n",
    "GADM_VERSION = \"4.1\"\n",
    "GADM_FILE_STEM = f\"gadm41_{COUNTRY_ISO3}\"\n",
    "GADM_URL = f\"https://geodata.ucdavis.edu/gadm/gadm{GADM_VERSION}/gpkg/{GADM_FILE_STEM}.gpkg\"\n",
    "\n",
    "def ensure_gadm_file(gadm_path: str) -> str:\n",
    "    gpkg_path = os.path.join(gadm_path, f\"{GADM_FILE_STEM}.gpkg\")\n",
    "    gpkg_dir = Path(gpkg_path).parent\n",
    "    if not Path(gpkg_path).is_file():\n",
    "        gpkg_dir.mkdir(parents=True, exist_ok=True)\n",
    "        import requests, shutil as _shutil\n",
    "        logger.info(f\"Downloading GADM: {GADM_URL}\")\n",
    "        resp = requests.get(GADM_URL, stream=True, timeout=300)\n",
    "        resp.raise_for_status()\n",
    "        with open(gpkg_path, \"wb\") as f:\n",
    "            _shutil.copyfileobj(resp.raw, f)\n",
    "    assert Path(gpkg_path).is_file(), f\"GADM not found or failed to download: {gpkg_path}\"\n",
    "    return gpkg_path\n",
    "\n",
    "try:\n",
    "    gadm_file = ensure_gadm_file(dirs[\"data/raw/gadm\"])\n",
    "    shapefile = gpd.read_file(gadm_file, layer=\"ADM_ADM_1\")\n",
    "except Exception as e:\n",
    "    shapefile = None\n",
    "    logger.warning(f\"Could not load GADM: {e}\")\n",
    "\n",
    "def get_node_by_region(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Ensure EPSG:4326 and add centroid x/y for convenience.\"\"\"\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "    gdf = gdf.copy()\n",
    "    gdf[\"x\"] = gdf.geometry.centroid.x\n",
    "    gdf[\"y\"] = gdf.geometry.centroid.y\n",
    "    return gdf\n",
    "\n",
    "def _haversine_km(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"Approximate distance (km) on a sphere.\"\"\"\n",
    "    import math\n",
    "    R = 6371.0\n",
    "    dlon = math.radians(lon2 - lon1)\n",
    "    dlat = math.radians(lat2 - lat1)\n",
    "    a = (math.sin(dlat/2)**2 +\n",
    "         math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(dlon/2)**2)\n",
    "    return 2 * R * math.asin(math.sqrt(a))\n",
    "\n",
    "def attach_node_to_region_or_nearest(gdf: gpd.GeoDataFrame, longitude: float, latitude: float):\n",
    "    \"\"\"\n",
    "    Try strict point-in-polygon. If no hit, assign the nearest polygon by centroid,\n",
    "    returning (region_name, region_centroid_x, region_centroid_y, distance_km, used_nearest: bool).\n",
    "    \"\"\"\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "    pt = Point(float(longitude), float(latitude))\n",
    "\n",
    "    # 1) strict containment\n",
    "    hit = gdf[gdf.contains(pt)]\n",
    "    if not hit.empty:\n",
    "        row = hit.iloc[0]\n",
    "        return row.get(\"NAME_1\", \"NA\"), row.get(\"x\", np.nan), row.get(\"y\", np.nan), 0.0, False\n",
    "\n",
    "    # 2) nearest by centroid (fast and robust; good enough for admin-1)\n",
    "    # If an sindex exists, you could use it; here we just compute argmin over centroids:\n",
    "    dx = (gdf[\"x\"] - float(longitude))\n",
    "    dy = (gdf[\"y\"] - float(latitude))\n",
    "    # pick row with minimal haversine distance\n",
    "    d_km = _haversine_km(float(longitude), float(latitude), gdf[\"x\"], gdf[\"y\"])\n",
    "    # d_km is a Series due to vectorized haversine\n",
    "    nearest_idx = d_km.astype(float).idxmin()\n",
    "    row = gdf.loc[nearest_idx]\n",
    "    return row.get(\"NAME_1\", \"NA\"), row.get(\"x\", np.nan), row.get(\"y\", np.nan), float(d_km.loc[nearest_idx]), True\n",
    "\n",
    "def attach_region_to_buses(gdf: gpd.GeoDataFrame, buses: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Attach each bus to a region. If the bus is on a border or outside polygons,\n",
    "    assign the nearest region and LOG it with lon/lat and distance.\n",
    "    \"\"\"\n",
    "    buses = buses.copy()\n",
    "    buses[\"region\"] = \"NA\"\n",
    "    buses[\"region_x\"] = np.nan\n",
    "    buses[\"region_y\"] = np.nan\n",
    "\n",
    "    if gdf is None or len(gdf) == 0:\n",
    "        logger.warning(\"No GADM dataframe available; skipping region attachment.\")\n",
    "        return buses\n",
    "\n",
    "    # Ensure we have lon/lat\n",
    "    if not {\"lon\", \"lat\"}.issubset(buses.columns):\n",
    "        logger.warning(\"Buses lack lon/lat; cannot attach regions.\")\n",
    "        return buses\n",
    "\n",
    "    # Iterate and attach\n",
    "    for i in buses.index:\n",
    "        try:\n",
    "            lon = float(buses.at[i, \"lon\"])\n",
    "            lat = float(buses.at[i, \"lat\"])\n",
    "            if not np.isfinite(lon) or not np.isfinite(lat):\n",
    "                continue\n",
    "            r, rx, ry, d_km, used_nearest = attach_node_to_region_or_nearest(gdf, lon, lat)\n",
    "            buses.at[i, \"region\"] = r\n",
    "            buses.at[i, \"region_x\"] = rx\n",
    "            buses.at[i, \"region_y\"] = ry\n",
    "            if used_nearest:\n",
    "                logger.warning(\n",
    "                    f\"Bus '{i}' was outside all polygons; assigned to nearest region='{r}' \"\n",
    "                    f\"(bus lon/lat=({lon:.6f},{lat:.6f}), ~{d_km:.1f} km to region centroid).\"\n",
    "                )\n",
    "        except Exception as ex:\n",
    "            # keep as NA and continue\n",
    "            logger.warning(f\"Region attach failed for bus '{i}': {ex}\")\n",
    "            continue\n",
    "    return buses\n",
    "\n",
    "# Build busmap and (optionally) cluster\n",
    "busmap = None\n",
    "if shapefile is not None:\n",
    "    region_node = get_node_by_region(shapefile)\n",
    "    try:\n",
    "        buses_with_region = attach_region_to_buses(region_node, network.buses)\n",
    "        # Ensure busmap index matches network.buses.index exactly\n",
    "        buses_with_region = buses_with_region.loc[network.buses.index]\n",
    "        busmap = buses_with_region[\"region\"]\n",
    "        busmap.to_csv(\"bus_mapped.csv\")\n",
    "        logger.info(\"Bus-to-region mapping written to bus_mapped.csv\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not attach regions to buses: {e}\")\n",
    "\n",
    "# --- Enforce identical bus coordinates within each region (Option A) ---\n",
    "# We will overwrite bus lon/lat (and x/y if present) with the region centroid,\n",
    "# so that all buses belonging to the same region have identical coordinates.\n",
    "# This satisfies PyPSA's \"attributes must agree within cluster\" requirement.\n",
    "\n",
    "if shapefile is not None and busmap is not None and busmap.notna().all():\n",
    "    # 1) Build region centroids (EPSG:4326) from the already-prepared region_node\n",
    "    #    region_node has columns: NAME_1, x, y (where x=lon, y=lat)\n",
    "    if not {\"NAME_1\", \"x\", \"y\"}.issubset(region_node.columns):\n",
    "        logger.warning(\"Region node lacks NAME_1/x/y; cannot enforce centroid coordinates.\")\n",
    "    else:\n",
    "        # Align busmap with network and cast indices consistently\n",
    "        if not busmap.index.equals(network.buses.index):\n",
    "            busmap = busmap.reindex(network.buses.index)\n",
    "            logger.info(\"Aligned busmap index to network.buses.index.\")\n",
    "\n",
    "        # 2) Basic sanity checks before mutation\n",
    "        n_buses = len(network.buses)\n",
    "        n_unassigned = busmap.isna().sum()\n",
    "        if n_unassigned:\n",
    "            logger.warning(f\"{n_unassigned} buses lack region labels; these will remain unchanged.\")\n",
    "        logger.info(f\"Unique regions in busmap: {busmap.dropna().nunique()} (over {n_buses} buses)\")\n",
    "\n",
    "        # 3) Map region label -> centroid lon/lat\n",
    "        centroids = region_node.set_index(\"NAME_1\")[[\"x\", \"y\"]].rename(columns={\"x\": \"cent_lon\", \"y\": \"cent_lat\"})\n",
    "        missing_regions = sorted(set(busmap.dropna().unique()) - set(centroids.index))\n",
    "        if missing_regions:\n",
    "            logger.warning(f\"{len(missing_regions)} region labels not found in centroids: {missing_regions[:5]}...\")\n",
    "\n",
    "        # 4) Apply representative lon/lat per region to buses\n",
    "        buses_mut = network.buses.copy()\n",
    "        # Ensure lon/lat columns exist\n",
    "        for col in (\"lon\", \"lat\"):\n",
    "            if col not in buses_mut.columns:\n",
    "                buses_mut[col] = np.nan\n",
    "\n",
    "        # Join centroids onto each bus via busmap\n",
    "        rep_coords = busmap.to_frame(\"region\").join(centroids, how=\"left\", on=\"region\")\n",
    "\n",
    "        # Count how many will be updated\n",
    "        n_to_update = rep_coords[\"cent_lon\"].notna().sum()\n",
    "        logger.info(f\"Setting representative lon/lat from region centroids for {n_to_update} buses.\")\n",
    "\n",
    "        # Overwrite lon/lat where centroid available\n",
    "        buses_mut.loc[rep_coords[\"cent_lon\"].notna(), \"lon\"] = rep_coords.loc[rep_coords[\"cent_lon\"].notna(), \"cent_lon\"].values\n",
    "        buses_mut.loc[rep_coords[\"cent_lat\"].notna(), \"lat\"] = rep_coords.loc[rep_coords[\"cent_lat\"].notna(), \"cent_lat\"].values\n",
    "\n",
    "        # 5) Recompute x/y from lon/lat so everything is consistent (projected CRS)\n",
    "        #    Use a metric CRS (Web Mercator EPSG:3857 is fine for consistency here).\n",
    "        try:\n",
    "            gtmp = gpd.GeoDataFrame(\n",
    "                buses_mut,\n",
    "                geometry=gpd.points_from_xy(buses_mut[\"lon\"], buses_mut[\"lat\"], crs=\"EPSG:4326\")\n",
    "            )\n",
    "            gtmp_merc = gtmp.to_crs(\"EPSG:3857\")\n",
    "            # Ensure x/y exist and overwrite with projected coords\n",
    "            buses_mut[\"x\"] = gtmp_merc.geometry.x.values\n",
    "            buses_mut[\"y\"] = gtmp_merc.geometry.y.values\n",
    "            # drop geometry to keep PyPSA table tidy\n",
    "            buses_mut = pd.DataFrame(buses_mut).drop(columns=[\"geometry\"], errors=\"ignore\")\n",
    "            logger.info(\"Recomputed bus x/y from lon/lat in EPSG:3857.\")\n",
    "        except Exception as ex:\n",
    "            logger.warning(f\"Could not recompute x/y from lon/lat: {ex}. Proceeding with lon/lat only.\")\n",
    "\n",
    "        # 6) Sanity checks after mutation\n",
    "        #    a) Within each region, lon/lat should be uniform\n",
    "        probe_regions = busmap.dropna().unique()[:5]  # sample a few for log brevity\n",
    "        for r in probe_regions:\n",
    "            idx = busmap[busmap == r].index\n",
    "            if len(idx) > 1:\n",
    "                lon_nunique = buses_mut.loc[idx, \"lon\"].nunique(dropna=False)\n",
    "                lat_nunique = buses_mut.loc[idx, \"lat\"].nunique(dropna=False)\n",
    "                if lon_nunique > 1 or lat_nunique > 1:\n",
    "                    logger.warning(f\"Region '{r}': lon unique={lon_nunique}, lat unique={lat_nunique} (expected 1).\")\n",
    "\n",
    "        #    b) No NaNs in lon/lat for clustered buses\n",
    "        n_missing_coords = buses_mut.loc[busmap.notna(), [\"lon\", \"lat\"]].isna().any(axis=1).sum()\n",
    "        if n_missing_coords:\n",
    "            logger.warning(f\"{n_missing_coords} clustered buses still have missing lon/lat after centroid assignment.\")\n",
    "\n",
    "        # 7) Write back to the network (mutating the active network)\n",
    "        network.buses.loc[buses_mut.index, buses_mut.columns] = buses_mut\n",
    "\n",
    "        # 8) Final note\n",
    "        logger.info(\"Assigned representative coordinates per region to all clustered buses.\")\n",
    "else:\n",
    "    logger.warning(\"Skipping coordinate unification: no shapefile or busmap with full labels.\")\n",
    "\n",
    "# --- Harmonize per-cluster voltage attributes (v_nom_raw and v_nom) ---\n",
    "try:\n",
    "    if shapefile is not None and busmap is not None and busmap.notna().all():\n",
    "        # Align index just in case\n",
    "        busmap = busmap.reindex(network.buses.index)\n",
    "\n",
    "        buses_mut = network.buses.copy()\n",
    "\n",
    "        # Ensure the columns exist and are numeric\n",
    "        if \"v_nom_raw\" not in buses_mut.columns and \"v_nom\" in buses_mut.columns:\n",
    "            buses_mut[\"v_nom_raw\"] = buses_mut[\"v_nom\"]\n",
    "        if \"v_nom_raw\" not in buses_mut.columns:\n",
    "            buses_mut[\"v_nom_raw\"] = np.nan\n",
    "        if \"v_nom\" not in buses_mut.columns:\n",
    "            buses_mut[\"v_nom\"] = buses_mut[\"v_nom_raw\"]\n",
    "\n",
    "        # Coerce to numeric\n",
    "        buses_mut[\"v_nom_raw\"] = pd.to_numeric(buses_mut[\"v_nom_raw\"], errors=\"coerce\")\n",
    "        buses_mut[\"v_nom\"]      = pd.to_numeric(buses_mut[\"v_nom\"], errors=\"coerce\")\n",
    "\n",
    "        # Allowed Ecuador voltage levels (kV)\n",
    "        allowed_levels = np.array([69.0, 138.0, 230.0, 500.0])\n",
    "\n",
    "        def _snap_voltage_kv(values: pd.Series) -> pd.Series:\n",
    "            \"\"\"Snap numeric series to nearest allowed voltage level.\"\"\"\n",
    "            arr = pd.to_numeric(values, errors=\"coerce\").values.astype(float)\n",
    "            snapped = np.full_like(arr, np.nan, dtype=float)\n",
    "            mask = np.isfinite(arr)\n",
    "            if mask.any():\n",
    "                # vectorized nearest snap\n",
    "                diffs = np.abs(arr[mask, None] - allowed_levels[None, :])\n",
    "                nearest_idx = np.argmin(diffs, axis=1)\n",
    "                snapped[mask] = allowed_levels[nearest_idx]\n",
    "            return pd.Series(snapped, index=values.index)\n",
    "\n",
    "        # Precompute snapped variants (helps with messy inputs)\n",
    "        snapped_raw = _snap_voltage_kv(buses_mut[\"v_nom_raw\"])\n",
    "        snapped_nom = _snap_voltage_kv(buses_mut[\"v_nom\"])\n",
    "        # Prefer raw if available; otherwise use v_nom\n",
    "        snapped_pref = snapped_raw.where(snapped_raw.notna(), snapped_nom)\n",
    "\n",
    "        changed_clusters = 0\n",
    "        total_clusters = 0\n",
    "\n",
    "        for region_label, idx in busmap.groupby(busmap).groups.items():\n",
    "            total_clusters += 1\n",
    "            # volts in this cluster (snapped)\n",
    "            v_cluster = snapped_pref.loc[idx]\n",
    "            # if already uniform (or all NaN), skip\n",
    "            uniq = v_cluster.dropna().unique()\n",
    "            if len(uniq) <= 1:\n",
    "                # still write back snapping to keep internal consistency\n",
    "                if len(uniq) == 1:\n",
    "                    rep = float(uniq[0])\n",
    "                    buses_mut.loc[idx, \"v_nom_raw\"] = rep\n",
    "                    buses_mut.loc[idx, \"v_nom\"] = rep\n",
    "                continue\n",
    "\n",
    "            # Choose representative: mode (majority). If tie, choose higher kV.\n",
    "            vc = v_cluster.value_counts(dropna=True)\n",
    "            if vc.empty:\n",
    "                # all NaN → skip; clustering won’t include these if busmap has NaN\n",
    "                continue\n",
    "\n",
    "            top = vc.max()\n",
    "            candidates = vc[vc == top].index.astype(float)\n",
    "            rep = float(np.max(candidates))  # tie → higher level\n",
    "\n",
    "            # Apply to both fields so PyPSA sees consistent values\n",
    "            buses_mut.loc[idx, \"v_nom_raw\"] = rep\n",
    "            buses_mut.loc[idx, \"v_nom\"] = rep\n",
    "            changed_clusters += 1\n",
    "\n",
    "            logger.warning(\n",
    "                f\"Cluster '{region_label}': unified voltages to {rep:.0f} kV. \"\n",
    "                f\"Counts (snapped): {vc.to_dict()}\"\n",
    "            )\n",
    "\n",
    "        if changed_clusters:\n",
    "            logger.info(\n",
    "                f\"Harmonized voltage in {changed_clusters}/{total_clusters} region clusters \"\n",
    "                f\"(fields: v_nom_raw, v_nom).\"\n",
    "            )\n",
    "\n",
    "        # Write back\n",
    "        network.buses.loc[buses_mut.index, [\"v_nom_raw\", \"v_nom\"]] = buses_mut[[\"v_nom_raw\", \"v_nom\"]]\n",
    "\n",
    "    else:\n",
    "        logger.warning(\"Skipping voltage harmonization: shapefile/busmap not ready.\")\n",
    "except Exception as ex:\n",
    "    logger.warning(f\"Failed to harmonize per-cluster voltages: {ex}\")\n",
    "\n",
    "\n",
    "# Optional: clustering when busmap is available\n",
    "try:\n",
    "    if busmap is not None and busmap.notna().all():\n",
    "        from pypsa.clustering.spatial import get_clustering_from_busmap\n",
    "        clustered = get_clustering_from_busmap(network, busmap).network\n",
    "        clustered.generators.to_csv(\"clustered_gens.csv\")\n",
    "        logger.info(\"Clustered network generators exported to clustered_gens.csv\")\n",
    "    else:\n",
    "        logger.warning(\"Busmap missing or contains NaNs; skipping clustering.\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Clustering failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e42283a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clustered' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mclustered\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'clustered' is not defined"
     ]
    }
   ],
   "source": [
    "clustered"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
