{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3c7457f",
   "metadata": {},
   "source": [
    "Setup environment and load the base PyPSA-Earth network for a specified country.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89719dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import pypsa\n",
    "import warnings\n",
    "import pypsa\n",
    "from pathlib import Path\n",
    "import os\n",
    "import copy\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import requests\n",
    "import shutil\n",
    "# Import all dirs\n",
    "parent_dir = Path(os.getcwd()).parents[0]\n",
    "sys.path.append(str(parent_dir))\n",
    "from src.paths import all_dirs\n",
    "\n",
    "dirs = all_dirs()\n",
    "\n",
    "import logging\n",
    "from pathlib import Path\n",
    "# Add PyPSA-Earth scripts to the system path\n",
    "scripts_path = os.path.join(parent_dir.parents[0], \"pypsa-earth\", \"scripts\")\n",
    "assert os.path.isdir(scripts_path), f\"Path not found: {scripts_path}\"\n",
    "\n",
    "sys.path.append(scripts_path)\n",
    "\n",
    "\n",
    "\n",
    "LOG_FILE = join(parent_dir, \"logs.log\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s: %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    handlers=[logging.FileHandler(LOG_FILE, encoding=\"utf-8\")],\n",
    ")\n",
    "\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "# Suppress unnecessary warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Load the base network file path\n",
    "network_dir = dirs[\"data/processed/networks\"]\n",
    "network_files = [\n",
    "    \"network_original\",\n",
    "    \"network_snapped\",\n",
    "    \"network_expanded\",\n",
    "    \"network_expanded_no_orphans\",\n",
    "    \"network_nuclear\",\n",
    "    \"network_prod_mix\",\n",
    "    \"network_base\",\n",
    "]\n",
    "\n",
    "networks_dict = {\n",
    "    nf: pypsa.Network(join(network_dir, f\"{nf}.nc\")) for nf in network_files\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1699ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ppl_file = join(dirs[\"data/processed/generation\"],\n",
    "                \"powerplants_all.csv\")\n",
    "ppl = pd.read_csv(ppl_file, index_col=0)\n",
    "# Update ppl to match pypsa network\n",
    "\n",
    "# Load config file\n",
    "ppmatching = os.path.join(\n",
    "    Path(os.getcwd()).parents[1],\n",
    "      \"pypsa-earth\", \"configs\", \"powerplantmatching_config.yaml\"\n",
    ")\n",
    "print(ppmatching)\n",
    "config = pm.get_config(ppmatching)\n",
    "\n",
    "# Select target countries\n",
    "config[\"target_countries\"] = [\"EC\"]\n",
    "\n",
    "import powerplantmatching as pm\n",
    "\n",
    "ppl_file = join(dirs[\"data/processed/generation\"],\n",
    "                \"powerplants_all.csv\")\n",
    "ppl = pd.read_csv(ppl_file, index_col=0)\n",
    "# Update ppl to match pypsa network\n",
    "\n",
    "# Load config file\n",
    "ppmatching = os.path.join(\n",
    "    Path(os.getcwd()).parents[1],\n",
    "      \"pypsa-earth\", \"configs\", \"powerplantmatching_config.yaml\"\n",
    ")\n",
    "print(ppmatching)\n",
    "config = pm.get_config(ppmatching)\n",
    "\n",
    "# Select target countries\n",
    "config[\"target_countries\"] = [\"EC\"]\n",
    "\n",
    "# For the base case, I take the 2025 trend scenario with linear growt\n",
    "path_loads = dirs[\"data/processed/scaled_loads\"]\n",
    "load_profile_name = \"load_base_2030_linear.csv\"\n",
    "load_profile_file = join(path_loads, load_profile_name)\n",
    "\n",
    "load_profile = pd.read_csv(load_profile_file, index_col=0, parse_dates=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf405a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "network = copy.deepcopy(networks_dict[\"network_base\"])\n",
    "\n",
    "# Add the load profile to the network\n",
    "network.madd(\n",
    "    \"Load\", load_profile.columns, bus=load_profile.columns, p_set=load_profile\n",
    ")\n",
    "#network_base.loads_t.p_set\n",
    "# Check peak load (rounded to 1 decimal place)\n",
    "peak_load = round(network.loads_t.p_set.T.sum().max() / 1000, 1)\n",
    "logging.info(f\"Peak load: {peak_load} GW\")  # GW# loads\n",
    "# For the base case, I take the 2025 trend scenario with linear growt\n",
    "path_loads = dirs[\"data/processed/scaled_loads\"]\n",
    "load_profile_name = \"load_base_2030_linear.csv\"\n",
    "load_profile_file = join(path_loads, load_profile_name)\n",
    "\n",
    "load_profile = pd.read_csv(load_profile_file, index_col=0, parse_dates=True)\n",
    "# Add the load profile to the network\n",
    "network.madd(\n",
    "    \"Load\", load_profile.columns, bus=load_profile.columns, p_set=load_profile\n",
    ")\n",
    "#network_base.loads_t.p_set\n",
    "# Check peak load (rounded to 1 decimal place)\n",
    "peak_load = round(network.loads_t.p_set.T.sum().max() / 1000, 1)\n",
    "logging.info(f\"Peak load: {peak_load} GW\")  # GW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51616042",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_p = ppl_f.powerplant.to_pypsa_names()\n",
    "ppl_p = ppl_p.dropna(axis=1, how=\"all\")\n",
    "\n",
    "ppl_p[\"p_nom\"] = pd.to_numeric(ppl_p[\"p_nom\"], errors=\"coerce\")\n",
    "\n",
    "\n",
    "# Ensure capacity numeric (handles things like \"1'500,00\")\n",
    "ppl_p[\"p_nom\"] = (\n",
    "    ppl_p[\"p_nom\"].astype(str)\n",
    "    .str.replace(\"'\", \"\", regex=False)\n",
    "    .str.replace(\",\", \".\", regex=False)\n",
    ")\n",
    "ppl_p[\"p_nom\"] = pd.to_numeric(ppl_p[\"p_nom\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Helper to normalize strings: trim, lowercase, remove accents, collapse spaces\n",
    "def _normalize(s):\n",
    "    if pd.isna(s):\n",
    "        return None\n",
    "    s = str(s).strip().lower()\n",
    "    # remove accents\n",
    "    s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    # collapse internal whitespace\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "# 2) Normalize the Spanish carriers to a helper column\n",
    "ppl_p[\"carrier_norm\"] = ppl_p[\"carrier\"].apply(_normalize)\n",
    "\n",
    "# 3) Your mapping (keys should be normalized/without accents/lowercase)\n",
    "carrier_map_es2en = {\n",
    "    \"hidraulica\": \"Hydro\",\n",
    "    \"hidroelectrico\": \"Hydro\",\n",
    "    \"termica\": \"Thermal\",\n",
    "    \"termoelectrico\": \"Thermal\",\n",
    "    \"biomasa\": \"Biomass\",\n",
    "    \"fotovoltaica\": \"Solar PV\",\n",
    "    \"eolica\": \"Wind\",\n",
    "    \"eolico\": \"Wind\",\n",
    "    \"biogas\": \"Biogas\",\n",
    "    \"ernc\": \"Other RE\", # or \"Other RE\" if you prefer  \n",
    "    \"nuclear\": \"Nuclear\"\n",
    "}\n",
    "\n",
    "# 4) Map to English (new column). Keep original untouched.\n",
    "ppl_p[\"carrier\"] = ppl_p[\"carrier_norm\"].map(carrier_map_es2en)\n",
    "\n",
    "# 5) (Optional) If you want to overwrite the original column instead:\n",
    "# ppl_p[\"carrier\"] = ppl_p[\"carrier_en\"].fillna(ppl_p[\"carrier\"])\n",
    "\n",
    "# 6) (Optional) Inspect what didnâ€™t map (so you can extend the dictionary)\n",
    "unmapped = (\n",
    "    ppl_p.loc[ppl_p[\"carrier\"].isna() & ppl_p[\"carrier_norm\"].notna(), \"carrier_norm\"]\n",
    "    .value_counts()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf22ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 0\n",
    "\n",
    "for i in ppl_filtered.index:\n",
    "    pp = ppl_filtered.loc[[i]]\n",
    "    network.madd(\"Generator\", pp.index, **pp)\n",
    "    n+=1\n",
    "    k = len(network.generators)\n",
    "    if n!= k:\n",
    "        print(\"not working for \", i)\n",
    "        n-=1\n",
    "\n",
    "#network.generators#Safe way to add the plant and detect duplicates\n",
    "n = 0\n",
    "\n",
    "for i in ppl_filtered.index:\n",
    "    pp = ppl_filtered.loc[[i]]\n",
    "    network.madd(\"Generator\", pp.index, **pp)\n",
    "    n+=1\n",
    "    k = len(network.generators)\n",
    "    if n!= k:\n",
    "        print(\"not working for \", i)\n",
    "        n-=1\n",
    "\n",
    "\n",
    "\n",
    "# Filter Power Plants by size and by conection to network\n",
    "FILTERING_POWER = 10\n",
    "DATE_IN = 2017\n",
    "ppl_connected = ppl_p[ppl_p['component']==\"S.N.I.\"]  # Keep only plants connected to the network\n",
    "ppl_connected_current = ppl_connected[ppl_connected['DateIn']<=DATE_IN]  # Example: filter plants commissioned before or in 2024\n",
    "#ppl_connected_future = ppl_connected[ppl_connected['DateIn']>2017]  # Example: filter plants commissioned before or in 2024\n",
    "ppl_filtered = ppl_connected_current[ppl_connected_current['p_nom']>=FILTERING_POWER]  # Example: filter plants with capacity >= 100 kW\n",
    "\n",
    "ppl_filtered# Filter Power Plants by size and by conection to network\n",
    "FILTERING_POWER = 10\n",
    "DATE_IN = 2017\n",
    "ppl_connected = ppl_p[ppl_p['component']==\"S.N.I.\"]  # Keep only plants connected to the network\n",
    "ppl_connected_current = ppl_connected[ppl_connected['DateIn']<=DATE_IN]  # Example: filter plants commissioned before or in 2024\n",
    "#ppl_connected_future = ppl_connected[ppl_connected['DateIn']>2017]  # Example: filter plants commissioned before or in 2024\n",
    "ppl_filtered = ppl_connected_current[ppl_connected_current['p_nom']>=FILTERING_POWER]  # Example: filter plants with capacity >= 100 kW\n",
    "\n",
    "ppl_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0e033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from scipy.spatial import cKDTree as KDTree\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "substation_i = network.buses.query(\"substation_lv\").index\n",
    "kdtree = KDTree(network.buses.loc[substation_i, [\"x\", \"y\"]].values)\n",
    "ppl_i = ppl_filtered.index\n",
    "\n",
    "tree_i = kdtree.query(ppl_filtered.loc[ppl_i, [\"lon\", \"lat\"]].values)[1]\n",
    "ppl_filtered.loc[ppl_i, \"bus\"] = substation_i.append(pd.Index([np.nan]))[tree_i]\n",
    "\n",
    "# Bus needs to be string apparently\n",
    "ppl_filtered[\"bus\"] = ppl_filtered[\"bus\"].astype(str)\n",
    "missing = ~ppl_filtered[\"bus\"].isin(network.buses.index)\n",
    "print(ppl_filtered[missing][[\"bus\"]].head()) # should be emptyfrom scipy.spatial import cKDTree as KDTree\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "substation_i = network.buses.query(\"substation_lv\").index\n",
    "kdtree = KDTree(network.buses.loc[substation_i, [\"x\", \"y\"]].values)\n",
    "ppl_i = ppl_filtered.index\n",
    "\n",
    "tree_i = kdtree.query(ppl_filtered.loc[ppl_i, [\"lon\", \"lat\"]].values)[1]\n",
    "ppl_filtered.loc[ppl_i, \"bus\"] = substation_i.append(pd.Index([np.nan]))[tree_i]\n",
    "\n",
    "# Bus needs to be string apparently\n",
    "ppl_filtered[\"bus\"] = ppl_filtered[\"bus\"].astype(str)\n",
    "missing = ~ppl_filtered[\"bus\"].isin(network.buses.index)\n",
    "print(ppl_filtered[missing][[\"bus\"]].head()) \n",
    "\n",
    "total_capacity = ppl_p['p_nom'].sum()\n",
    "total_connected = ppl_connected['p_nom'].sum()\n",
    "total_connected_current = ppl_connected_current['p_nom'].sum()\n",
    "total_filtered = ppl_filtered['p_nom'].sum()\n",
    "logging.info(f\"\\nTotal capacity: {total_capacity} kW,\\nConnected capacity: {total_connected} kW, \\\n",
    "             \\nConnected current capacity: {total_connected_current} kW, \\\n",
    "             \\nFiltered capacity: {total_filtered} kW\")  \n",
    "total_capacity = ppl_p['p_nom'].sum()\n",
    "total_connected = ppl_connected['p_nom'].sum()\n",
    "total_connected_current = ppl_connected_current['p_nom'].sum()\n",
    "total_filtered = ppl_filtered['p_nom'].sum()\n",
    "logging.info(f\"\\nTotal capacity: {total_capacity} kW,\\nConnected capacity: {total_connected} kW, \\\n",
    "             \\nConnected current capacity: {total_connected_current} kW, \\\n",
    "             \\nFiltered capacity: {total_filtered} kW\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b969924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the shapefile boundaries on the same axis\n",
    "\n",
    "# Where to put / find GADM (expects dirs[\"data/raw/gadm\"])\n",
    "COUNTRY_ISO3 = \"ECU\"\n",
    "GADM_BASE_DIR = None  # will be inferred from dirs dict below\n",
    "GADM_VERSION = \"4.1\"\n",
    "GADM_FILE_STEM = f\"gadm41_{COUNTRY_ISO3}\"\n",
    "GADM_LAYER_L1 = \"ADM_ADM_1\"\n",
    "GADM_URL = f\"https://geodata.ucdavis.edu/gadm/gadm{GADM_VERSION}/gpkg/{GADM_FILE_STEM}.gpkg\"\n",
    "\n",
    "def ensure_gadm_file(gadm_path: str) -> str:\n",
    "    \"\"\"Ensure GADM gpkg exists locally; download if missing. Return path to gpkg.\"\"\"\n",
    "    gpkg_path = os.path.join(gadm_path, f\"{GADM_FILE_STEM}.gpkg\")\n",
    "    gpkg_dir = Path(gpkg_path).parent\n",
    "    if not Path(gpkg_path).is_file():\n",
    "        gpkg_dir.mkdir(parents=True, exist_ok=True)\n",
    "        resp = requests.get(GADM_URL, stream=True, timeout=300)\n",
    "        resp.raise_for_status()\n",
    "        with open(gpkg_path, \"wb\") as f:\n",
    "            shutil.copyfileobj(resp.raw, f)\n",
    "    else:\n",
    "        print(f\"GADM file already exists: {gpkg_path}\")\n",
    "    assert Path(gpkg_path).is_file(), f\"GADM file not found or failed to download: {gpkg_path}\"\n",
    "    return gpkg_path\n",
    "\n",
    "GADM_inputfile_gpkg = ensure_gadm_file(dirs[\"data/raw/gadm\"])\n",
    "\n",
    "shapefile = gpd.read_file(GADM_inputfile_gpkg, layer='ADM_ADM_1')\n",
    "#shapefile.boundary.plot(ax=ax, linewidth=0.2, color='black')#need to load the shapefile before nodes\n",
    "# Plot the shapefile boundaries on the same axis\n",
    "\n",
    "# Where to put / find GADM (expects dirs[\"data/raw/gadm\"])\n",
    "COUNTRY_ISO3 = \"ECU\"\n",
    "GADM_BASE_DIR = None  # will be inferred from dirs dict below\n",
    "GADM_VERSION = \"4.1\"\n",
    "GADM_FILE_STEM = f\"gadm41_{COUNTRY_ISO3}\"\n",
    "GADM_LAYER_L1 = \"ADM_ADM_1\"\n",
    "GADM_URL = f\"https://geodata.ucdavis.edu/gadm/gadm{GADM_VERSION}/gpkg/{GADM_FILE_STEM}.gpkg\"\n",
    "\n",
    "def ensure_gadm_file(gadm_path: str) -> str:\n",
    "    \"\"\"Ensure GADM gpkg exists locally; download if missing. Return path to gpkg.\"\"\"\n",
    "    gpkg_path = os.path.join(gadm_path, f\"{GADM_FILE_STEM}.gpkg\")\n",
    "    gpkg_dir = Path(gpkg_path).parent\n",
    "    if not Path(gpkg_path).is_file():\n",
    "        gpkg_dir.mkdir(parents=True, exist_ok=True)\n",
    "        resp = requests.get(GADM_URL, stream=True, timeout=300)\n",
    "        resp.raise_for_status()\n",
    "        with open(gpkg_path, \"wb\") as f:\n",
    "            shutil.copyfileobj(resp.raw, f)\n",
    "    else:\n",
    "        print(f\"GADM file already exists: {gpkg_path}\")\n",
    "    assert Path(gpkg_path).is_file(), f\"GADM file not found or failed to download: {gpkg_path}\"\n",
    "    return gpkg_path\n",
    "\n",
    "GADM_inputfile_gpkg = ensure_gadm_file(dirs[\"data/raw/gadm\"])\n",
    "\n",
    "shapefile = gpd.read_file(GADM_inputfile_gpkg, layer='ADM_ADM_1')\n",
    "#shapefile.boundary.plot(ax=ax, linewidth=0.2, color='black')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2148b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add cost data from 'costs' to 'network.generators'\n",
    "network.generators.drop(['marginal_cost', 'capital_cost'], axis=1, inplace=True)\n",
    "\n",
    "network.generators = pd.merge(network.generators, costs, left_on='carrier', right_index=True)\n",
    "network.generators[['bus', 'p_nom', 'carrier', 'marginal_cost', 'capital_cost']]\n",
    "network.generators.drop(['marginal_cost', 'capital_cost'], axis=1, inplace=True)\n",
    "\n",
    "# Add cost data from 'costs' to 'network.generators'\n",
    "network.generators = pd.merge(network.generators, costs, left_on='carrier', right_index=True)\n",
    "network.generators[['bus', 'p_nom', 'carrier', 'marginal_cost', 'capital_cost']]\n",
    "\n",
    " # filename = 'resources/' + country_code + '/costs_nuclear.csv' # The CSV file has been created with PyPSA-Earth\n",
    "cost_filename = join(dirs[\"data/raw/generation\"], \"technology_cost.csv\")\n",
    "# TODO: Calculate annuity for capital cost, check marginal cost with variable maintenance\n",
    "costs = pd.read_csv(\n",
    "    cost_filename,\n",
    "    index_col=0,\n",
    "    comment = \"#\"\n",
    ")# filename = 'resources/' + country_code + '/costs_nuclear.csv' # The CSV file has been created with PyPSA-Earth\n",
    "cost_filename = join(dirs[\"data/raw/generation\"], \"technology_cost.csv\")\n",
    "# TODO: Calculate annuity for capital cost, check marginal cost with variable maintenance\n",
    "costs = pd.read_csv(\n",
    "    cost_filename,\n",
    "    index_col=0,\n",
    "    comment = \"#\"\n",
    ")\n",
    "\n",
    "from shapely.geometry import Point\n",
    "# Attach node to region\n",
    "def attach_node_to_region(gdf, longitude, latitude):\n",
    "    # Define the point (longitude, latitude)\n",
    "    point = Point(longitude, latitude)\n",
    "    # Ensure the CRS of GeoDataFrame and point match\n",
    "    if gdf.crs != 'EPSG:4326': # Assuming the point is in WGS84 (EPSG:4326)\n",
    "        gdf = gdf.to_crs('EPSG:4326')\n",
    "    # Find the polygon that contains the point and get the coordinates of the centroid\n",
    "    region = gdf[gdf.contains(point)]['NAME_1'].values[0]\n",
    "    region_x = gdf[gdf.contains(point)]['x'].values[0]\n",
    "    region_y = gdf[gdf.contains(point)]['y'].values[0]\n",
    "    return region, region_x, region_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef7fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Attach node to region\n",
    "def attach_node_to_region(gdf, longitude, latitude):\n",
    "    # Define the point (longitude, latitude)\n",
    "    point = Point(longitude, latitude)\n",
    "    # Ensure the CRS of GeoDataFrame and point match\n",
    "    if gdf.crs != 'EPSG:4326': # Assuming the point is in WGS84 (EPSG:4326)\n",
    "        gdf = gdf.to_crs('EPSG:4326')\n",
    "    # Find the polygon that contains the point and get the coordinates of the centroid\n",
    "    region = gdf[gdf.contains(point)]['NAME_1'].values[0]\n",
    "    region_x = gdf[gdf.contains(point)]['x'].values[0]\n",
    "    region_y = gdf[gdf.contains(point)]['y'].values[0]\n",
    "    return region, region_x, region_y\n",
    "\n",
    "def get_node_by_region(region_node):\n",
    "    \"\"\"Consider each region as a node.\n",
    "    The centroid of the polygon for regions is the position of the node.\n",
    "\n",
    "    Important:\n",
    "    The projection of the maps is ``EPSG:4674 (SIRGAS 2000) <https://epsg.io/4674>``, unit- degree, geographic CRS.\n",
    "    To use calculations, e.g, \"centroid\", \"sjoin\",  the map needs to be reprojected, ``.to_crs('epsg:4087')``\n",
    "\n",
    "    EPSG:4326 is also a geographic CRS, unit - degree,\n",
    "    # issue: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
    "    # previous: to_crs('epsg:4326'), now: set .to_crs('epsg:4087') solved: see https://gis.stackexchange.com/questions/372564/userwarning-when-trying-to-get-centroid-from-a-polygon-geopandas\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    region_node['x'] = region_node.centroid.x\n",
    "    region_node['y'] = region_node.centroid.y\n",
    "    return region_node\n",
    "\n",
    "region_node = get_node_by_region(shapefile)\n",
    "#region_node.to_csv(\"regions.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc282370",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the cutout\n",
    "cutout_file = join(dirs[\"data/raw/cutouts\"], \"cutout-2013-era5.nc\")\n",
    "cutout = atlite.Cutout(cutout_file)import atlite\n",
    "\n",
    "# Load the cutout\n",
    "cutout_file = join(dirs[\"data/raw/cutouts\"], \"cutout-2013-era5.nc\")\n",
    "cutout = atlite.Cutout(cutout_file)\n",
    "\n",
    " # Cluster the network based on the busmap\n",
    "from pypsa.clustering.spatial import get_clustering_from_busmap\n",
    "clustered = get_clustering_from_busmap(network, busmap).network\n",
    "clustered.generators.to_csv(\"clustered_gens.csv\")\n",
    "\n",
    "# Cluster the network based on the busmap\n",
    "from pypsa.clustering.spatial import get_clustering_from_busmap\n",
    "clustered = get_clustering_from_busmap(network, busmap).network\n",
    "clustered.generators.to_csv(\"clustered_gens.csv\")ƒx°X \u0002\u0001ƒ{ƒ{ õ'network.lines = network.lines.reindex(columns=network.components['Line']['attrs'].index[1:])\n",
    "network.lines['type'] = np.nan\n",
    "network.buses = network.buses.reindex(columns=network.components['Bus']['attrs'].index[1:])\n",
    "network.buses['frequency'] = 60network.lines = network.lines.reindex(columns=network.components['Line']['attrs'].index[1:])\n",
    "network.lines['type'] = np.nan\n",
    "network.buses = network.buses.reindex(columns=network.components['Bus']['attrs'].index[1:])\n",
    "network.buses['frequency'] = 60L°W \u0002\u0001OO õ&busmap = attach_region_to_buses(region_node, network.buses)['region']\n",
    "busmap.to_csv(\"bus_mapped\")busmap = attach_region_to_buses(region_node, network.buses)['region']\n",
    "busmap.to_csv(\"bus_mapped\")‰\n",
    "°V \u0002\u0001‰\n",
    "‰\n",
    " õ%# Attach region to buses using the attach_node_to_region function\n",
    "def attach_region_to_buses(gdf, buses):\n",
    "    buses['region'] = 'NA'\n",
    "    for i in buses.index:\n",
    "        lon = buses.loc[i]['lon']\n",
    "        lat = buses.loc[i]['lat']\n",
    "        try:\n",
    "            region, region_x, region_y = attach_node_to_region(gdf, lon, lat)\n",
    "        except Exception as e:\n",
    "            print('Error:', e)\n",
    "            print('Index:', i)\n",
    "            continue\n",
    "        buses.loc[i, 'region'] = region\n",
    "        buses.loc[i, 'region_x'] = region_x\n",
    "        buses.loc[i, 'region_y'] = region_y\n",
    "    return buses# Attach region to buses using the attach_node_to_region function\n",
    "def attach_region_to_buses(gdf, buses):\n",
    "    buses['region'] = 'NA'\n",
    "    for i in buses.index:\n",
    "        lon = buses.loc[i]['lon']\n",
    "        lat = buses.loc[i]['lat']\n",
    "        try:\n",
    "            region, region_x, region_y = attach_node_to_region(gdf, lon, lat)\n",
    "        except Exception as e:\n",
    "            print('Error:', e)\n",
    "            print('Index:', i)\n",
    "            continue\n",
    "        buses.loc[i, 'region'] = region\n",
    "        buses.loc[i, 'region_x'] = region_x\n",
    "        buses.loc[i, 'region_y'] = region_y\n",
    "    return buses  \u0012! vectorized as shvec\n",
    "# from shapely import vectorized as shvec  # works when available\n",
    "\n",
    "# # 0) Mainland polygon (exclude GalÃ¡pagos by longitude)\n",
    "# shp_geo = shapefile.to_crs(4326)\n",
    "# continental = shp_geo[shp_geo.geometry.centroid.x > -85]\n",
    "# mainland_geom = unary_union(continental.geometry)\n",
    "\n",
    "# # 1) Pick coord names from your preprocessed objects (X,Y are already defined above)\n",
    "# def prepare_for_plot(da):\n",
    "#     # wrap longitudes to -180..180 if needed\n",
    "#     x = da[X].values\n",
    "#     if x.min() >= 0 and x.max() > 180:\n",
    "#         x_wrapped = ((x + 180) % 360) - 180\n",
    "#         order = np.argsort(x_wrapped)\n",
    "#         da = da.isel({X: order}).assign_coords({X: x_wrapped[order]})\n",
    "#     # sort so y increases upward and x increases to the right\n",
    "#     if da[Y][0] > da[Y][-1]:\n",
    "#         da = da.sortby(Y)\n",
    "#     if da[X][0] > da[X][-1]:\n",
    "#         da = da.sortby(X)\n",
    "#     return da\n",
    "\n",
    "# wind_da = prepare_for_plot(wnd100m)\n",
    "# solar_da = prepare_for_plot(influx)\n",
    "\n",
    "\n",
    "# # --- 2) Build a cell-intersection mask (keeps tiles partially inside) ---\n",
    "# import numpy as np\n",
    "# from shapely.ops import unary_union\n",
    "# from shapely.geometry import box\n",
    "# import shapely\n",
    "\n",
    "# # mainland_geom already computed above; wind_da/solar_da already prepared\n",
    "\n",
    "# # helper: edges from center coordinates (assumes monotonic)\n",
    "# def edges_from_centers(c):\n",
    "#     c = np.asarray(c, dtype=float)\n",
    "#     d = np.diff(c)\n",
    "#     e = np.empty(c.size + 1, dtype=float)\n",
    "#     e[1:-1] = c[:-1] + d/2\n",
    "#     e[0]     = c[0]  - d[0]/2\n",
    "#     e[-1]    = c[-1] + d[-1]/2\n",
    "#     return e\n",
    "\n",
    "# # Use the wind grid (same grid as solar after your coarsen); reuse for both\n",
    "# x_centers = wind_da[X].values\n",
    "# y_centers = wind_da[Y].values\n",
    "# x_edges = edges_from_centers(x_centers)\n",
    "# y_edges = edges_from_centers(y_centers)\n",
    "\n",
    "# nx, ny = len(x_centers), len(y_centers)\n",
    "\n",
    "# # build cell polygons (vectorized if shapely>=2; fallback to list comp)\n",
    "# # boxes shape: (ny, nx)\n",
    "# boxes = np.empty((ny, nx), dtype=object)\n",
    "# for j in range(ny):\n",
    "#     for i in range(nx):\n",
    "#         boxes[j, i] = box(x_edges[i], y_edges[j], x_edges[i+1], y_edges[j+1])\n",
    "\n",
    "# try:\n",
    "#     # shapely 2.0 vectorized predicate (fast)\n",
    "#     mask_cells = shapely.intersects(boxes, mainland_geom)\n",
    "# except Exception:\n",
    "#     # fallback\n",
    "#     mask_cells = np.array([[boxes[j, i].intersects(mainland_geom) for i in range(nx)]\n",
    "#                            for j in range(ny)], dtype=bool)\n",
    "\n",
    "# # apply mask: keep cells that intersect the mainland polygon\n",
    "# wind_masked  = np.ma.masked_where(~mask_cells, wind_da.values)\n",
    "# # solar uses same mask because same grid\n",
    "# solar_masked = np.ma.masked_where(~mask_cells, solar_da.values)\n",
    "\n",
    "\n",
    "# # --- build a cell-intersection mask (keeps tiles that touch Ecuador) ---\n",
    "# from shapely.geometry import box\n",
    "# from shapely.ops import unary_union\n",
    "# import shapely\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import cartopy.crs as ccrs\n",
    "\n",
    "# # mainland polygon (you already computed this above)\n",
    "# shp_geo = shapefile.to_crs(4326)\n",
    "# continental = shp_geo[shp_geo.geometry.centroid.x > -85]\n",
    "# mainland_geom = unary_union(continental.geometry)\n",
    "\n",
    "# def edges_from_centers(c):\n",
    "#     c = np.asarray(c, dtype=float)\n",
    "#     d = np.diff(c)\n",
    "#     e = np.empty(c.size + 1, dtype=float)\n",
    "#     e[1:-1] = c[:-1] + d/2\n",
    "#     e[0]     = c[0]  - d[0]/2\n",
    "#     e[-1]    = c[-1] + d[-1]/2\n",
    "#     return e\n",
    "\n",
    "# # Use wind grid (same as solar after your coarsen)\n",
    "# x_cent = wind_da[X].values\n",
    "# y_cent = wind_da[Y].values\n",
    "# x_ed = edges_from_centers(x_cent)\n",
    "# y_ed = edges_from_centers(y_cent)\n",
    "# nx, ny = len(x_cent), len(y_cent)\n",
    "\n",
    "# # cell polygons & intersects mask\n",
    "# mask_cells = np.zeros((ny, nx), dtype=bool)\n",
    "# for j in range(ny):\n",
    "#     for i in range(nx):\n",
    "#         cell = box(x_ed[i], y_ed[j], x_ed[i+1], y_ed[j+1])\n",
    "#         mask_cells[j, i] = cell.intersects(mainland_geom)\n",
    "\n",
    "# # keep whole-cell values where mask is True, NaN elsewhere\n",
    "# wind_vals  = np.where(mask_cells, wind_da.values,  np.nan)\n",
    "# solar_vals = np.where(mask_cells, solar_da.values, np.nan)\n",
    "\n",
    "# # --- color limits (choose fixed or robust) ---\n",
    "# w_vmin, w_vmax = 0, 8     # e.g., m/s\n",
    "# s_vmin, s_vmax = 0, 250    # e.g., W/mÂ² (d  \u0012\"aily mean-ish)\n",
    "\n",
    "# # --- PLOT AS FULL TILES with pcolormesh ---\n",
    "# fig, ax = plt.subplots(1, 2, subplot_kw={'projection': ccrs.PlateCarree()},\n",
    "#                        figsize=(15, 7), constrained_layout=True)\n",
    "\n",
    "# pc0 = ax[0].pcolormesh(x_ed, y_ed, wind_vals,\n",
    "#                        transform=ccrs.PlateCarree(),\n",
    "#                        vmin=w_vmin, vmax=w_vmax,\n",
    "#                        shading='flat', rasterized=True)\n",
    "# ax[0].set_title(\"Mean Wind Potential in m/s\", fontsize=14, pad=20)\n",
    "# continental.plot(ax=ax[0], edgecolor=\"k\", facecolor=\"none\", linewidth=0.6)\n",
    "# plt.colorbar(pc0, ax=ax[0], label=\"m/s\")\n",
    "\n",
    "# pc1 = ax[1].pcolormesh(x_ed, y_ed, solar_vals,\n",
    "#                        transform=ccrs.PlateCarree(),\n",
    "#                        vmin=s_vmin, vmax=s_vmax,\n",
    "#                        shading='flat', rasterized=True)\n",
    "# ax[1].set_title(\"Mean Influx in W/mÂ²\", fontsize=14)\n",
    "# continental.plot(ax=ax[1], edgecolor=\"k\", facecolor=\"none\", linewidth=0.6)\n",
    "# plt.colorbar(pc1, ax=ax[1], label=\"W/mÂ²\")\n",
    "\n",
    "# # zoom to mainland bounds\n",
    "# minx, miny, maxx, maxy = continental.total_bounds\n",
    "# for a in ax:\n",
    "#     a.set_extent([minx, maxx, miny, maxy], crs=ccrs.PlateCarree())\n",
    "\n",
    "# plt.show()# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import cartopy.crs as ccrs\n",
    "\n",
    "# geo_crs = 4326\n",
    "# area_crs = 6933\n",
    "\n",
    "# # 0) Compute Ecuador-only bounds (you already set them)\n",
    "# shp_geo = shapefile.to_crs(4326)\n",
    "# minx, miny, maxx, maxy = shp_geo.total_bounds\n",
    "# # --- SPEED UPS START HERE ---\n",
    "\n",
    "# # 1) Pull only needed variables and spatially crop FIRST\n",
    "# ds = cutout.data[[\"wnd100m\", \"influx_direct\", \"influx_diffuse\"]]\n",
    "\n",
    "# # atlite cutouts usually use x/y coords; fall back to lon/lat if present\n",
    "# X = \"x\" if \"x\" in ds.coords else (\"lon\" if \"lon\" in ds.coords else None)\n",
    "# Y = \"y\" if \"y\" in ds.coords else (\"lat\" if \"lat\" in ds.coords else None)\n",
    "# assert X and Y, f\"Couldn't find spatial coords. Have: {list(ds.coords)}\"\n",
    "\n",
    "# # Handle ascending/descending coordinate order when slicing\n",
    "# xslice = slice(minx, maxx) if ds[X][0] < ds[X][-1] else slice(maxx, minx)\n",
    "# yslice = slice(miny, maxy) if ds[Y][0] < ds[Y][-1] else slice(maxy, miny)\n",
    "# ds = ds.sel({X: xslice, Y: yslice})\n",
    "\n",
    "# # 2) Thin the time axis (e.g., take every 6th time step) and cast to float32\n",
    "# ds = ds.isel(time=slice(None, None, 6)).astype(\"float32\")\n",
    "\n",
    "# # 3) Coarsen spatially (increase factors if still slow: e.g., 6 or 8)\n",
    "# ds = ds.coarsen({X: 4, Y: 4}, boundary=\"trim\").mean()\n",
    "\n",
    "# # 4) Reduce over time ONCE and materialize in memory\n",
    "# wnd100m = ds[\"wnd100m\"].mean(dim=\"time\").compute()\n",
    "# influx  = (ds[\"influx_direct\"].mean(dim=\"time\") + ds[\"influx_diffuse\"].mean(dim=\"time\")).compute()\n",
    "\n",
    "\n",
    "# # --- PLOTTING ONLY: replace everything from \"fig, ax = plt.subplots(...)\" down ---\n",
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import cartopy.crs as ccrs\n",
    "# from shapely.ops import unary_union\n",
    "# import shapely\n",
    "# # if using Shapely <2.0, use: from shapely import vectorized as shvec\n",
    "# from shapely import vectorized as shvec  # works when available\n",
    "\n",
    "# # 0) Mainland polygon (exclude GalÃ¡pagos by longitude)\n",
    "# shp_geo = shapefile.to_crs(4326)\n",
    "# continental = shp_geo[shp_geo.geometry.centroid.x > -85]\n",
    "# mainland_geom = unary_union(continental.geometry)\n",
    "\n",
    "# # 1) Pick coord names from your preprocessed objects (X,Y are already defined above)\n",
    "# def prepare_for_plot(da):\n",
    "#     # wrap longitudes to -180..180 if needed\n",
    "#     x = da[X].values\n",
    "#     if x.min() >= 0 and x.max() > 180:\n",
    "#         x_wrapped = ((x + 180) % 360) - 180\n",
    "#         order = np.argsort(x_wrapped)\n",
    "#         da = da.isel({X: order}).assign_coords({X: x_wrapped[order]})\n",
    "#     # sort so y increases upward and x increases to the right\n",
    "#     if da[Y][0] > da[Y][-1]:\n",
    "#         da = da.sortby(Y)\n",
    "#     if da[X][0] > da[X][-1]:\n",
    "#         da = da.sortby(X)\n",
    "#     return da\n",
    "\n",
    "# wind_da = prepare_for_plot(wnd100m)\n",
    "# solar_da = prepare_for_plot(influx)\n",
    "\n",
    "\n",
    "# # --- 2) Build a cell-intersection mask (keeps tiles partially inside) ---\n",
    "# import numpy as np\n",
    "# from shapely.ops import unary_union\n",
    "# from shapely.geometry import box\n",
    "# import shapely\n",
    "\n",
    "# # mainland_geom already computed a    bove; wind_da/solar_da already prepared\n",
    "\n",
    "# # helper: edges from center coordinates (assumes monotonic)\n",
    "# def edges_from_centers(c):\n",
    "#     c = np.asarray(c, dtype=float)\n",
    "#     d = np.diff(c)\n",
    "#     e = np.empty(c.size + 1, dtype=float)\n",
    "#     e[1:-1] = c[:-1] + d/2\n",
    "#     e[0]     = c[0]  - d[0]/2\n",
    "#     e[-1]    = c[-1] + d[-1]/2\n",
    "#     return e\n",
    "\n",
    "# # Use the wind grid (same grid as solar after your coarsen); reuse for both\n",
    "# x_centers = wind_da[X].values\n",
    "# y_centers = wind_da[Y].values\n",
    "# x_edges = edges_from_centers(x_centers)\n",
    "# y_edges = edges_from_centers(y_centers)\n",
    "\n",
    "# nx, ny = len(x_centers), len(y_centers)\n",
    "\n",
    "# # build cell polygons (vectorized if shapely>=2; fallback to list comp)\n",
    "# # boxes shape: (ny, nx)\n",
    "# boxes = np.empty((ny, nx), dtype=object)\n",
    "# for j in range(ny):\n",
    "#     for i in range(nx):\n",
    "#         boxes[j, i] = box(x_edges[i], y_edges[j], x_edges[i+1], y_edges[j+1])\n",
    "\n",
    "# try:\n",
    "#     # shapely 2.0 vectorized predicate (fast)\n",
    "#     mask_cells = shapely.intersects(boxes, mainland_geom)\n",
    "# except Exception:\n",
    "#     # fallback\n",
    "#     mask_cells = np.array([[boxes[j, i].intersects(mainland_geom) for i in range(nx)]\n",
    "#                            for j in range(ny)], dtype=bool)\n",
    "\n",
    "# # apply mask: keep cells that intersect the mainland polygon\n",
    "# wind_masked  = np.ma.masked_where(~mask_cells, wind_da.values)\n",
    "# # solar uses same mask because same grid\n",
    "# solar_masked = np.ma.masked_where(~mask_cells, solar_da.values)\n",
    "\n",
    "\n",
    "# # --- build a cell-intersection mask (keeps tiles that touch Ecuador) ---\n",
    "# from shapely.geometry import box\n",
    "# from shapely.ops import unary_union\n",
    "# import shapely\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import cartopy.crs as ccrs\n",
    "\n",
    "# # mainland polygon (you already computed this above)\n",
    "# shp_geo = shapefile.to_crs(4326)\n",
    "# continental = shp_geo[shp_geo.geometry.centroid.x > -85]\n",
    "# mainland_geom = unary_union(continental.geometry)\n",
    "\n",
    "# def edges_from_centers(c):\n",
    "#     c = np.asarray(c, dtype=float)\n",
    "#     d = np.diff(c)\n",
    "#     e = np.empty(c.size + 1, dtype=float)\n",
    "#     e[1:-1] = c[:-1] + d/2\n",
    "#     e[0]     = c[0]  - d[0]/2\n",
    "#     e[-1]    = c[-1] + d[-1]/2\n",
    "#     return e\n",
    "\n",
    "# # Use wind grid (same as solar after your coarsen)\n",
    "# x_cent = wind_da[X].values\n",
    "# y_cent = wind_da[Y].values\n",
    "# x_ed = edges_from_centers(x_cent)\n",
    "# y_ed = edges_from_centers(y_cent)\n",
    "# nx, ny = len(x_cent), len(y_cent)\n",
    "\n",
    "# # cell polygons & intersects mask\n",
    "# mask_cells = np.zeros((ny, nx), dtype=bool)\n",
    "# for j in range(ny):\n",
    "#     for i in range(nx):\n",
    "#         cell = box(x_ed[i], y_ed[j], x_ed[i+1], y_ed[j+1])\n",
    "#         mask_cells[j, i] = cell.intersects(mainland_geom)\n",
    "\n",
    "# # keep whole-cell values where mask is True, NaN elsewhere\n",
    "# wind_vals  = np.where(mask_cells, wind_da.values,  np.nan)\n",
    "# solar_vals = np.where(mask_cells, solar_da.values, np.nan)\n",
    "\n",
    "# # --- color limits (choose fixed or robust) ---\n",
    "# w_vmin, w_vmax = 0, 8     # e.g., m/s\n",
    "# s_vmin, s_vmax = 0, 250    # e.g., W/mÂ² (daily mean-ish)\n",
    "\n",
    "# # --- PLOT AS FULL TILES with pcolormesh ---\n",
    "# fig, ax = plt.subplots(1, 2, subplot_kw={'projection': ccrs.PlateCarree()},\n",
    "#                        figsize=(15, 7), constrained_layout=True)\n",
    "\n",
    "# pc0 = ax[0].pcolormesh(x_ed, y_ed, wind_vals,\n",
    "#                        transform=ccrs.PlateCarree(),\n",
    "#                        vmin=w_vmin, vmax=w_vmax,\n",
    "#                        shading='flat', rasterized=True)\n",
    "# ax[0].set_title(\"Mean Wind Potential in m/s\", fontsize=14, pad=20)\n",
    "# continental.plot(ax=ax[0], edgecolor=\"k\", facecolor=\"none\", linewidth=0.6)\n",
    "# plt.colorbar(pc0, ax=ax[0], label=\"m/s\")\n",
    "\n",
    "# pc1 = ax[1].pcolormesh(x_ed, y_ed, solar_vals,\n",
    "#                        transform=ccrs.PlateCarree(),\n",
    "#                        vmin=s_vmin, vmax=s_vmax,\n",
    "#                        shading='flat', rasterized=True)\n",
    "# ax[1].set_title(\"Mean Influx in W/mÂ²\", fontsize=14)\n",
    "# continental.plot(ax=ax[1], edgecolor=\"k\", facecolor=\"none\", linewidth=0.6)\n",
    "# plt.colorbar(pc1, ax=ax[1], label=\"W/mÂ²\")\n",
    "\n",
    "# # zoom to mainland bounds\n",
    "# minx, miny, maxx, maxy = continental.total_bounds\n",
    "# for a in ax:\n",
    "#     a.set_extent([minx, maxx, miny, maxy], crs=ccrs.PlateCarree())\n",
    "\n",
    "# plt.show()\n",
    "   \u0002 Z \t( Z                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ƒJ°\\ \u0002\u0001ƒMƒM õ+import xarray as xr\n",
    "\n",
    "# Calculate the area of each cell in the cutout\n",
    "area = cutout.grid.to_crs(area_crs).area / 1e6\n",
    "area = xr.DataArray(area.values.reshape(cutout.shape), [cutout.coords[\"y\"], cutout.coords[\"x\"]])\n",
    "\n",
    "area[0][0]import xarray as xr\n",
    "\n",
    "# Calculate the area of each cell in the cutout\n",
    "area = cutout.grid.to_crs(area_crs).area / 1e6\n",
    "area = xr.DataArray(area.values.reshape(cutout.shape), [cutout.coords[\"y\"], cutout.coords[\"x\"]])\n",
    "\n",
    "area[0][0]íD°[ \u0002\u0001íGíG õ*# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import cartopy.crs as ccrs\n",
    "\n",
    "# geo_crs = 4326\n",
    "# area_crs = 6933\n",
    "\n",
    "# # 0) Compute Ecuador-only bounds (you already set them)\n",
    "# shp_geo = shapefile.to_crs(4326)\n",
    "# minx, miny, maxx, maxy = shp_geo.total_bounds\n",
    "# # --- SPEED UPS START HERE ---\n",
    "\n",
    "# # 1) Pull only needed variables and spatially crop FIRST\n",
    "# ds = cutout.data[[\"wnd100m\", \"influx_direct\", \"influx_diffuse\"]]\n",
    "\n",
    "# # atlite cutouts usually use x/y coords; fall back to lon/lat if present\n",
    "# X = \"x\" if \"x\" in ds.coords else (\"lon\" if \"lon\" in ds.coords else None)\n",
    "# Y = \"y\" if \"y\" in ds.coords else (\"lat\" if \"lat\" in ds.coords else None)\n",
    "# assert X and Y, f\"Couldn't find spatial coords. Have: {list(ds.coords)}\"\n",
    "\n",
    "# # Handle ascending/descending coordinate order when slicing\n",
    "# xslice = slice(minx, maxx) if ds[X][0] < ds[X][-1] else slice(maxx, minx)\n",
    "# yslice = slice(miny, maxy) if ds[Y][0] < ds[Y][-1] else slice(maxy, miny)\n",
    "# ds = ds.sel({X: xslice, Y: yslice})\n",
    "\n",
    "# # 2) Thin the time axis (e.g., take every 6th time step) and cast to float32\n",
    "# ds = ds.isel(time=slice(None, None, 6)).astype(\"float32\")\n",
    "\n",
    "# # 3) Coarsen spatially (increase factors if still slow: e.g., 6 or 8)\n",
    "# ds = ds.coarsen({X: 4, Y: 4}, boundary=\"trim\").mean()\n",
    "\n",
    "# # 4) Reduce over time ONCE and materialize in memory\n",
    "# wnd100m = ds[\"wnd100m\"].mean(dim=\"time\").compute()\n",
    "# influx  = (ds[\"influx_direct\"].mean(dim=\"time\") + ds[\"influx_diffuse\"].mean(dim=\"time\")).compute()\n",
    "\n",
    "\n",
    "# # --- PLOTTING ONLY: replace everything from \"fig, ax = plt.subplots(...)\" down ---\n",
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import cartopy.crs as ccrs\n",
    "# from shapely.ops import unary_union\n",
    "# import shapely\n",
    "# # if using Shapely <2.0, use: from shapely import  \u0012 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff81a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datasets# Define renewable resources\n",
    "resources = [\n",
    "    {'method': 'wind', 'turbine': 'Vestas_V112_3MW', 'capacity_per_sqkm': 4.6, 'resource': 'onwind'},\n",
    "    {'method': 'pv', 'panel': 'CSi', 'orientation': 'latitude_optimal', 'capacity_per_sqkm': 2, 'resource': 'solar'},\n",
    "]\n",
    "correction_factor = 1\n",
    "\n",
    "# Create new geodataframe for the capacities\n",
    "capacities = shapefile\n",
    "datasets = {}\n",
    "shape = shapefile.set_index(\"NAME_1\")\n",
    "\n",
    "# Calculate capacities per resource\n",
    "# TODO: check the algorithm for correctness and efficiency\n",
    "for resource in resources:\n",
    "    method =  resource['method']\n",
    "    res = resource['resource']\n",
    "    profile_path = join(dirs[\"data/processed/generation\"], f\"{res}.nc\")\n",
    "    if Path(profile_path).is_file():\n",
    "        print('Profile found:', res)\n",
    "        ds = xr.open_dataset(profile_path)\n",
    "        profile = ds['profile']\n",
    "        capacity = ds['capacities']\n",
    "        capacities[method] = capacity\n",
    "        datasets[res] = ds\n",
    "    else:\n",
    "        print('Profile not found:', res)\n",
    "        cap_per_sqkm = resource['capacity_per_sqkm']\n",
    "        print(\"method\", method)\n",
    "        params = [resource.pop(key) for key in ['method', 'capacity_per_sqkm', 'resource']][0]\n",
    "        print(\"resource\", resource)\n",
    "        func = getattr(cutout, params)\n",
    "        print(\"func obtained\")\n",
    "        capacity_factor = correction_factor * func(capacity_factor=True, **resource)\n",
    "        print(\"cf\", capacity_factor)\n",
    "        layout = capacity_factor * area * cap_per_sqkm\n",
    "        print(\"layour\", layout)\n",
    "        profile, capacity = func(shapes=shape, per_unit=True, return_capacity=True, layout=layout, **resource)\n",
    "        print(\"capacity\", capacity)\n",
    "        capacities[method] = capacity\n",
    "        ds = xr.Dataset({\n",
    "        'profile': profile.rename({'NAME_1': 'bus'}),\n",
    "        'capacities': capacity.rename({'NAME_1': 'bus'})\n",
    "        })\n",
    "        datasets[res] = ds\n",
    "        ds.to_netcdf(profile_path)\n",
    "\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa14bc52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
